<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>ba15bcdc0558451b956aa4f71db095f6</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<section id="pytorch---autograd-is-what-we-need" class="cell markdown"
id="sD1TdQ6sIkwb">
<h1>PyTorch - Autograd (is what we need.)</h1>
<p>Advanced Learning 2024</p>
</section>
<div class="cell markdown" id="6fkKJP76I-Hu">

</div>
<div class="cell markdown" id="LsLdJV3tzcEb">
<p>For SUBMISSION:</p>
<p>Please upload the complete and executed <code>ipynb</code> to your
git repository. Verify that all of your output can be viewed directly
from github, and provide a link to that git file below.</p>
<pre><code>STUDENT ID: 209307396</code></pre>
<pre><code>STUDENT GIT LINK: https://github.com/meirabar/Adv.-computational-learning-and-data-analysis---52025.git</code></pre>
<p>In Addition, don't forget to add your ID to the files:<br />
<code>PS1_Part3_Autograd_2024_ID_[209307396].html</code></p>
</div>
<div class="cell markdown" id="bpGvQkizUkAJ">
<p>This part of the problem-set is divided into 2 sections: (a) a short
introduction to PyTorch autograd, and (b) some practice questions.<br />
The practice questions cover calculating the gradients and updating the
weights of a statistical model (linear regression), with and without
PyTorch autograd.</p>
</div>
<section id="a-short-intro-to-pytorch-autograd" class="cell markdown"
id="tMu8INSVK6N3">
<h2>A Short Intro to PyTorch Autograd</h2>
</section>
<div class="cell markdown" id="xLMa2IKcIzWC">
<p>In PyTorch, autograd, short for automatic differentiation, is a core
functionality that empowers you to efficiently calculate gradients
during deep learning model training. It acts as a computational engine
that automatically tracks the operations performed on your tensors and
calculates the gradients of any loss function with respect to the
model's learnable parameters.</p>
<p><strong>Why is Autograd Important?</strong></p>
<ul>
<li>Gradient Calculation: Deep learning relies heavily on gradient-based
optimization algorithms like stochastic gradient descent (SGD). These
algorithms adjust the weights and biases (learnable parameters) of your
model based on the gradients of the loss function. Autograd automates
this process, saving you from manually calculating complex
derivatives.</li>
<li>Efficient Backpropagation: Backpropagation is the algorithm at the
heart of training deep neural networks. It propagates the error (loss)
backward through the network, allowing the model to adjust its
parameters in the direction that minimizes the loss. Autograd
streamlines this process, making training more efficient.</li>
</ul>
</div>
<div class="cell markdown" id="H0-iHMFVJ87I">
<p><strong>How Does Autograd Work?</strong></p>
<ul>
<li><p>Tracking Operations: When you enable autograd mode (with
<code>torch.autograd.record_grad()</code>), PyTorch creates a
computational graph in the background. This graph tracks all the
operations performed on your tensors</p></li>
<li><p>Backward Pass: Once you calculate a loss function (which
typically depends on the output of your model), you can call the
<code>backward()</code> method on the loss tensor. This triggers the
backward pass through the computational graph.</p></li>
<li><p>Gradient Calculation: As the backward pass traverses the graph,
autograd computes the gradients of the loss function with respect to
each learnable parameter in the network. These gradients indicate how
much a specific parameter contributed to the overall loss.</p></li>
<li><p>Parameter Update: By utilizing these gradients, optimization
algorithms like SGD update the model's learnable parameters in a
direction that minimizes the loss function. This iterative process
continues until the model converges (achieves a desired level of
performance).</p></li>
</ul>
<p><strong>Key Points about Autograd:</strong></p>
<ul>
<li>Automatic</li>
<li>Only for Learnable Parameters: Autograd tracks gradients only for
tensors that have <code>requires_grad=True</code>.</li>
<li>Dynamic vs. Static Graphs (Optional): PyTorch offers both dynamic
computational graphs (eager execution) and static graphs (graph mode)
for model development.</li>
</ul>
</div>
<div class="cell code" data-execution_count="1" id="yHpXMD96Mo7K">
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="2"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="d3FXn1y40muL" data-outputId="734bbc0f-addd-4f4c-cded-0a5f6a0ad7a1">
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> <span class="st">&quot;cuda&quot;</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">&quot;cpu&quot;</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>torch.set_default_device(device)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Using </span><span class="sc">{</span>device<span class="sc">}</span><span class="ss">!&quot;</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Using cuda!
</code></pre>
</div>
</div>
<section id="0-loading-data-and-defining-our-model-and-loss"
class="cell markdown" id="HvKljalBTVy0">
<h2>0. Loading data and defining our model and loss</h2>
</section>
<div class="cell markdown" id="dTVwaEhqPRC-">
<p>Load the California Housing Market dataset into a panda's
DataFrame.<br />
In addition, scale the data (why?).</p>
</div>
<div class="cell code" data-execution_count="3"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:223}"
id="gEoX5S8uHv_l" data-outputId="44519190-56fe-4668-9aa3-63be2d60d276">
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the dataset.</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>cali_train_path <span class="op">=</span> <span class="st">&quot;https://download.mlcc.google.com/mledu-datasets/california_housing_train.csv&quot;</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>train_df <span class="op">=</span> pd.read_csv(filepath_or_buffer<span class="op">=</span>cali_train_path).drop([<span class="st">&#39;longitude&#39;</span>,<span class="st">&#39;latitude&#39;</span>],axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>train_df_norm<span class="op">=</span>(train_df<span class="op">-</span>train_df.mean(axis<span class="op">=</span><span class="dv">0</span>))<span class="op">/</span>train_df.std(axis<span class="op">=</span><span class="dv">0</span>) <span class="co"># column-wise operators</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Scale the labels:</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Dataset dimension: </span><span class="sc">{</span>train_df_norm<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>train_df_norm.head()</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Dataset dimension: (17000, 7)
</code></pre>
</div>
<div class="output execute_result" data-execution_count="3">

  <div id="df-ff78ffa6-e0ea-4f6e-9c89-e95cdfe54edd" class="colab-df-container">
    <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>housing_median_age</th>
      <th>total_rooms</th>
      <th>total_bedrooms</th>
      <th>population</th>
      <th>households</th>
      <th>median_income</th>
      <th>median_house_value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>-1.079639</td>
      <td>1.361655</td>
      <td>1.764152</td>
      <td>-0.361173</td>
      <td>-0.075996</td>
      <td>-1.252506</td>
      <td>-1.210522</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-0.761850</td>
      <td>2.296540</td>
      <td>3.230346</td>
      <td>-0.261858</td>
      <td>-0.099401</td>
      <td>-1.081451</td>
      <td>-1.096713</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-0.920744</td>
      <td>-0.882436</td>
      <td>-0.866931</td>
      <td>-0.955326</td>
      <td>-0.999223</td>
      <td>-1.170071</td>
      <td>-1.048430</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-1.159087</td>
      <td>-0.524171</td>
      <td>-0.480216</td>
      <td>-0.796769</td>
      <td>-0.715753</td>
      <td>-0.362590</td>
      <td>-1.154480</td>
    </tr>
    <tr>
      <th>4</th>
      <td>-0.682402</td>
      <td>-0.545731</td>
      <td>-0.506313</td>
      <td>-0.701809</td>
      <td>-0.622130</td>
      <td>-1.026424</td>
      <td>-1.222593</td>
    </tr>
  </tbody>
</table>
</div>
    <div class="colab-df-buttons">

  <div class="colab-df-container">
    <button class="colab-df-convert" onclick="convertToInteractive('df-ff78ffa6-e0ea-4f6e-9c89-e95cdfe54edd')"
            title="Convert this dataframe to an interactive table."
            style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px" viewBox="0 -960 960 960">
    <path d="M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z"/>
  </svg>
    </button>

  <style>
    .colab-df-container {
      display:flex;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    .colab-df-buttons div {
      margin-bottom: 4px;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

    <script>
      const buttonEl =
        document.querySelector('#df-ff78ffa6-e0ea-4f6e-9c89-e95cdfe54edd button.colab-df-convert');
      buttonEl.style.display =
        google.colab.kernel.accessAllowed ? 'block' : 'none';

      async function convertToInteractive(key) {
        const element = document.querySelector('#df-ff78ffa6-e0ea-4f6e-9c89-e95cdfe54edd');
        const dataTable =
          await google.colab.kernel.invokeFunction('convertToInteractive',
                                                    [key], {});
        if (!dataTable) return;

        const docLinkHtml = 'Like what you see? Visit the ' +
          '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
          + ' to learn more about interactive tables.';
        element.innerHTML = '';
        dataTable['output_type'] = 'display_data';
        await google.colab.output.renderOutput(dataTable, element);
        const docLink = document.createElement('div');
        docLink.innerHTML = docLinkHtml;
        element.appendChild(docLink);
      }
    </script>
  </div>


<div id="df-ceb79c7e-a97b-4f47-9a6b-b86df4067eca">
  <button class="colab-df-quickchart" onclick="quickchart('df-ceb79c7e-a97b-4f47-9a6b-b86df4067eca')"
            title="Suggest charts"
            style="display:none;">

<svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
     width="24px">
    <g>
        <path d="M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z"/>
    </g>
</svg>
  </button>

<style>
  .colab-df-quickchart {
      --bg-color: #E8F0FE;
      --fill-color: #1967D2;
      --hover-bg-color: #E2EBFA;
      --hover-fill-color: #174EA6;
      --disabled-fill-color: #AAA;
      --disabled-bg-color: #DDD;
  }

  [theme=dark] .colab-df-quickchart {
      --bg-color: #3B4455;
      --fill-color: #D2E3FC;
      --hover-bg-color: #434B5C;
      --hover-fill-color: #FFFFFF;
      --disabled-bg-color: #3B4455;
      --disabled-fill-color: #666;
  }

  .colab-df-quickchart {
    background-color: var(--bg-color);
    border: none;
    border-radius: 50%;
    cursor: pointer;
    display: none;
    fill: var(--fill-color);
    height: 32px;
    padding: 0;
    width: 32px;
  }

  .colab-df-quickchart:hover {
    background-color: var(--hover-bg-color);
    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);
    fill: var(--button-hover-fill-color);
  }

  .colab-df-quickchart-complete:disabled,
  .colab-df-quickchart-complete:disabled:hover {
    background-color: var(--disabled-bg-color);
    fill: var(--disabled-fill-color);
    box-shadow: none;
  }

  .colab-df-spinner {
    border: 2px solid var(--fill-color);
    border-color: transparent;
    border-bottom-color: var(--fill-color);
    animation:
      spin 1s steps(1) infinite;
  }

  @keyframes spin {
    0% {
      border-color: transparent;
      border-bottom-color: var(--fill-color);
      border-left-color: var(--fill-color);
    }
    20% {
      border-color: transparent;
      border-left-color: var(--fill-color);
      border-top-color: var(--fill-color);
    }
    30% {
      border-color: transparent;
      border-left-color: var(--fill-color);
      border-top-color: var(--fill-color);
      border-right-color: var(--fill-color);
    }
    40% {
      border-color: transparent;
      border-right-color: var(--fill-color);
      border-top-color: var(--fill-color);
    }
    60% {
      border-color: transparent;
      border-right-color: var(--fill-color);
    }
    80% {
      border-color: transparent;
      border-right-color: var(--fill-color);
      border-bottom-color: var(--fill-color);
    }
    90% {
      border-color: transparent;
      border-bottom-color: var(--fill-color);
    }
  }
</style>

  <script>
    async function quickchart(key) {
      const quickchartButtonEl =
        document.querySelector('#' + key + ' button');
      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.
      quickchartButtonEl.classList.add('colab-df-spinner');
      try {
        const charts = await google.colab.kernel.invokeFunction(
            'suggestCharts', [key], {});
      } catch (error) {
        console.error('Error during call to suggestCharts:', error);
      }
      quickchartButtonEl.classList.remove('colab-df-spinner');
      quickchartButtonEl.classList.add('colab-df-quickchart-complete');
    }
    (() => {
      let quickchartButtonEl =
        document.querySelector('#df-ceb79c7e-a97b-4f47-9a6b-b86df4067eca button');
      quickchartButtonEl.style.display =
        google.colab.kernel.accessAllowed ? 'block' : 'none';
    })();
  </script>
</div>

    </div>
  </div>

</div>
</div>
<div class="cell markdown" id="bVaPDR7AQomq">
<p>We define a linear model to fit the scaled median house value to our
data:</p>
<p><strong>Our model :</strong>.</p>
<p><span
class="math display">median_house_value<sub><em>i</em></sub> = <em>y</em><sub><em>i</em></sub> = <em>β</em><sub>0</sub> + <em>β</em><sub>1</sub>housing_median_age<sub><em>i</em></sub> + <em>β</em><sub>2</sub>total_rooms<sub><em>i</em></sub> + <em>β</em><sub>3</sub>total_bedrooms<sub><em>i</em></sub> + <em>β</em><sub>4</sub>population<sub><em>i</em></sub> + <em>β</em><sub>5</sub>households<sub><em>i</em></sub> + <em>β</em><sub>6</sub>median_income<sub><em>i</em></sub></span></p>
<p><strong>Our loss :</strong><br />
<span class="math display">$$
\hat{L}_i(y_i,\hat{y}_i) = MSE(\text{y}_i,\hat{\text{y}}_i)
$$</span></p>
</div>
<section
id="1-manually-calculating-the-gradients-and-updating-the-weights"
class="cell markdown" id="Ohg3pW3pKbPH">
<h2>1. Manually Calculating the Gradients and Updating the Weights</h2>
</section>
<div class="cell markdown" id="OnZq3CEPG4_K">
<p>Here we take a step back, and implement forward and backward
propogation manually (so we can fully appreciate the autograd
later...).</p>
</div>
<div class="cell markdown" id="Zlpam3w2QY8w">
<p>First, create numpy arrays for our data (X) and target (Y):</p>
</div>
<div class="cell code" data-execution_count="4"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="S-7cJRcPaMoh" data-outputId="abde6c73-b671-4158-ef6e-611c0cff8891">
<div class="sourceCode" id="cb8"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> train_df_norm.drop([<span class="st">&#39;median_house_value&#39;</span>],axis<span class="op">=</span><span class="dv">1</span>).to_numpy()</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span>  np.c_[  np.ones(X.shape[<span class="dv">0</span>]), X ] <span class="co"># adding a column of &quot;1&quot;s for intercept</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> train_df_norm[<span class="st">&#39;median_house_value&#39;</span>]</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;X dimension: </span><span class="sc">{</span>X<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Y dimension: </span><span class="sc">{</span>Y<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>X dimension: (17000, 7)
Y dimension: (17000,)
</code></pre>
</div>
</div>
<section id="11-calculating-the-weights-analytically-15pts"
class="cell markdown" id="7ZbiF4BCVoTh">
<h3>1.1 Calculating the weights analytically (15pts)</h3>
<p>Apply linear algebra operations to <code>X</code> and <code>Y</code>
to analytically solve the linear regression problem (solve for
<code>W</code>).<br />
Please use only numpy. Round the weights to 3 decimal places.</p>
</section>
<div class="cell code" data-execution_count="5"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="1axJ1nXAWNPB" data-outputId="b24ccba0-aaa1-458e-ae71-35680038e154">
<div class="sourceCode" id="cb10"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co">#### SOLUTION REQUIRED ####</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>t1 <span class="op">=</span> time.time()</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>weights_rounded <span class="op">=</span>  np.<span class="bu">round</span>(np.linalg.inv(X.T <span class="op">@</span> X) <span class="op">@</span> (X.T <span class="op">@</span> Y), <span class="dv">3</span>)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Total process time: </span><span class="sc">{</span><span class="bu">round</span>(time.time() <span class="op">-</span> t1,<span class="dv">3</span>)<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;The estimated weights using the analytic solution:</span><span class="ch">\n</span><span class="ss"> </span><span class="sc">{</span>weights_rounded<span class="sc">.</span><span class="bu">round</span>(<span class="dv">3</span>)<span class="sc">}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Total process time: 0.009
The estimated weights using the analytic solution:
 [ 0.     0.204 -0.375  0.362 -0.347  0.422  0.79 ]
</code></pre>
</div>
</div>
<div class="cell markdown" id="5CJITEi2jsY2">
<p>Visualizing the errors:</p>
</div>
<div class="cell code" data-execution_count="6"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:410}"
id="NjV70_FqbfwL" data-outputId="3ad598f1-c87b-46c4-d676-2795a9daceda">
<div class="sourceCode" id="cb12"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the errors and density (sanity check):</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>fig, (ax1,ax2) <span class="op">=</span> plt.subplots(nrows<span class="op">=</span><span class="dv">1</span>, ncols<span class="op">=</span><span class="dv">2</span>,figsize<span class="op">=</span>(<span class="dv">10</span>,<span class="dv">4</span>))</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>error <span class="op">=</span> (np.matmul( X, weights_rounded)<span class="op">-</span>Y)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>ax1.scatter(<span class="bu">range</span>(Y.shape[<span class="dv">0</span>]), error ,s<span class="op">=</span><span class="fl">0.05</span>,alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>ax1.set_title(<span class="st">&quot;The error of the predictions from the truth &quot;</span>)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>sns.kdeplot(error,ax<span class="op">=</span>ax2)</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>ax2.set_title(<span class="st">&quot;The density of the error&quot;</span>)</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>fig.show()</span></code></pre></div>
<div class="output display_data">
<p><img
src="vertopal_09a10213244446baaa48d168e1875f1e/c0ac4ce265f4ef1ee23a11f5e787354d36c1f1b9.png" /></p>
</div>
</div>
<section
id="12-calculating-the-weights-using-numpy-and-gradient-descent-30pts"
class="cell markdown" id="f7ui5id-YOy1">
<h3>1.2 Calculating the weights using numpy and gradient descent
(30pts)</h3>
</section>
<div class="cell markdown" id="b64HQDd5vfii">
<p>Now you are ready to implement an algorithm that iteratively
calculates the weights using numpy and gradient descent.</p>
<p>Please fill in your code in the <code>### YOUR CODE HERE</code>
sections in the code block below.<br />
Please don't change the other parts of the code (unless specified).</p>
<p>An epoch in machine learning means one complete pass of the training
dataset through the algorithm.</p>
<p>While developing your code, you may reduce the number of
epochs.<br />
However, for final output please use the pre-defined epochs number.</p>
</div>
<div class="cell code" data-execution_count="7"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="ciPD0RXnQih0" data-outputId="e7c923b7-3ddb-479f-9c60-41ac4ddf2f41">
<div class="sourceCode" id="cb13"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co">#### SOLUTION REQUIRED ####</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>t2 <span class="op">=</span> time.time()</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Randomly initialize weights using numpy</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>b0 <span class="op">=</span> np.random.randn()</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>b1 <span class="op">=</span> np.random.randn()</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>b2 <span class="op">=</span> np.random.randn()</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>b3 <span class="op">=</span> np.random.randn()</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>b4 <span class="op">=</span> np.random.randn()</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>b5 <span class="op">=</span> np.random.randn()</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>b6 <span class="op">=</span> np.random.randn()</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Set learning rate (this can be a little fiddly, requires some tunning)</span></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Tip: start with a very small steps...</span></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span>  <span class="fl">0.01</span></span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Set number of epochs:</span></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="bu">int</span>(<span class="fl">5e3</span>)</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Loop through epochs, and apply gradient descent to update the weights:</span></span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Forward pass: compute predicted y</span></span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># y_pred = b0x0 + b1x1 + b2x2 + b3x3 + b4x4 + b5x5 + b6x6</span></span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a>    Y_pred <span class="op">=</span> b0 <span class="op">*</span> X[:, <span class="dv">0</span>] <span class="op">+</span> b1 <span class="op">*</span> X[:, <span class="dv">1</span>] <span class="op">+</span> b2 <span class="op">*</span> X[:, <span class="dv">2</span>] <span class="op">+</span> b3 <span class="op">*</span> X[:, <span class="dv">3</span>] <span class="op">+</span> b4 <span class="op">*</span> X[:, <span class="dv">4</span>] <span class="op">+</span> b5 <span class="op">*</span> X[:, <span class="dv">5</span>] <span class="op">+</span> b6 <span class="op">*</span> X[:, <span class="dv">6</span>]</span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute and print loss</span></span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> np.mean((Y <span class="op">-</span> Y_pred) <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> t <span class="op">%</span> <span class="dv">1000</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f&quot;Epoch:</span><span class="sc">{</span>t<span class="sc">}</span><span class="ss">, loss:</span><span class="sc">{</span>loss<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Backprop to compute gradients of betas with respect to loss</span></span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a>    grad_y_pred <span class="op">=</span> <span class="op">-</span><span class="dv">2</span> <span class="op">*</span> (Y <span class="op">-</span> Y_pred) <span class="op">/</span> <span class="bu">len</span>(Y)</span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a>    grad_b0 <span class="op">=</span> np.<span class="bu">sum</span>(grad_y_pred <span class="op">*</span> X[:, <span class="dv">0</span>])</span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true" tabindex="-1"></a>    grad_b1 <span class="op">=</span> np.<span class="bu">sum</span>(grad_y_pred <span class="op">*</span> X[:, <span class="dv">1</span>])</span>
<span id="cb13-37"><a href="#cb13-37" aria-hidden="true" tabindex="-1"></a>    grad_b2 <span class="op">=</span> np.<span class="bu">sum</span>(grad_y_pred <span class="op">*</span> X[:, <span class="dv">2</span>])</span>
<span id="cb13-38"><a href="#cb13-38" aria-hidden="true" tabindex="-1"></a>    grad_b3 <span class="op">=</span> np.<span class="bu">sum</span>(grad_y_pred <span class="op">*</span> X[:, <span class="dv">3</span>])</span>
<span id="cb13-39"><a href="#cb13-39" aria-hidden="true" tabindex="-1"></a>    grad_b4 <span class="op">=</span> np.<span class="bu">sum</span>(grad_y_pred <span class="op">*</span> X[:, <span class="dv">4</span>])</span>
<span id="cb13-40"><a href="#cb13-40" aria-hidden="true" tabindex="-1"></a>    grad_b5 <span class="op">=</span> np.<span class="bu">sum</span>(grad_y_pred <span class="op">*</span> X[:, <span class="dv">5</span>])</span>
<span id="cb13-41"><a href="#cb13-41" aria-hidden="true" tabindex="-1"></a>    grad_b6 <span class="op">=</span> np.<span class="bu">sum</span>(grad_y_pred <span class="op">*</span> X[:, <span class="dv">6</span>])</span>
<span id="cb13-42"><a href="#cb13-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-43"><a href="#cb13-43" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Update weights</span></span>
<span id="cb13-44"><a href="#cb13-44" aria-hidden="true" tabindex="-1"></a>    b0 <span class="op">-=</span> learning_rate <span class="op">*</span> grad_b0</span>
<span id="cb13-45"><a href="#cb13-45" aria-hidden="true" tabindex="-1"></a>    b1 <span class="op">-=</span> learning_rate <span class="op">*</span> grad_b1</span>
<span id="cb13-46"><a href="#cb13-46" aria-hidden="true" tabindex="-1"></a>    b2 <span class="op">-=</span> learning_rate <span class="op">*</span> grad_b2</span>
<span id="cb13-47"><a href="#cb13-47" aria-hidden="true" tabindex="-1"></a>    b3 <span class="op">-=</span> learning_rate <span class="op">*</span> grad_b3</span>
<span id="cb13-48"><a href="#cb13-48" aria-hidden="true" tabindex="-1"></a>    b4 <span class="op">-=</span> learning_rate <span class="op">*</span> grad_b4</span>
<span id="cb13-49"><a href="#cb13-49" aria-hidden="true" tabindex="-1"></a>    b5 <span class="op">-=</span> learning_rate <span class="op">*</span> grad_b5</span>
<span id="cb13-50"><a href="#cb13-50" aria-hidden="true" tabindex="-1"></a>    b6 <span class="op">-=</span> learning_rate <span class="op">*</span> grad_b6</span>
<span id="cb13-51"><a href="#cb13-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-52"><a href="#cb13-52" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Result: y_hat = </span><span class="sc">{</span>b0<span class="sc">}</span><span class="ss"> x0 + </span><span class="sc">{</span>b1<span class="sc">}</span><span class="ss"> x1 + </span><span class="sc">{</span>b2<span class="sc">}</span><span class="ss"> x2 + </span><span class="sc">{</span>b3<span class="sc">}</span><span class="ss"> x3</span><span class="ch">\n</span><span class="ss"> + </span><span class="sc">{</span>b4<span class="sc">}</span><span class="ss"> x4 + </span><span class="sc">{</span>b5<span class="sc">}</span><span class="ss"> x5 + </span><span class="sc">{</span>b6<span class="sc">}</span><span class="ss"> x6 </span><span class="ch">\n</span><span class="ss">&quot;</span>)</span>
<span id="cb13-53"><a href="#cb13-53" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Total process time: </span><span class="sc">{</span><span class="bu">round</span>(time.time() <span class="op">-</span> t2,<span class="dv">3</span>)<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb13-54"><a href="#cb13-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-55"><a href="#cb13-55" aria-hidden="true" tabindex="-1"></a>weights_gd_rounded <span class="op">=</span> np.array([b0,b1,b2,b3,b4,b5,b6])</span>
<span id="cb13-56"><a href="#cb13-56" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;The estimated weights using the gradient descent solution:</span><span class="ch">\n</span><span class="ss"> </span><span class="sc">{</span>weights_gd_rounded<span class="sc">.</span><span class="bu">round</span>(<span class="dv">3</span>)<span class="sc">}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Epoch:0, loss:6.80191016131367
Epoch:1000, loss:0.4334788613580438
Epoch:2000, loss:0.42897129670776835
Epoch:3000, loss:0.4286567603608499
Epoch:4000, loss:0.4286319681087047
Result: y_hat = 3.5492934524188423e-16 x0 + 0.2041979359259371 x1 + -0.375801165382349 x2 + 0.36974940165792236 x3
 + -0.3449056764114517 x4 + 0.4129239551243659 x5 + 0.7905759571006504 x6 

Total process time: 18.15
The estimated weights using the gradient descent solution:
 [ 0.     0.204 -0.376  0.37  -0.345  0.413  0.791]
</code></pre>
</div>
</div>
<section
id="13-calculating-the-weights-using-pytorch-tensors-and-gradient-descent-25pts"
class="cell markdown" id="QqDJ3haFfRc5">
<h3>1.3 Calculating the weights using PyTorch Tensors and gradient
descent (25pts)</h3>
</section>
<div class="cell markdown" id="oA46IpDHhhtp">
<p>Here you are asked to implement the same manual gradient algorithm as
above.<br />
However, this time please use PyTorch tensors and operators (instead of
numpy).</p>
</div>
<div class="cell code" data-execution_count="8"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="8OzIvT-4grTG" data-outputId="f5255845-7a50-4293-d28c-48a53a8c45ff">
<div class="sourceCode" id="cb15"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co">#### SOLUTION REQUIRED ####</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Creating data and target tensors:</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>tX <span class="op">=</span> torch.tensor(X)</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>tY <span class="op">=</span> torch.tensor(Y)</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>t3 <span class="op">=</span> time.time()</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Randomly initialize weights using PyTorch</span></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>tb0 <span class="op">=</span> torch.randn(<span class="dv">1</span>, requires_grad<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>tb1 <span class="op">=</span> torch.randn(<span class="dv">1</span>, requires_grad<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>tb2 <span class="op">=</span> torch.randn(<span class="dv">1</span>, requires_grad<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>tb3 <span class="op">=</span> torch.randn(<span class="dv">1</span>, requires_grad<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>tb4 <span class="op">=</span> torch.randn(<span class="dv">1</span>, requires_grad<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>tb5 <span class="op">=</span> torch.randn(<span class="dv">1</span>, requires_grad<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>tb6 <span class="op">=</span> torch.randn(<span class="dv">1</span>, requires_grad<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Set learning rate</span></span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Set number of epochs:</span></span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="bu">int</span>(<span class="fl">5e3</span>)</span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Loop through epochs, and apply gradient descent to update the weights:</span></span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Forward pass: compute predicted y</span></span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># y_pred = b0x0 + b1x1 + b2x2 + b3x3 + b4x4 + b5x5 + b6x6</span></span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a>    tY_pred <span class="op">=</span> tb0 <span class="op">*</span> tX[:, <span class="dv">0</span>] <span class="op">+</span> tb1 <span class="op">*</span> tX[:, <span class="dv">1</span>] <span class="op">+</span> tb2 <span class="op">*</span> tX[:, <span class="dv">2</span>] <span class="op">+</span> tb3 <span class="op">*</span> tX[:, <span class="dv">3</span>] <span class="op">+</span> tb4 <span class="op">*</span> tX[:, <span class="dv">4</span>] <span class="op">+</span> tb5 <span class="op">*</span> tX[:, <span class="dv">5</span>] <span class="op">+</span> tb6 <span class="op">*</span> tX[:, <span class="dv">6</span>]</span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute and print loss</span></span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> torch.mean((tY <span class="op">-</span> tY_pred) <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb15-33"><a href="#cb15-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> t <span class="op">%</span> <span class="dv">1000</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb15-34"><a href="#cb15-34" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f&quot;epoch:</span><span class="sc">{</span>t<span class="sc">}</span><span class="ss">, loss:</span><span class="sc">{</span>loss<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb15-35"><a href="#cb15-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-36"><a href="#cb15-36" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Backprop to compute gradients of betas with respect to loss</span></span>
<span id="cb15-37"><a href="#cb15-37" aria-hidden="true" tabindex="-1"></a>    grad_y_pred <span class="op">=</span> <span class="op">-</span><span class="dv">2</span> <span class="op">*</span> (tY <span class="op">-</span> tY_pred) <span class="op">/</span> <span class="bu">len</span>(tY)</span>
<span id="cb15-38"><a href="#cb15-38" aria-hidden="true" tabindex="-1"></a>    <span class="co"># print(grad_y_pred.sum())</span></span>
<span id="cb15-39"><a href="#cb15-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-40"><a href="#cb15-40" aria-hidden="true" tabindex="-1"></a>    grad_b0 <span class="op">=</span> torch.<span class="bu">sum</span>(grad_y_pred <span class="op">*</span> tX[:, <span class="dv">0</span>])</span>
<span id="cb15-41"><a href="#cb15-41" aria-hidden="true" tabindex="-1"></a>    grad_b1 <span class="op">=</span> torch.<span class="bu">sum</span>(grad_y_pred <span class="op">*</span> tX[:, <span class="dv">1</span>])</span>
<span id="cb15-42"><a href="#cb15-42" aria-hidden="true" tabindex="-1"></a>    grad_b2 <span class="op">=</span> torch.<span class="bu">sum</span>(grad_y_pred <span class="op">*</span> tX[:, <span class="dv">2</span>])</span>
<span id="cb15-43"><a href="#cb15-43" aria-hidden="true" tabindex="-1"></a>    grad_b3 <span class="op">=</span> torch.<span class="bu">sum</span>(grad_y_pred <span class="op">*</span> tX[:, <span class="dv">3</span>])</span>
<span id="cb15-44"><a href="#cb15-44" aria-hidden="true" tabindex="-1"></a>    grad_b4 <span class="op">=</span> torch.<span class="bu">sum</span>(grad_y_pred <span class="op">*</span> tX[:, <span class="dv">4</span>])</span>
<span id="cb15-45"><a href="#cb15-45" aria-hidden="true" tabindex="-1"></a>    grad_b5 <span class="op">=</span> torch.<span class="bu">sum</span>(grad_y_pred <span class="op">*</span> tX[:, <span class="dv">5</span>])</span>
<span id="cb15-46"><a href="#cb15-46" aria-hidden="true" tabindex="-1"></a>    grad_b6 <span class="op">=</span> torch.<span class="bu">sum</span>(grad_y_pred <span class="op">*</span> tX[:, <span class="dv">6</span>])</span>
<span id="cb15-47"><a href="#cb15-47" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Update weights</span></span>
<span id="cb15-48"><a href="#cb15-48" aria-hidden="true" tabindex="-1"></a>    tb0 <span class="op">-=</span> learning_rate <span class="op">*</span> grad_b0</span>
<span id="cb15-49"><a href="#cb15-49" aria-hidden="true" tabindex="-1"></a>    tb1 <span class="op">-=</span> learning_rate <span class="op">*</span> grad_b1</span>
<span id="cb15-50"><a href="#cb15-50" aria-hidden="true" tabindex="-1"></a>    tb2 <span class="op">-=</span> learning_rate <span class="op">*</span> grad_b2</span>
<span id="cb15-51"><a href="#cb15-51" aria-hidden="true" tabindex="-1"></a>    tb3 <span class="op">-=</span> learning_rate <span class="op">*</span> grad_b3</span>
<span id="cb15-52"><a href="#cb15-52" aria-hidden="true" tabindex="-1"></a>    tb4 <span class="op">-=</span> learning_rate <span class="op">*</span> grad_b4</span>
<span id="cb15-53"><a href="#cb15-53" aria-hidden="true" tabindex="-1"></a>    tb5 <span class="op">-=</span> learning_rate <span class="op">*</span> grad_b5</span>
<span id="cb15-54"><a href="#cb15-54" aria-hidden="true" tabindex="-1"></a>    tb6 <span class="op">-=</span> learning_rate <span class="op">*</span> grad_b6</span>
<span id="cb15-55"><a href="#cb15-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-56"><a href="#cb15-56" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Result: y_hat = </span><span class="sc">{</span>tb0<span class="sc">}</span><span class="ss"> x0 + </span><span class="sc">{</span>tb1<span class="sc">}</span><span class="ss"> x1 + </span><span class="sc">{</span>tb2<span class="sc">}</span><span class="ss"> x2 + </span><span class="sc">{</span>tb3<span class="sc">}</span><span class="ss"> x3</span><span class="ch">\n</span><span class="ss"> + </span><span class="sc">{</span>tb4<span class="sc">}</span><span class="ss"> x4 + </span><span class="sc">{</span>tb5<span class="sc">}</span><span class="ss"> x5 + </span><span class="sc">{</span>tb6<span class="sc">}</span><span class="ss"> x6 </span><span class="ch">\n</span><span class="ss">&quot;</span>)</span>
<span id="cb15-57"><a href="#cb15-57" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Total process time: </span><span class="sc">{</span><span class="bu">round</span>(time.time() <span class="op">-</span> t3,<span class="dv">3</span>)<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb15-58"><a href="#cb15-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-59"><a href="#cb15-59" aria-hidden="true" tabindex="-1"></a>weights_gd_tensors_rounded <span class="op">=</span> np.array([tb0.item(),tb1.item(),tb2.item(),tb3.item(),</span>
<span id="cb15-60"><a href="#cb15-60" aria-hidden="true" tabindex="-1"></a>                                       tb4.item(),tb5.item(),tb6.item()])</span>
<span id="cb15-61"><a href="#cb15-61" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;The estimated weights using the gradient descent on tensors:</span><span class="ch">\n</span><span class="ss"> </span><span class="sc">{</span>weights_gd_tensors_rounded<span class="sc">.</span><span class="bu">round</span>(<span class="dv">3</span>)<span class="sc">}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>epoch:0, loss:6.9200138704649
epoch:1000, loss:0.43068214420238826
epoch:2000, loss:0.428646628702355
epoch:3000, loss:0.4286327140538701
epoch:4000, loss:0.4286298395090129
Result: y_hat = tensor([3.5385e-16], device=&#39;cuda:0&#39;) x0 + tensor([0.2041], device=&#39;cuda:0&#39;) x1 + tensor([-0.3766], device=&#39;cuda:0&#39;) x2 + tensor([0.3696], device=&#39;cuda:0&#39;) x3
 + tensor([-0.3451], device=&#39;cuda:0&#39;) x4 + tensor([0.4141], device=&#39;cuda:0&#39;) x5 + tensor([0.7907], device=&#39;cuda:0&#39;) x6 

Total process time: 5.027
The estimated weights using the gradient descent on tensors:
 [ 0.     0.204 -0.377  0.37  -0.345  0.414  0.791]
</code></pre>
</div>
</div>
<section id="14-calculating-the-weights-using-pytorch-autograd-15pts"
class="cell markdown" id="tn3kGGIb0LQT">
<h3>1.4 Calculating the weights using PyTorch Autograd! (15pts)</h3>
</section>
<div class="cell markdown" id="oSN0J3UriRwA">
<p>Lastly, you can finally make a full transition to PyTorch autograd
framework.<br />
Please replace your manual gradient calculations with PyTorch's
automatic gradient tracking.<br />
In addition, instead of declaring the parameters separately, we will use
one tensor to represent all of the model's parameters.</p>
</div>
<div class="cell code" data-execution_count="9"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="nF_vyqlO1Ya7" data-outputId="5ccc38ea-a363-4c3c-e611-8ad1d9335aac">
<div class="sourceCode" id="cb17"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># FILL IN THE MISSING CODE</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>t4 <span class="op">=</span> time.time()</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Randomly initialize weights using PyTorch</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="co"># we can use 1x7 tensor for all the weights. Make sure `requires_grad = True`.</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>tb <span class="op">=</span> torch.randn(<span class="dv">7</span>, requires_grad<span class="op">=</span><span class="va">True</span>, dtype<span class="op">=</span>torch.float32)</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Set learning rate</span></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Set number of epochs:</span></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="bu">int</span>(<span class="fl">5e3</span>)</span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Loop through epochs, and apply gradient descent to update the weights:</span></span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Forward pass: compute predicted y</span></span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a>    tY_pred <span class="op">=</span> torch.<span class="bu">sum</span>(tb <span class="op">*</span> tX, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute and print loss</span></span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> torch.mean((tY <span class="op">-</span> tY_pred) <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> t <span class="op">%</span> <span class="dv">1000</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f&quot;epoch:</span><span class="sc">{</span>t<span class="sc">}</span><span class="ss">, loss:</span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Backprop to compute gradients of betas with respect to loss</span></span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-29"><a href="#cb17-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb17-30"><a href="#cb17-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Update weights</span></span>
<span id="cb17-31"><a href="#cb17-31" aria-hidden="true" tabindex="-1"></a>        tb <span class="op">-=</span> learning_rate <span class="op">*</span> tb.grad</span>
<span id="cb17-32"><a href="#cb17-32" aria-hidden="true" tabindex="-1"></a>        tb.grad <span class="op">=</span> <span class="va">None</span> <span class="co"># manually clear gradient after update</span></span>
<span id="cb17-33"><a href="#cb17-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-34"><a href="#cb17-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-35"><a href="#cb17-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Result: betas = </span><span class="sc">{</span>tb<span class="sc">}</span><span class="ss"> </span><span class="ch">\n</span><span class="ss">&quot;</span>)</span>
<span id="cb17-36"><a href="#cb17-36" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Total process time: </span><span class="sc">{</span><span class="bu">round</span>(time.time() <span class="op">-</span> t4,<span class="dv">3</span>)<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb17-37"><a href="#cb17-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-38"><a href="#cb17-38" aria-hidden="true" tabindex="-1"></a>weights_autograd_rounded <span class="op">=</span> tb.cpu().detach().numpy()</span>
<span id="cb17-39"><a href="#cb17-39" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;The estimated weights using the pytorch autograd:</span><span class="ch">\n</span><span class="ss"> </span><span class="sc">{</span>weights_autograd_rounded<span class="sc">.</span><span class="bu">round</span>(<span class="dv">3</span>)<span class="sc">}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>epoch:0, loss:2.875649647964988
epoch:1000, loss:0.44576587835607157
epoch:2000, loss:0.4299995838305064
epoch:3000, loss:0.428872097871489
epoch:4000, loss:0.4287269397918958
Result: betas = tensor([ 3.4699e-16,  2.0349e-01, -3.6434e-01,  3.1854e-01, -3.5431e-01,
         4.6167e-01,  7.8713e-01], device=&#39;cuda:0&#39;, requires_grad=True) 

Total process time: 3.534
The estimated weights using the pytorch autograd:
 [ 0.     0.203 -0.364  0.319 -0.354  0.462  0.787]
</code></pre>
</div>
</div>
<section id="15-conclusion-15pts" class="cell markdown"
id="vVruKYunlbNJ">
<h3>1.5 Conclusion (15pts)</h3>
</section>
<div class="cell markdown" id="teXbcts-lelv">
<ol>
<li>Which method is the most accurate in finding the "true" weights?
Why? Is it always applicabale? Hint - you should think about it from a
statistical theory point of view.</li>
<li>Why did we scale the data ? (answer with respect to the
model/algorithm)</li>
<li>Do you think our model specification produced good results? Why or
why not? Justify your answer.</li>
<li>What can you say about the total process times between methods?</li>
<li>Can you suggest some improvements to the model definition?</li>
<li>Can you suggest some improvements to the solution algorithm?</li>
</ol>
</div>
<div class="cell markdown" id="8x3MTup5lt9B">
<p>YOUR ANSWERS HERE</p>
<ol>
<li><p>pytorch autograd method is usally more accurate due to optimized
gradient computation and GPU support but numpy can preform better in
small datasets where manual tuning is possible, but is challanging for
larger datasets and more challanging problems.</p></li>
<li><p>scaling makes sure all the features are on the same scale,
leading to stabler and faster convergence during gradient descent by
making sure large features dont over infulance and avoiding numerical
instability.</p></li>
<li><p>the model produced pretty good results for linear regression with
the given data,becuase the data relationships were not so complex or
noisy.</p></li>
<li><p>pytorch methods are significantly faster than numpy gradient
descent due to optimizations and GPU support, with autograd being the
fastest.</p></li>
<li><p>improvements could be adding regularization , exploring
non-linear models or interaction terms, and performing more advanced
feature engineering to find complex relationships.</p></li>
<li><p>improving the learning rate, switching to mini-batch gradient
descent, implementing early stopping, and utilizing GPU acceleration
could be more efficient and get better model performance.</p></li>
</ol>
</div>
<div class="cell markdown" id="GVoUSVc2pCxM">
<p>PART 2: Coming soon...</p>
</div>
</body>
</html>
