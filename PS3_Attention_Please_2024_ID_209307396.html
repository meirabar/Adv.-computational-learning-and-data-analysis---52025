<!DOCTYPE html>

<html lang="en">
<head><meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>PS3_Attention_Please_2024_ID_209307396</title><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.10/require.min.js"></script>
<style type="text/css">
    pre { line-height: 125%; }
td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
.highlight .hll { background-color: var(--jp-cell-editor-active-background) }
.highlight { background: var(--jp-cell-editor-background); color: var(--jp-mirror-editor-variable-color) }
.highlight .c { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment */
.highlight .err { color: var(--jp-mirror-editor-error-color) } /* Error */
.highlight .k { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword */
.highlight .o { color: var(--jp-mirror-editor-operator-color); font-weight: bold } /* Operator */
.highlight .p { color: var(--jp-mirror-editor-punctuation-color) } /* Punctuation */
.highlight .ch { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Multiline */
.highlight .cp { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Preproc */
.highlight .cpf { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Single */
.highlight .cs { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Special */
.highlight .kc { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Pseudo */
.highlight .kr { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Type */
.highlight .m { color: var(--jp-mirror-editor-number-color) } /* Literal.Number */
.highlight .s { color: var(--jp-mirror-editor-string-color) } /* Literal.String */
.highlight .ow { color: var(--jp-mirror-editor-operator-color); font-weight: bold } /* Operator.Word */
.highlight .pm { color: var(--jp-mirror-editor-punctuation-color) } /* Punctuation.Marker */
.highlight .w { color: var(--jp-mirror-editor-variable-color) } /* Text.Whitespace */
.highlight .mb { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Bin */
.highlight .mf { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Float */
.highlight .mh { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Hex */
.highlight .mi { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Integer */
.highlight .mo { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Oct */
.highlight .sa { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Affix */
.highlight .sb { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Backtick */
.highlight .sc { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Char */
.highlight .dl { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Delimiter */
.highlight .sd { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Doc */
.highlight .s2 { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Double */
.highlight .se { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Escape */
.highlight .sh { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Heredoc */
.highlight .si { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Interpol */
.highlight .sx { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Other */
.highlight .sr { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Regex */
.highlight .s1 { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Single */
.highlight .ss { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Symbol */
.highlight .il { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Integer.Long */
  </style>
<style type="text/css">
/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*
 * Mozilla scrollbar styling
 */

/* use standard opaque scrollbars for most nodes */
[data-jp-theme-scrollbars='true'] {
  scrollbar-color: rgb(var(--jp-scrollbar-thumb-color))
    var(--jp-scrollbar-background-color);
}

/* for code nodes, use a transparent style of scrollbar. These selectors
 * will match lower in the tree, and so will override the above */
[data-jp-theme-scrollbars='true'] .CodeMirror-hscrollbar,
[data-jp-theme-scrollbars='true'] .CodeMirror-vscrollbar {
  scrollbar-color: rgba(var(--jp-scrollbar-thumb-color), 0.5) transparent;
}

/* tiny scrollbar */

.jp-scrollbar-tiny {
  scrollbar-color: rgba(var(--jp-scrollbar-thumb-color), 0.5) transparent;
  scrollbar-width: thin;
}

/* tiny scrollbar */

.jp-scrollbar-tiny::-webkit-scrollbar,
.jp-scrollbar-tiny::-webkit-scrollbar-corner {
  background-color: transparent;
  height: 4px;
  width: 4px;
}

.jp-scrollbar-tiny::-webkit-scrollbar-thumb {
  background: rgba(var(--jp-scrollbar-thumb-color), 0.5);
}

.jp-scrollbar-tiny::-webkit-scrollbar-track:horizontal {
  border-left: 0 solid transparent;
  border-right: 0 solid transparent;
}

.jp-scrollbar-tiny::-webkit-scrollbar-track:vertical {
  border-top: 0 solid transparent;
  border-bottom: 0 solid transparent;
}

/*
 * Lumino
 */

.lm-ScrollBar[data-orientation='horizontal'] {
  min-height: 16px;
  max-height: 16px;
  min-width: 45px;
  border-top: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='vertical'] {
  min-width: 16px;
  max-width: 16px;
  min-height: 45px;
  border-left: 1px solid #a0a0a0;
}

.lm-ScrollBar-button {
  background-color: #f0f0f0;
  background-position: center center;
  min-height: 15px;
  max-height: 15px;
  min-width: 15px;
  max-width: 15px;
}

.lm-ScrollBar-button:hover {
  background-color: #dadada;
}

.lm-ScrollBar-button.lm-mod-active {
  background-color: #cdcdcd;
}

.lm-ScrollBar-track {
  background: #f0f0f0;
}

.lm-ScrollBar-thumb {
  background: #cdcdcd;
}

.lm-ScrollBar-thumb:hover {
  background: #bababa;
}

.lm-ScrollBar-thumb.lm-mod-active {
  background: #a0a0a0;
}

.lm-ScrollBar[data-orientation='horizontal'] .lm-ScrollBar-thumb {
  height: 100%;
  min-width: 15px;
  border-left: 1px solid #a0a0a0;
  border-right: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='vertical'] .lm-ScrollBar-thumb {
  width: 100%;
  min-height: 15px;
  border-top: 1px solid #a0a0a0;
  border-bottom: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='horizontal']
  .lm-ScrollBar-button[data-action='decrement'] {
  background-image: var(--jp-icon-caret-left);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='horizontal']
  .lm-ScrollBar-button[data-action='increment'] {
  background-image: var(--jp-icon-caret-right);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='vertical']
  .lm-ScrollBar-button[data-action='decrement'] {
  background-image: var(--jp-icon-caret-up);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='vertical']
  .lm-ScrollBar-button[data-action='increment'] {
  background-image: var(--jp-icon-caret-down);
  background-size: 17px;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-Widget {
  box-sizing: border-box;
  position: relative;
  overflow: hidden;
}

.lm-Widget.lm-mod-hidden {
  display: none !important;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.lm-AccordionPanel[data-orientation='horizontal'] > .lm-AccordionPanel-title {
  /* Title is rotated for horizontal accordion panel using CSS */
  display: block;
  transform-origin: top left;
  transform: rotate(-90deg) translate(-100%);
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-CommandPalette {
  display: flex;
  flex-direction: column;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-CommandPalette-search {
  flex: 0 0 auto;
}

.lm-CommandPalette-content {
  flex: 1 1 auto;
  margin: 0;
  padding: 0;
  min-height: 0;
  overflow: auto;
  list-style-type: none;
}

.lm-CommandPalette-header {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.lm-CommandPalette-item {
  display: flex;
  flex-direction: row;
}

.lm-CommandPalette-itemIcon {
  flex: 0 0 auto;
}

.lm-CommandPalette-itemContent {
  flex: 1 1 auto;
  overflow: hidden;
}

.lm-CommandPalette-itemShortcut {
  flex: 0 0 auto;
}

.lm-CommandPalette-itemLabel {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.lm-close-icon {
  border: 1px solid transparent;
  background-color: transparent;
  position: absolute;
  z-index: 1;
  right: 3%;
  top: 0;
  bottom: 0;
  margin: auto;
  padding: 7px 0;
  display: none;
  vertical-align: middle;
  outline: 0;
  cursor: pointer;
}
.lm-close-icon:after {
  content: 'X';
  display: block;
  width: 15px;
  height: 15px;
  text-align: center;
  color: #000;
  font-weight: normal;
  font-size: 12px;
  cursor: pointer;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-DockPanel {
  z-index: 0;
}

.lm-DockPanel-widget {
  z-index: 0;
}

.lm-DockPanel-tabBar {
  z-index: 1;
}

.lm-DockPanel-handle {
  z-index: 2;
}

.lm-DockPanel-handle.lm-mod-hidden {
  display: none !important;
}

.lm-DockPanel-handle:after {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  content: '';
}

.lm-DockPanel-handle[data-orientation='horizontal'] {
  cursor: ew-resize;
}

.lm-DockPanel-handle[data-orientation='vertical'] {
  cursor: ns-resize;
}

.lm-DockPanel-handle[data-orientation='horizontal']:after {
  left: 50%;
  min-width: 8px;
  transform: translateX(-50%);
}

.lm-DockPanel-handle[data-orientation='vertical']:after {
  top: 50%;
  min-height: 8px;
  transform: translateY(-50%);
}

.lm-DockPanel-overlay {
  z-index: 3;
  box-sizing: border-box;
  pointer-events: none;
}

.lm-DockPanel-overlay.lm-mod-hidden {
  display: none !important;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-Menu {
  z-index: 10000;
  position: absolute;
  white-space: nowrap;
  overflow-x: hidden;
  overflow-y: auto;
  outline: none;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-Menu-content {
  margin: 0;
  padding: 0;
  display: table;
  list-style-type: none;
}

.lm-Menu-item {
  display: table-row;
}

.lm-Menu-item.lm-mod-hidden,
.lm-Menu-item.lm-mod-collapsed {
  display: none !important;
}

.lm-Menu-itemIcon,
.lm-Menu-itemSubmenuIcon {
  display: table-cell;
  text-align: center;
}

.lm-Menu-itemLabel {
  display: table-cell;
  text-align: left;
}

.lm-Menu-itemShortcut {
  display: table-cell;
  text-align: right;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-MenuBar {
  outline: none;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-MenuBar-content {
  margin: 0;
  padding: 0;
  display: flex;
  flex-direction: row;
  list-style-type: none;
}

.lm-MenuBar-item {
  box-sizing: border-box;
}

.lm-MenuBar-itemIcon,
.lm-MenuBar-itemLabel {
  display: inline-block;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-ScrollBar {
  display: flex;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-ScrollBar[data-orientation='horizontal'] {
  flex-direction: row;
}

.lm-ScrollBar[data-orientation='vertical'] {
  flex-direction: column;
}

.lm-ScrollBar-button {
  box-sizing: border-box;
  flex: 0 0 auto;
}

.lm-ScrollBar-track {
  box-sizing: border-box;
  position: relative;
  overflow: hidden;
  flex: 1 1 auto;
}

.lm-ScrollBar-thumb {
  box-sizing: border-box;
  position: absolute;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-SplitPanel-child {
  z-index: 0;
}

.lm-SplitPanel-handle {
  z-index: 1;
}

.lm-SplitPanel-handle.lm-mod-hidden {
  display: none !important;
}

.lm-SplitPanel-handle:after {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  content: '';
}

.lm-SplitPanel[data-orientation='horizontal'] > .lm-SplitPanel-handle {
  cursor: ew-resize;
}

.lm-SplitPanel[data-orientation='vertical'] > .lm-SplitPanel-handle {
  cursor: ns-resize;
}

.lm-SplitPanel[data-orientation='horizontal'] > .lm-SplitPanel-handle:after {
  left: 50%;
  min-width: 8px;
  transform: translateX(-50%);
}

.lm-SplitPanel[data-orientation='vertical'] > .lm-SplitPanel-handle:after {
  top: 50%;
  min-height: 8px;
  transform: translateY(-50%);
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-TabBar {
  display: flex;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-TabBar[data-orientation='horizontal'] {
  flex-direction: row;
  align-items: flex-end;
}

.lm-TabBar[data-orientation='vertical'] {
  flex-direction: column;
  align-items: flex-end;
}

.lm-TabBar-content {
  margin: 0;
  padding: 0;
  display: flex;
  flex: 1 1 auto;
  list-style-type: none;
}

.lm-TabBar[data-orientation='horizontal'] > .lm-TabBar-content {
  flex-direction: row;
}

.lm-TabBar[data-orientation='vertical'] > .lm-TabBar-content {
  flex-direction: column;
}

.lm-TabBar-tab {
  display: flex;
  flex-direction: row;
  box-sizing: border-box;
  overflow: hidden;
  touch-action: none; /* Disable native Drag/Drop */
}

.lm-TabBar-tabIcon,
.lm-TabBar-tabCloseIcon {
  flex: 0 0 auto;
}

.lm-TabBar-tabLabel {
  flex: 1 1 auto;
  overflow: hidden;
  white-space: nowrap;
}

.lm-TabBar-tabInput {
  user-select: all;
  width: 100%;
  box-sizing: border-box;
}

.lm-TabBar-tab.lm-mod-hidden {
  display: none !important;
}

.lm-TabBar-addButton.lm-mod-hidden {
  display: none !important;
}

.lm-TabBar.lm-mod-dragging .lm-TabBar-tab {
  position: relative;
}

.lm-TabBar.lm-mod-dragging[data-orientation='horizontal'] .lm-TabBar-tab {
  left: 0;
  transition: left 150ms ease;
}

.lm-TabBar.lm-mod-dragging[data-orientation='vertical'] .lm-TabBar-tab {
  top: 0;
  transition: top 150ms ease;
}

.lm-TabBar.lm-mod-dragging .lm-TabBar-tab.lm-mod-dragging {
  transition: none;
}

.lm-TabBar-tabLabel .lm-TabBar-tabInput {
  user-select: all;
  width: 100%;
  box-sizing: border-box;
  background: inherit;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-TabPanel-tabBar {
  z-index: 1;
}

.lm-TabPanel-stackedPanel {
  z-index: 0;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Collapse {
  display: flex;
  flex-direction: column;
  align-items: stretch;
}

.jp-Collapse-header {
  padding: 1px 12px;
  background-color: var(--jp-layout-color1);
  border-bottom: solid var(--jp-border-width) var(--jp-border-color2);
  color: var(--jp-ui-font-color1);
  cursor: pointer;
  display: flex;
  align-items: center;
  font-size: var(--jp-ui-font-size0);
  font-weight: 600;
  text-transform: uppercase;
  user-select: none;
}

.jp-Collapser-icon {
  height: 16px;
}

.jp-Collapse-header-collapsed .jp-Collapser-icon {
  transform: rotate(-90deg);
  margin: auto 0;
}

.jp-Collapser-title {
  line-height: 25px;
}

.jp-Collapse-contents {
  padding: 0 12px;
  background-color: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  overflow: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* This file was auto-generated by ensureUiComponents() in @jupyterlab/buildutils */

/**
 * (DEPRECATED) Support for consuming icons as CSS background images
 */

/* Icons urls */

:root {
  --jp-icon-add-above: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPGcgY2xpcC1wYXRoPSJ1cmwoI2NsaXAwXzEzN18xOTQ5MikiPgo8cGF0aCBjbGFzcz0ianAtaWNvbjMiIGQ9Ik00Ljc1IDQuOTMwNjZINi42MjVWNi44MDU2NkM2LjYyNSA3LjAxMTkxIDYuNzkzNzUgNy4xODA2NiA3IDcuMTgwNjZDNy4yMDYyNSA3LjE4MDY2IDcuMzc1IDcuMDExOTEgNy4zNzUgNi44MDU2NlY0LjkzMDY2SDkuMjVDOS40NTYyNSA0LjkzMDY2IDkuNjI1IDQuNzYxOTEgOS42MjUgNC41NTU2NkM5LjYyNSA0LjM0OTQxIDkuNDU2MjUgNC4xODA2NiA5LjI1IDQuMTgwNjZINy4zNzVWMi4zMDU2NkM3LjM3NSAyLjA5OTQxIDcuMjA2MjUgMS45MzA2NiA3IDEuOTMwNjZDNi43OTM3NSAxLjkzMDY2IDYuNjI1IDIuMDk5NDEgNi42MjUgMi4zMDU2NlY0LjE4MDY2SDQuNzVDNC41NDM3NSA0LjE4MDY2IDQuMzc1IDQuMzQ5NDEgNC4zNzUgNC41NTU2NkM0LjM3NSA0Ljc2MTkxIDQuNTQzNzUgNC45MzA2NiA0Ljc1IDQuOTMwNjZaIiBmaWxsPSIjNjE2MTYxIiBzdHJva2U9IiM2MTYxNjEiIHN0cm9rZS13aWR0aD0iMC43Ii8+CjwvZz4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiIGNsaXAtcnVsZT0iZXZlbm9kZCIgZD0iTTExLjUgOS41VjExLjVMMi41IDExLjVWOS41TDExLjUgOS41Wk0xMiA4QzEyLjU1MjMgOCAxMyA4LjQ0NzcyIDEzIDlWMTJDMTMgMTIuNTUyMyAxMi41NTIzIDEzIDEyIDEzTDIgMTNDMS40NDc3MiAxMyAxIDEyLjU1MjMgMSAxMlY5QzEgOC40NDc3MiAxLjQ0NzcxIDggMiA4TDEyIDhaIiBmaWxsPSIjNjE2MTYxIi8+CjxkZWZzPgo8Y2xpcFBhdGggaWQ9ImNsaXAwXzEzN18xOTQ5MiI+CjxyZWN0IGNsYXNzPSJqcC1pY29uMyIgd2lkdGg9IjYiIGhlaWdodD0iNiIgZmlsbD0id2hpdGUiIHRyYW5zZm9ybT0ibWF0cml4KC0xIDAgMCAxIDEwIDEuNTU1NjYpIi8+CjwvY2xpcFBhdGg+CjwvZGVmcz4KPC9zdmc+Cg==);
  --jp-icon-add-below: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPGcgY2xpcC1wYXRoPSJ1cmwoI2NsaXAwXzEzN18xOTQ5OCkiPgo8cGF0aCBjbGFzcz0ianAtaWNvbjMiIGQ9Ik05LjI1IDEwLjA2OTNMNy4zNzUgMTAuMDY5M0w3LjM3NSA4LjE5NDM0QzcuMzc1IDcuOTg4MDkgNy4yMDYyNSA3LjgxOTM0IDcgNy44MTkzNEM2Ljc5Mzc1IDcuODE5MzQgNi42MjUgNy45ODgwOSA2LjYyNSA4LjE5NDM0TDYuNjI1IDEwLjA2OTNMNC43NSAxMC4wNjkzQzQuNTQzNzUgMTAuMDY5MyA0LjM3NSAxMC4yMzgxIDQuMzc1IDEwLjQ0NDNDNC4zNzUgMTAuNjUwNiA0LjU0Mzc1IDEwLjgxOTMgNC43NSAxMC44MTkzTDYuNjI1IDEwLjgxOTNMNi42MjUgMTIuNjk0M0M2LjYyNSAxMi45MDA2IDYuNzkzNzUgMTMuMDY5MyA3IDEzLjA2OTNDNy4yMDYyNSAxMy4wNjkzIDcuMzc1IDEyLjkwMDYgNy4zNzUgMTIuNjk0M0w3LjM3NSAxMC44MTkzTDkuMjUgMTAuODE5M0M5LjQ1NjI1IDEwLjgxOTMgOS42MjUgMTAuNjUwNiA5LjYyNSAxMC40NDQzQzkuNjI1IDEwLjIzODEgOS40NTYyNSAxMC4wNjkzIDkuMjUgMTAuMDY5M1oiIGZpbGw9IiM2MTYxNjEiIHN0cm9rZT0iIzYxNjE2MSIgc3Ryb2tlLXdpZHRoPSIwLjciLz4KPC9nPgo8cGF0aCBjbGFzcz0ianAtaWNvbjMiIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMi41IDUuNUwyLjUgMy41TDExLjUgMy41TDExLjUgNS41TDIuNSA1LjVaTTIgN0MxLjQ0NzcyIDcgMSA2LjU1MjI4IDEgNkwxIDNDMSAyLjQ0NzcyIDEuNDQ3NzIgMiAyIDJMMTIgMkMxMi41NTIzIDIgMTMgMi40NDc3MiAxMyAzTDEzIDZDMTMgNi41NTIyOSAxMi41NTIzIDcgMTIgN0wyIDdaIiBmaWxsPSIjNjE2MTYxIi8+CjxkZWZzPgo8Y2xpcFBhdGggaWQ9ImNsaXAwXzEzN18xOTQ5OCI+CjxyZWN0IGNsYXNzPSJqcC1pY29uMyIgd2lkdGg9IjYiIGhlaWdodD0iNiIgZmlsbD0id2hpdGUiIHRyYW5zZm9ybT0ibWF0cml4KDEgMS43NDg0NmUtMDcgMS43NDg0NmUtMDcgLTEgNCAxMy40NDQzKSIvPgo8L2NsaXBQYXRoPgo8L2RlZnM+Cjwvc3ZnPgo=);
  --jp-icon-add: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE5IDEzaC02djZoLTJ2LTZINXYtMmg2VjVoMnY2aDZ2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-bell: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE2IDE2IiB2ZXJzaW9uPSIxLjEiPgogICA8cGF0aCBjbGFzcz0ianAtaWNvbjIganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMzMzMzMzIgogICAgICBkPSJtOCAwLjI5Yy0xLjQgMC0yLjcgMC43My0zLjYgMS44LTEuMiAxLjUtMS40IDMuNC0xLjUgNS4yLTAuMTggMi4yLTAuNDQgNC0yLjMgNS4zbDAuMjggMS4zaDVjMC4wMjYgMC42NiAwLjMyIDEuMSAwLjcxIDEuNSAwLjg0IDAuNjEgMiAwLjYxIDIuOCAwIDAuNTItMC40IDAuNi0xIDAuNzEtMS41aDVsMC4yOC0xLjNjLTEuOS0wLjk3LTIuMi0zLjMtMi4zLTUuMy0wLjEzLTEuOC0wLjI2LTMuNy0xLjUtNS4yLTAuODUtMS0yLjItMS44LTMuNi0xLjh6bTAgMS40YzAuODggMCAxLjkgMC41NSAyLjUgMS4zIDAuODggMS4xIDEuMSAyLjcgMS4yIDQuNCAwLjEzIDEuNyAwLjIzIDMuNiAxLjMgNS4yaC0xMGMxLjEtMS42IDEuMi0zLjQgMS4zLTUuMiAwLjEzLTEuNyAwLjMtMy4zIDEuMi00LjQgMC41OS0wLjcyIDEuNi0xLjMgMi41LTEuM3ptLTAuNzQgMTJoMS41Yy0wLjAwMTUgMC4yOCAwLjAxNSAwLjc5LTAuNzQgMC43OS0wLjczIDAuMDAxNi0wLjcyLTAuNTMtMC43NC0wLjc5eiIgLz4KPC9zdmc+Cg==);
  --jp-icon-bug-dot: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiPgogICAgICAgIDxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMTcuMTkgOEgyMFYxMEgxNy45MUMxNy45NiAxMC4zMyAxOCAxMC42NiAxOCAxMVYxMkgyMFYxNEgxOC41SDE4VjE0LjAyNzVDMTUuNzUgMTQuMjc2MiAxNCAxNi4xODM3IDE0IDE4LjVDMTQgMTkuMjA4IDE0LjE2MzUgMTkuODc3OSAxNC40NTQ5IDIwLjQ3MzlDMTMuNzA2MyAyMC44MTE3IDEyLjg3NTcgMjEgMTIgMjFDOS43OCAyMSA3Ljg1IDE5Ljc5IDYuODEgMThINFYxNkg2LjA5QzYuMDQgMTUuNjcgNiAxNS4zNCA2IDE1VjE0SDRWMTJINlYxMUM2IDEwLjY2IDYuMDQgMTAuMzMgNi4wOSAxMEg0VjhINi44MUM3LjI2IDcuMjIgNy44OCA2LjU1IDguNjIgNi4wNEw3IDQuNDFMOC40MSAzTDEwLjU5IDUuMTdDMTEuMDQgNS4wNiAxMS41MSA1IDEyIDVDMTIuNDkgNSAxMi45NiA1LjA2IDEzLjQyIDUuMTdMMTUuNTkgM0wxNyA0LjQxTDE1LjM3IDYuMDRDMTYuMTIgNi41NSAxNi43NCA3LjIyIDE3LjE5IDhaTTEwIDE2SDE0VjE0SDEwVjE2Wk0xMCAxMkgxNFYxMEgxMFYxMloiIGZpbGw9IiM2MTYxNjEiLz4KICAgICAgICA8cGF0aCBkPSJNMjIgMTguNUMyMiAyMC40MzMgMjAuNDMzIDIyIDE4LjUgMjJDMTYuNTY3IDIyIDE1IDIwLjQzMyAxNSAxOC41QzE1IDE2LjU2NyAxNi41NjcgMTUgMTguNSAxNUMyMC40MzMgMTUgMjIgMTYuNTY3IDIyIDE4LjVaIiBmaWxsPSIjNjE2MTYxIi8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-bug: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik0yMCA4aC0yLjgxYy0uNDUtLjc4LTEuMDctMS40NS0xLjgyLTEuOTZMMTcgNC40MSAxNS41OSAzbC0yLjE3IDIuMTdDMTIuOTYgNS4wNiAxMi40OSA1IDEyIDVjLS40OSAwLS45Ni4wNi0xLjQxLjE3TDguNDEgMyA3IDQuNDFsMS42MiAxLjYzQzcuODggNi41NSA3LjI2IDcuMjIgNi44MSA4SDR2MmgyLjA5Yy0uMDUuMzMtLjA5LjY2LS4wOSAxdjFINHYyaDJ2MWMwIC4zNC4wNC42Ny4wOSAxSDR2MmgyLjgxYzEuMDQgMS43OSAyLjk3IDMgNS4xOSAzczQuMTUtMS4yMSA1LjE5LTNIMjB2LTJoLTIuMDljLjA1LS4zMy4wOS0uNjYuMDktMXYtMWgydi0yaC0ydi0xYzAtLjM0LS4wNC0uNjctLjA5LTFIMjBWOHptLTYgOGgtNHYtMmg0djJ6bTAtNGgtNHYtMmg0djJ6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-build: url(data:image/svg+xml;base64,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);
  --jp-icon-caret-down-empty-thin: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwb2x5Z29uIGNsYXNzPSJzdDEiIHBvaW50cz0iOS45LDEzLjYgMy42LDcuNCA0LjQsNi42IDkuOSwxMi4yIDE1LjQsNi43IDE2LjEsNy40ICIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-caret-down-empty: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik01LjIsNS45TDksOS43bDMuOC0zLjhsMS4yLDEuMmwtNC45LDVsLTQuOS01TDUuMiw1Ljl6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-caret-down: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik01LjIsNy41TDksMTEuMmwzLjgtMy44SDUuMnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-caret-left: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwYXRoIGQ9Ik0xMC44LDEyLjhMNy4xLDlsMy44LTMuOGwwLDcuNkgxMC44eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-caret-right: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik03LjIsNS4yTDEwLjksOWwtMy44LDMuOFY1LjJINy4yeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-caret-up-empty-thin: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwb2x5Z29uIGNsYXNzPSJzdDEiIHBvaW50cz0iMTUuNCwxMy4zIDkuOSw3LjcgNC40LDEzLjIgMy42LDEyLjUgOS45LDYuMyAxNi4xLDEyLjYgIi8+Cgk8L2c+Cjwvc3ZnPgo=);
  --jp-icon-caret-up: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwYXRoIGQ9Ik01LjIsMTAuNUw5LDYuOGwzLjgsMy44SDUuMnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-case-sensitive: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KICA8ZyBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiM0MTQxNDEiPgogICAgPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjE2IiBoZWlnaHQ9IjE2Ii8+CiAgPC9nPgogIDxnIGNsYXNzPSJqcC1pY29uLWFjY2VudDIiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTcuNiw4aDAuOWwzLjUsOGgtMS4xTDEwLDE0SDZsLTAuOSwySDRMNy42LDh6IE04LDkuMUw2LjQsMTNoMy4yTDgsOS4xeiIvPgogICAgPHBhdGggZD0iTTE2LjYsOS44Yy0wLjIsMC4xLTAuNCwwLjEtMC43LDAuMWMtMC4yLDAtMC40LTAuMS0wLjYtMC4yYy0wLjEtMC4xLTAuMi0wLjQtMC4yLTAuNyBjLTAuMywwLjMtMC42LDAuNS0wLjksMC43Yy0wLjMsMC4xLTAuNywwLjItMS4xLDAuMmMtMC4zLDAtMC41LDAtMC43LTAuMWMtMC4yLTAuMS0wLjQtMC4yLTAuNi0wLjNjLTAuMi0wLjEtMC4zLTAuMy0wLjQtMC41IGMtMC4xLTAuMi0wLjEtMC40LTAuMS0wLjdjMC0wLjMsMC4xLTAuNiwwLjItMC44YzAuMS0wLjIsMC4zLTAuNCwwLjQtMC41QzEyLDcsMTIuMiw2LjksMTIuNSw2LjhjMC4yLTAuMSwwLjUtMC4xLDAuNy0wLjIgYzAuMy0wLjEsMC41LTAuMSwwLjctMC4xYzAuMiwwLDAuNC0wLjEsMC42LTAuMWMwLjIsMCwwLjMtMC4xLDAuNC0wLjJjMC4xLTAuMSwwLjItMC4yLDAuMi0wLjRjMC0xLTEuMS0xLTEuMy0xIGMtMC40LDAtMS40LDAtMS40LDEuMmgtMC45YzAtMC40LDAuMS0wLjcsMC4yLTFjMC4xLTAuMiwwLjMtMC40LDAuNS0wLjZjMC4yLTAuMiwwLjUtMC4zLDAuOC0wLjNDMTMuMyw0LDEzLjYsNCwxMy45LDQgYzAuMywwLDAuNSwwLDAuOCwwLjFjMC4zLDAsMC41LDAuMSwwLjcsMC4yYzAuMiwwLjEsMC40LDAuMywwLjUsMC41QzE2LDUsMTYsNS4yLDE2LDUuNnYyLjljMCwwLjIsMCwwLjQsMCwwLjUgYzAsMC4xLDAuMSwwLjIsMC4zLDAuMmMwLjEsMCwwLjIsMCwwLjMsMFY5Ljh6IE0xNS4yLDYuOWMtMS4yLDAuNi0zLjEsMC4yLTMuMSwxLjRjMCwxLjQsMy4xLDEsMy4xLTAuNVY2Ljl6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-check: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik05IDE2LjE3TDQuODMgMTJsLTEuNDIgMS40MUw5IDE5IDIxIDdsLTEuNDEtMS40MXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-circle-empty: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyIDJDNi40NyAyIDIgNi40NyAyIDEyczQuNDcgMTAgMTAgMTAgMTAtNC40NyAxMC0xMFMxNy41MyAyIDEyIDJ6bTAgMThjLTQuNDEgMC04LTMuNTktOC04czMuNTktOCA4LTggOCAzLjU5IDggOC0zLjU5IDgtOCA4eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-circle: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPGNpcmNsZSBjeD0iOSIgY3k9IjkiIHI9IjgiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-clear: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8bWFzayBpZD0iZG9udXRIb2xlIj4KICAgIDxyZWN0IHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgZmlsbD0id2hpdGUiIC8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSI4IiBmaWxsPSJibGFjayIvPgogIDwvbWFzaz4KCiAgPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxyZWN0IGhlaWdodD0iMTgiIHdpZHRoPSIyIiB4PSIxMSIgeT0iMyIgdHJhbnNmb3JtPSJyb3RhdGUoMzE1LCAxMiwgMTIpIi8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSIxMCIgbWFzaz0idXJsKCNkb251dEhvbGUpIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-close: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1ub25lIGpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIGpwLWljb24zLWhvdmVyIiBmaWxsPSJub25lIj4KICAgIDxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjExIi8+CiAgPC9nPgoKICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIGpwLWljb24tYWNjZW50Mi1ob3ZlciIgZmlsbD0iIzYxNjE2MSI+CiAgICA8cGF0aCBkPSJNMTkgNi40MUwxNy41OSA1IDEyIDEwLjU5IDYuNDEgNSA1IDYuNDEgMTAuNTkgMTIgNSAxNy41OSA2LjQxIDE5IDEyIDEzLjQxIDE3LjU5IDE5IDE5IDE3LjU5IDEzLjQxIDEyeiIvPgogIDwvZz4KCiAgPGcgY2xhc3M9ImpwLWljb24tbm9uZSBqcC1pY29uLWJ1c3kiIGZpbGw9Im5vbmUiPgogICAgPGNpcmNsZSBjeD0iMTIiIGN5PSIxMiIgcj0iNyIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-code-check: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBzaGFwZS1yZW5kZXJpbmc9Imdlb21ldHJpY1ByZWNpc2lvbiI+CiAgICA8cGF0aCBkPSJNNi41OSwzLjQxTDIsOEw2LjU5LDEyLjZMOCwxMS4xOEw0LjgyLDhMOCw0LjgyTDYuNTksMy40MU0xMi40MSwzLjQxTDExLDQuODJMMTQuMTgsOEwxMSwxMS4xOEwxMi40MSwxMi42TDE3LDhMMTIuNDEsMy40MU0yMS41OSwxMS41OUwxMy41LDE5LjY4TDkuODMsMTZMOC40MiwxNy40MUwxMy41LDIyLjVMMjMsMTNMMjEuNTksMTEuNTlaIiAvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-code: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIiIGhlaWdodD0iMjIiIHZpZXdCb3g9IjAgMCAyOCAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTExLjQgMTguNkw2LjggMTRMMTEuNCA5LjRMMTAgOEw0IDE0TDEwIDIwTDExLjQgMTguNlpNMTYuNiAxOC42TDIxLjIgMTRMMTYuNiA5LjRMMTggOEwyNCAxNEwxOCAyMEwxNi42IDE4LjZWMTguNloiLz4KCTwvZz4KPC9zdmc+Cg==);
  --jp-icon-collapse-all: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTggMmMxIDAgMTEgMCAxMiAwczIgMSAyIDJjMCAxIDAgMTEgMCAxMnMwIDItMiAyQzIwIDE0IDIwIDQgMjAgNFMxMCA0IDYgNGMwLTIgMS0yIDItMnoiIC8+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTE4IDhjMC0xLTEtMi0yLTJTNSA2IDQgNnMtMiAxLTIgMmMwIDEgMCAxMSAwIDEyczEgMiAyIDJjMSAwIDExIDAgMTIgMHMyLTEgMi0yYzAtMSAwLTExIDAtMTJ6bS0yIDB2MTJINFY4eiIgLz4KICAgICAgICA8cGF0aCBkPSJNNiAxM3YyaDh2LTJ6IiAvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-console: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwMCAyMDAiPgogIDxnIGNsYXNzPSJqcC1jb25zb2xlLWljb24tYmFja2dyb3VuZC1jb2xvciBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiMwMjg4RDEiPgogICAgPHBhdGggZD0iTTIwIDE5LjhoMTYwdjE1OS45SDIweiIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtY29uc29sZS1pY29uLWNvbG9yIGpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIiBmaWxsPSIjZmZmIj4KICAgIDxwYXRoIGQ9Ik0xMDUgMTI3LjNoNDB2MTIuOGgtNDB6TTUxLjEgNzdMNzQgOTkuOWwtMjMuMyAyMy4zIDEwLjUgMTAuNSAyMy4zLTIzLjNMOTUgOTkuOSA4NC41IDg5LjQgNjEuNiA2Ni41eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-copy: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTExLjksMUgzLjJDMi40LDEsMS43LDEuNywxLjcsMi41djEwLjJoMS41VjIuNWg4LjdWMXogTTE0LjEsMy45aC04Yy0wLjgsMC0xLjUsMC43LTEuNSwxLjV2MTAuMmMwLDAuOCwwLjcsMS41LDEuNSwxLjVoOCBjMC44LDAsMS41LTAuNywxLjUtMS41VjUuNEMxNS41LDQuNiwxNC45LDMuOSwxNC4xLDMuOXogTTE0LjEsMTUuNWgtOFY1LjRoOFYxNS41eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-copyright: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGVuYWJsZS1iYWNrZ3JvdW5kPSJuZXcgMCAwIDI0IDI0IiBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCI+CiAgPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik0xMS44OCw5LjE0YzEuMjgsMC4wNiwxLjYxLDEuMTUsMS42MywxLjY2aDEuNzljLTAuMDgtMS45OC0xLjQ5LTMuMTktMy40NS0zLjE5QzkuNjQsNy42MSw4LDksOCwxMi4xNCBjMCwxLjk0LDAuOTMsNC4yNCwzLjg0LDQuMjRjMi4yMiwwLDMuNDEtMS42NSwzLjQ0LTIuOTVoLTEuNzljLTAuMDMsMC41OS0wLjQ1LDEuMzgtMS42MywxLjQ0QzEwLjU1LDE0LjgzLDEwLDEzLjgxLDEwLDEyLjE0IEMxMCw5LjI1LDExLjI4LDkuMTYsMTEuODgsOS4xNHogTTEyLDJDNi40OCwyLDIsNi40OCwyLDEyczQuNDgsMTAsMTAsMTBzMTAtNC40OCwxMC0xMFMxNy41MiwyLDEyLDJ6IE0xMiwyMGMtNC40MSwwLTgtMy41OS04LTggczMuNTktOCw4LThzOCwzLjU5LDgsOFMxNi40MSwyMCwxMiwyMHoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-cut: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkuNjQgNy42NGMuMjMtLjUuMzYtMS4wNS4zNi0xLjY0IDAtMi4yMS0xLjc5LTQtNC00UzIgMy43OSAyIDZzMS43OSA0IDQgNGMuNTkgMCAxLjE0LS4xMyAxLjY0LS4zNkwxMCAxMmwtMi4zNiAyLjM2QzcuMTQgMTQuMTMgNi41OSAxNCA2IDE0Yy0yLjIxIDAtNCAxLjc5LTQgNHMxLjc5IDQgNCA0IDQtMS43OSA0LTRjMC0uNTktLjEzLTEuMTQtLjM2LTEuNjRMMTIgMTRsNyA3aDN2LTFMOS42NCA3LjY0ek02IDhjLTEuMSAwLTItLjg5LTItMnMuOS0yIDItMiAyIC44OSAyIDItLjkgMi0yIDJ6bTAgMTJjLTEuMSAwLTItLjg5LTItMnMuOS0yIDItMiAyIC44OSAyIDItLjkgMi0yIDJ6bTYtNy41Yy0uMjggMC0uNS0uMjItLjUtLjVzLjIyLS41LjUtLjUuNS4yMi41LjUtLjIyLjUtLjUuNXpNMTkgM2wtNiA2IDIgMiA3LTdWM3oiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-delete: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjE2cHgiIGhlaWdodD0iMTZweCI+CiAgICA8cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIiAvPgogICAgPHBhdGggY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjI2MjYyIiBkPSJNNiAxOWMwIDEuMS45IDIgMiAyaDhjMS4xIDAgMi0uOSAyLTJWN0g2djEyek0xOSA0aC0zLjVsLTEtMWgtNWwtMSAxSDV2MmgxNFY0eiIgLz4KPC9zdmc+Cg==);
  --jp-icon-download: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE5IDloLTRWM0g5djZINWw3IDcgNy03ek01IDE4djJoMTR2LTJINXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-duplicate: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiIGNsaXAtcnVsZT0iZXZlbm9kZCIgZD0iTTIuNzk5OTggMC44NzVIOC44OTU4MkM5LjIwMDYxIDAuODc1IDkuNDQ5OTggMS4xMzkxNCA5LjQ0OTk4IDEuNDYxOThDOS40NDk5OCAxLjc4NDgyIDkuMjAwNjEgMi4wNDg5NiA4Ljg5NTgyIDIuMDQ4OTZIMy4zNTQxNUMzLjA0OTM2IDIuMDQ4OTYgMi43OTk5OCAyLjMxMzEgMi43OTk5OCAyLjYzNTk0VjkuNjc5NjlDMi43OTk5OCAxMC4wMDI1IDIuNTUwNjEgMTAuMjY2NyAyLjI0NTgyIDEwLjI2NjdDMS45NDEwMyAxMC4yNjY3IDEuNjkxNjUgMTAuMDAyNSAxLjY5MTY1IDkuNjc5NjlWMi4wNDg5NkMxLjY5MTY1IDEuNDAzMjggMi4xOTA0IDAuODc1IDIuNzk5OTggMC44NzVaTTUuMzY2NjUgMTEuOVY0LjU1SDExLjA4MzNWMTEuOUg1LjM2NjY1Wk00LjE0MTY1IDQuMTQxNjdDNC4xNDE2NSAzLjY5MDYzIDQuNTA3MjggMy4zMjUgNC45NTgzMiAzLjMyNUgxMS40OTE3QzExLjk0MjcgMy4zMjUgMTIuMzA4MyAzLjY5MDYzIDEyLjMwODMgNC4xNDE2N1YxMi4zMDgzQzEyLjMwODMgMTIuNzU5NCAxMS45NDI3IDEzLjEyNSAxMS40OTE3IDEzLjEyNUg0Ljk1ODMyQzQuNTA3MjggMTMuMTI1IDQuMTQxNjUgMTIuNzU5NCA0LjE0MTY1IDEyLjMwODNWNC4xNDE2N1oiIGZpbGw9IiM2MTYxNjEiLz4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBkPSJNOS40MzU3NCA4LjI2NTA3SDguMzY0MzFWOS4zMzY1QzguMzY0MzEgOS40NTQzNSA4LjI2Nzg4IDkuNTUwNzggOC4xNTAwMiA5LjU1MDc4QzguMDMyMTcgOS41NTA3OCA3LjkzNTc0IDkuNDU0MzUgNy45MzU3NCA5LjMzNjVWOC4yNjUwN0g2Ljg2NDMxQzYuNzQ2NDUgOC4yNjUwNyA2LjY1MDAyIDguMTY4NjQgNi42NTAwMiA4LjA1MDc4QzYuNjUwMDIgNy45MzI5MiA2Ljc0NjQ1IDcuODM2NSA2Ljg2NDMxIDcuODM2NUg3LjkzNTc0VjYuNzY1MDdDNy45MzU3NCA2LjY0NzIxIDguMDMyMTcgNi41NTA3OCA4LjE1MDAyIDYuNTUwNzhDOC4yNjc4OCA2LjU1MDc4IDguMzY0MzEgNi42NDcyMSA4LjM2NDMxIDYuNzY1MDdWNy44MzY1SDkuNDM1NzRDOS41NTM2IDcuODM2NSA5LjY1MDAyIDcuOTMyOTIgOS42NTAwMiA4LjA1MDc4QzkuNjUwMDIgOC4xNjg2NCA5LjU1MzYgOC4yNjUwNyA5LjQzNTc0IDguMjY1MDdaIiBmaWxsPSIjNjE2MTYxIiBzdHJva2U9IiM2MTYxNjEiIHN0cm9rZS13aWR0aD0iMC41Ii8+Cjwvc3ZnPgo=);
  --jp-icon-edit: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTMgMTcuMjVWMjFoMy43NUwxNy44MSA5Ljk0bC0zLjc1LTMuNzVMMyAxNy4yNXpNMjAuNzEgNy4wNGMuMzktLjM5LjM5LTEuMDIgMC0xLjQxbC0yLjM0LTIuMzRjLS4zOS0uMzktMS4wMi0uMzktMS40MSAwbC0xLjgzIDEuODMgMy43NSAzLjc1IDEuODMtMS44M3oiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-ellipses: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPGNpcmNsZSBjeD0iNSIgY3k9IjEyIiByPSIyIi8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSIyIi8+CiAgICA8Y2lyY2xlIGN4PSIxOSIgY3k9IjEyIiByPSIyIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-error: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj48Y2lyY2xlIGN4PSIxMiIgY3k9IjE5IiByPSIyIi8+PHBhdGggZD0iTTEwIDNoNHYxMmgtNHoiLz48L2c+CjxwYXRoIGZpbGw9Im5vbmUiIGQ9Ik0wIDBoMjR2MjRIMHoiLz4KPC9zdmc+Cg==);
  --jp-icon-expand-all: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTggMmMxIDAgMTEgMCAxMiAwczIgMSAyIDJjMCAxIDAgMTEgMCAxMnMwIDItMiAyQzIwIDE0IDIwIDQgMjAgNFMxMCA0IDYgNGMwLTIgMS0yIDItMnoiIC8+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTE4IDhjMC0xLTEtMi0yLTJTNSA2IDQgNnMtMiAxLTIgMmMwIDEgMCAxMSAwIDEyczEgMiAyIDJjMSAwIDExIDAgMTIgMHMyLTEgMi0yYzAtMSAwLTExIDAtMTJ6bS0yIDB2MTJINFY4eiIgLz4KICAgICAgICA8cGF0aCBkPSJNMTEgMTBIOXYzSDZ2MmgzdjNoMnYtM2gzdi0yaC0zeiIgLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-extension: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwLjUgMTFIMTlWN2MwLTEuMS0uOS0yLTItMmgtNFYzLjVDMTMgMi4xMiAxMS44OCAxIDEwLjUgMVM4IDIuMTIgOCAzLjVWNUg0Yy0xLjEgMC0xLjk5LjktMS45OSAydjMuOEgzLjVjMS40OSAwIDIuNyAxLjIxIDIuNyAyLjdzLTEuMjEgMi43LTIuNyAyLjdIMlYyMGMwIDEuMS45IDIgMiAyaDMuOHYtMS41YzAtMS40OSAxLjIxLTIuNyAyLjctMi43IDEuNDkgMCAyLjcgMS4yMSAyLjcgMi43VjIySDE3YzEuMSAwIDItLjkgMi0ydi00aDEuNWMxLjM4IDAgMi41LTEuMTIgMi41LTIuNVMyMS44OCAxMSAyMC41IDExeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-fast-forward: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTQgMThsOC41LTZMNCA2djEyem05LTEydjEybDguNS02TDEzIDZ6Ii8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-file-upload: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkgMTZoNnYtNmg0bC03LTctNyA3aDR6bS00IDJoMTR2Mkg1eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-file: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkuMyA4LjJsLTUuNS01LjVjLS4zLS4zLS43LS41LTEuMi0uNUgzLjljLS44LjEtMS42LjktMS42IDEuOHYxNC4xYzAgLjkuNyAxLjYgMS42IDEuNmgxNC4yYy45IDAgMS42LS43IDEuNi0xLjZWOS40Yy4xLS41LS4xLS45LS40LTEuMnptLTUuOC0zLjNsMy40IDMuNmgtMy40VjQuOXptMy45IDEyLjdINC43Yy0uMSAwLS4yIDAtLjItLjJWNC43YzAtLjIuMS0uMy4yLS4zaDcuMnY0LjRzMCAuOC4zIDEuMWMuMy4zIDEuMS4zIDEuMS4zaDQuM3Y3LjJzLS4xLjItLjIuMnoiLz4KPC9zdmc+Cg==);
  --jp-icon-filter-dot: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTE0LDEyVjE5Ljg4QzE0LjA0LDIwLjE4IDEzLjk0LDIwLjUgMTMuNzEsMjAuNzFDMTMuMzIsMjEuMSAxMi42OSwyMS4xIDEyLjMsMjAuNzFMMTAuMjksMTguN0MxMC4wNiwxOC40NyA5Ljk2LDE4LjE2IDEwLDE3Ljg3VjEySDkuOTdMNC4yMSw0LjYyQzMuODcsNC4xOSAzLjk1LDMuNTYgNC4zOCwzLjIyQzQuNTcsMy4wOCA0Ljc4LDMgNSwzVjNIMTlWM0MxOS4yMiwzIDE5LjQzLDMuMDggMTkuNjIsMy4yMkMyMC4wNSwzLjU2IDIwLjEzLDQuMTkgMTkuNzksNC42MkwxNC4wMywxMkgxNFoiIC8+CiAgPC9nPgogIDxnIGNsYXNzPSJqcC1pY29uLWRvdCIgZmlsbD0iI0ZGRiI+CiAgICA8Y2lyY2xlIGN4PSIxOCIgY3k9IjE3IiByPSIzIj48L2NpcmNsZT4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-filter-list: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEwIDE4aDR2LTJoLTR2MnpNMyA2djJoMThWNkgzem0zIDdoMTJ2LTJINnYyeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-filter: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTE0LDEyVjE5Ljg4QzE0LjA0LDIwLjE4IDEzLjk0LDIwLjUgMTMuNzEsMjAuNzFDMTMuMzIsMjEuMSAxMi42OSwyMS4xIDEyLjMsMjAuNzFMMTAuMjksMTguN0MxMC4wNiwxOC40NyA5Ljk2LDE4LjE2IDEwLDE3Ljg3VjEySDkuOTdMNC4yMSw0LjYyQzMuODcsNC4xOSAzLjk1LDMuNTYgNC4zOCwzLjIyQzQuNTcsMy4wOCA0Ljc4LDMgNSwzVjNIMTlWM0MxOS4yMiwzIDE5LjQzLDMuMDggMTkuNjIsMy4yMkMyMC4wNSwzLjU2IDIwLjEzLDQuMTkgMTkuNzksNC42MkwxNC4wMywxMkgxNFoiIC8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-folder-favorite: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjRweCIgdmlld0JveD0iMCAwIDI0IDI0IiB3aWR0aD0iMjRweCIgZmlsbD0iIzAwMDAwMCI+CiAgPHBhdGggZD0iTTAgMGgyNHYyNEgwVjB6IiBmaWxsPSJub25lIi8+PHBhdGggY2xhc3M9ImpwLWljb24zIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzYxNjE2MSIgZD0iTTIwIDZoLThsLTItMkg0Yy0xLjEgMC0yIC45LTIgMnYxMmMwIDEuMS45IDIgMiAyaDE2YzEuMSAwIDItLjkgMi0yVjhjMC0xLjEtLjktMi0yLTJ6bS0yLjA2IDExTDE1IDE1LjI4IDEyLjA2IDE3bC43OC0zLjMzLTIuNTktMi4yNCAzLjQxLS4yOUwxNSA4bDEuMzQgMy4xNCAzLjQxLjI5LTIuNTkgMi4yNC43OCAzLjMzeiIvPgo8L3N2Zz4K);
  --jp-icon-folder: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTAgNEg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMThjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY4YzAtMS4xLS45LTItMi0yaC04bC0yLTJ6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-home: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjRweCIgdmlld0JveD0iMCAwIDI0IDI0IiB3aWR0aD0iMjRweCIgZmlsbD0iIzAwMDAwMCI+CiAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPjxwYXRoIGNsYXNzPSJqcC1pY29uMyBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xMCAyMHYtNmg0djZoNXYtOGgzTDEyIDMgMiAxMmgzdjh6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-html5: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDUxMiA1MTIiPgogIDxwYXRoIGNsYXNzPSJqcC1pY29uMCBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiMwMDAiIGQ9Ik0xMDguNCAwaDIzdjIyLjhoMjEuMlYwaDIzdjY5aC0yM1Y0NmgtMjF2MjNoLTIzLjJNMjA2IDIzaC0yMC4zVjBoNjMuN3YyM0gyMjl2NDZoLTIzbTUzLjUtNjloMjQuMWwxNC44IDI0LjNMMzEzLjIgMGgyNC4xdjY5aC0yM1YzNC44bC0xNi4xIDI0LjgtMTYuMS0yNC44VjY5aC0yMi42bTg5LjItNjloMjN2NDYuMmgzMi42VjY5aC01NS42Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI2U0NGQyNiIgZD0iTTEwNy42IDQ3MWwtMzMtMzcwLjRoMzYyLjhsLTMzIDM3MC4yTDI1NS43IDUxMiIvPgogIDxwYXRoIGNsYXNzPSJqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNmMTY1MjkiIGQ9Ik0yNTYgNDgwLjVWMTMxaDE0OC4zTDM3NiA0NDciLz4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNlYmViZWIiIGQ9Ik0xNDIgMTc2LjNoMTE0djQ1LjRoLTY0LjJsNC4yIDQ2LjVoNjB2NDUuM0gxNTQuNG0yIDIyLjhIMjAybDMuMiAzNi4zIDUwLjggMTMuNnY0Ny40bC05My4yLTI2Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIiBmaWxsPSIjZmZmIiBkPSJNMzY5LjYgMTc2LjNIMjU1Ljh2NDUuNGgxMDkuNm0tNC4xIDQ2LjVIMjU1Ljh2NDUuNGg1NmwtNS4zIDU5LTUwLjcgMTMuNnY0Ny4ybDkzLTI1LjgiLz4KPC9zdmc+Cg==);
  --jp-icon-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1icmFuZDQganAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNGRkYiIGQ9Ik0yLjIgMi4yaDE3LjV2MTcuNUgyLjJ6Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzNGNTFCNSIgZD0iTTIuMiAyLjJ2MTcuNWgxNy41bC4xLTE3LjVIMi4yem0xMi4xIDIuMmMxLjIgMCAyLjIgMSAyLjIgMi4ycy0xIDIuMi0yLjIgMi4yLTIuMi0xLTIuMi0yLjIgMS0yLjIgMi4yLTIuMnpNNC40IDE3LjZsMy4zLTguOCAzLjMgNi42IDIuMi0zLjIgNC40IDUuNEg0LjR6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-info: url(data:image/svg+xml;base64,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);
  --jp-icon-inspector: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaW5zcGVjdG9yLWljb24tY29sb3IganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMjAgNEg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMThjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY2YzAtMS4xLS45LTItMi0yem0tNSAxNEg0di00aDExdjR6bTAtNUg0VjloMTF2NHptNSA1aC00VjloNHY5eiIvPgo8L3N2Zz4K);
  --jp-icon-json: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtanNvbi1pY29uLWNvbG9yIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI0Y5QTgyNSI+CiAgICA8cGF0aCBkPSJNMjAuMiAxMS44Yy0xLjYgMC0xLjcuNS0xLjcgMSAwIC40LjEuOS4xIDEuMy4xLjUuMS45LjEgMS4zIDAgMS43LTEuNCAyLjMtMy41IDIuM2gtLjl2LTEuOWguNWMxLjEgMCAxLjQgMCAxLjQtLjggMC0uMyAwLS42LS4xLTEgMC0uNC0uMS0uOC0uMS0xLjIgMC0xLjMgMC0xLjggMS4zLTItMS4zLS4yLTEuMy0uNy0xLjMtMiAwLS40LjEtLjguMS0xLjIuMS0uNC4xLS43LjEtMSAwLS44LS40LS43LTEuNC0uOGgtLjVWNC4xaC45YzIuMiAwIDMuNS43IDMuNSAyLjMgMCAuNC0uMS45LS4xIDEuMy0uMS41LS4xLjktLjEgMS4zIDAgLjUuMiAxIDEuNyAxdjEuOHpNMS44IDEwLjFjMS42IDAgMS43LS41IDEuNy0xIDAtLjQtLjEtLjktLjEtMS4zLS4xLS41LS4xLS45LS4xLTEuMyAwLTEuNiAxLjQtMi4zIDMuNS0yLjNoLjl2MS45aC0uNWMtMSAwLTEuNCAwLTEuNC44IDAgLjMgMCAuNi4xIDEgMCAuMi4xLjYuMSAxIDAgMS4zIDAgMS44LTEuMyAyQzYgMTEuMiA2IDExLjcgNiAxM2MwIC40LS4xLjgtLjEgMS4yLS4xLjMtLjEuNy0uMSAxIDAgLjguMy44IDEuNC44aC41djEuOWgtLjljLTIuMSAwLTMuNS0uNi0zLjUtMi4zIDAtLjQuMS0uOS4xLTEuMy4xLS41LjEtLjkuMS0xLjMgMC0uNS0uMi0xLTEuNy0xdi0xLjl6Ii8+CiAgICA8Y2lyY2xlIGN4PSIxMSIgY3k9IjEzLjgiIHI9IjIuMSIvPgogICAgPGNpcmNsZSBjeD0iMTEiIGN5PSI4LjIiIHI9IjIuMSIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-julia: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDMyNSAzMDAiPgogIDxnIGNsYXNzPSJqcC1icmFuZDAganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjY2IzYzMzIj4KICAgIDxwYXRoIGQ9Ik0gMTUwLjg5ODQzOCAyMjUgQyAxNTAuODk4NDM4IDI2Ni40MjE4NzUgMTE3LjMyMDMxMiAzMDAgNzUuODk4NDM4IDMwMCBDIDM0LjQ3NjU2MiAzMDAgMC44OTg0MzggMjY2LjQyMTg3NSAwLjg5ODQzOCAyMjUgQyAwLjg5ODQzOCAxODMuNTc4MTI1IDM0LjQ3NjU2MiAxNTAgNzUuODk4NDM4IDE1MCBDIDExNy4zMjAzMTIgMTUwIDE1MC44OTg0MzggMTgzLjU3ODEyNSAxNTAuODk4NDM4IDIyNSIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzM4OTgyNiI+CiAgICA8cGF0aCBkPSJNIDIzNy41IDc1IEMgMjM3LjUgMTE2LjQyMTg3NSAyMDMuOTIxODc1IDE1MCAxNjIuNSAxNTAgQyAxMjEuMDc4MTI1IDE1MCA4Ny41IDExNi40MjE4NzUgODcuNSA3NSBDIDg3LjUgMzMuNTc4MTI1IDEyMS4wNzgxMjUgMCAxNjIuNSAwIEMgMjAzLjkyMTg3NSAwIDIzNy41IDMzLjU3ODEyNSAyMzcuNSA3NSIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzk1NThiMiI+CiAgICA8cGF0aCBkPSJNIDMyNC4xMDE1NjIgMjI1IEMgMzI0LjEwMTU2MiAyNjYuNDIxODc1IDI5MC41MjM0MzggMzAwIDI0OS4xMDE1NjIgMzAwIEMgMjA3LjY3OTY4OCAzMDAgMTc0LjEwMTU2MiAyNjYuNDIxODc1IDE3NC4xMDE1NjIgMjI1IEMgMTc0LjEwMTU2MiAxODMuNTc4MTI1IDIwNy42Nzk2ODggMTUwIDI0OS4xMDE1NjIgMTUwIEMgMjkwLjUyMzQzOCAxNTAgMzI0LjEwMTU2MiAxODMuNTc4MTI1IDMyNC4xMDE1NjIgMjI1Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-jupyter-favicon: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTUyIiBoZWlnaHQ9IjE2NSIgdmlld0JveD0iMCAwIDE1MiAxNjUiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgPGcgY2xhc3M9ImpwLWp1cHl0ZXItaWNvbi1jb2xvciIgZmlsbD0iI0YzNzcyNiI+CiAgICA8cGF0aCB0cmFuc2Zvcm09InRyYW5zbGF0ZSgwLjA3ODk0NywgMTEwLjU4MjkyNykiIGQ9Ik03NS45NDIyODQyLDI5LjU4MDQ1NjEgQzQzLjMwMjM5NDcsMjkuNTgwNDU2MSAxNC43OTY3ODMyLDE3LjY1MzQ2MzQgMCwwIEM1LjUxMDgzMjExLDE1Ljg0MDY4MjkgMTUuNzgxNTM4OSwyOS41NjY3NzMyIDI5LjM5MDQ5NDcsMzkuMjc4NDE3MSBDNDIuOTk5Nyw0OC45ODk4NTM3IDU5LjI3MzcsNTQuMjA2NzgwNSA3NS45NjA1Nzg5LDU0LjIwNjc4MDUgQzkyLjY0NzQ1NzksNTQuMjA2NzgwNSAxMDguOTIxNDU4LDQ4Ljk4OTg1MzcgMTIyLjUzMDY2MywzOS4yNzg0MTcxIEMxMzYuMTM5NDUzLDI5LjU2Njc3MzIgMTQ2LjQxMDI4NCwxNS44NDA2ODI5IDE1MS45MjExNTgsMCBDMTM3LjA4Nzg2OCwxNy42NTM0NjM0IDEwOC41ODI1ODksMjkuNTgwNDU2MSA3NS45NDIyODQyLDI5LjU4MDQ1NjEgTDc1Ljk0MjI4NDIsMjkuNTgwNDU2MSBaIiAvPgogICAgPHBhdGggdHJhbnNmb3JtPSJ0cmFuc2xhdGUoMC4wMzczNjgsIDAuNzA0ODc4KSIgZD0iTTc1Ljk3ODQ1NzksMjQuNjI2NDA3MyBDMTA4LjYxODc2MywyNC42MjY0MDczIDEzNy4xMjQ0NTgsMzYuNTUzNDQxNSAxNTEuOTIxMTU4LDU0LjIwNjc4MDUgQzE0Ni40MTAyODQsMzguMzY2MjIyIDEzNi4xMzk0NTMsMjQuNjQwMTMxNyAxMjIuNTMwNjYzLDE0LjkyODQ4NzggQzEwOC45MjE0NTgsNS4yMTY4NDM5IDkyLjY0NzQ1NzksMCA3NS45NjA1Nzg5LDAgQzU5LjI3MzcsMCA0Mi45OTk3LDUuMjE2ODQzOSAyOS4zOTA0OTQ3LDE0LjkyODQ4NzggQzE1Ljc4MTUzODksMjQuNjQwMTMxNyA1LjUxMDgzMjExLDM4LjM2NjIyMiAwLDU0LjIwNjc4MDUgQzE0LjgzMzA4MTYsMzYuNTg5OTI5MyA0My4zMzg1Njg0LDI0LjYyNjQwNzMgNzUuOTc4NDU3OSwyNC42MjY0MDczIEw3NS45Nzg0NTc5LDI0LjYyNjQwNzMgWiIgLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-jupyter: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMzkiIGhlaWdodD0iNTEiIHZpZXdCb3g9IjAgMCAzOSA1MSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyB0cmFuc2Zvcm09InRyYW5zbGF0ZSgtMTYzOCAtMjI4MSkiPgogICAgIDxnIGNsYXNzPSJqcC1qdXB5dGVyLWljb24tY29sb3IiIGZpbGw9IiNGMzc3MjYiPgogICAgICA8cGF0aCB0cmFuc2Zvcm09InRyYW5zbGF0ZSgxNjM5Ljc0IDIzMTEuOTgpIiBkPSJNIDE4LjI2NDYgNy4xMzQxMUMgMTAuNDE0NSA3LjEzNDExIDMuNTU4NzIgNC4yNTc2IDAgMEMgMS4zMjUzOSAzLjgyMDQgMy43OTU1NiA3LjEzMDgxIDcuMDY4NiA5LjQ3MzAzQyAxMC4zNDE3IDExLjgxNTIgMTQuMjU1NyAxMy4wNzM0IDE4LjI2OSAxMy4wNzM0QyAyMi4yODIzIDEzLjA3MzQgMjYuMTk2MyAxMS44MTUyIDI5LjQ2OTQgOS40NzMwM0MgMzIuNzQyNCA3LjEzMDgxIDM1LjIxMjYgMy44MjA0IDM2LjUzOCAwQyAzMi45NzA1IDQuMjU3NiAyNi4xMTQ4IDcuMTM0MTEgMTguMjY0NiA3LjEzNDExWiIvPgogICAgICA8cGF0aCB0cmFuc2Zvcm09InRyYW5zbGF0ZSgxNjM5LjczIDIyODUuNDgpIiBkPSJNIDE4LjI3MzMgNS45MzkzMUMgMjYuMTIzNSA1LjkzOTMxIDMyLjk3OTMgOC44MTU4MyAzNi41MzggMTMuMDczNEMgMzUuMjEyNiA5LjI1MzAzIDMyLjc0MjQgNS45NDI2MiAyOS40Njk0IDMuNjAwNEMgMjYuMTk2MyAxLjI1ODE4IDIyLjI4MjMgMCAxOC4yNjkgMEMgMTQuMjU1NyAwIDEwLjM0MTcgMS4yNTgxOCA3LjA2ODYgMy42MDA0QyAzLjc5NTU2IDUuOTQyNjIgMS4zMjUzOSA5LjI1MzAzIDAgMTMuMDczNEMgMy41Njc0NSA4LjgyNDYzIDEwLjQyMzIgNS45MzkzMSAxOC4yNzMzIDUuOTM5MzFaIi8+CiAgICA8L2c+CiAgICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgICA8cGF0aCB0cmFuc2Zvcm09InRyYW5zbGF0ZSgxNjY5LjMgMjI4MS4zMSkiIGQ9Ik0gNS44OTM1MyAyLjg0NEMgNS45MTg4OSAzLjQzMTY1IDUuNzcwODUgNC4wMTM2NyA1LjQ2ODE1IDQuNTE2NDVDIDUuMTY1NDUgNS4wMTkyMiA0LjcyMTY4IDUuNDIwMTUgNC4xOTI5OSA1LjY2ODUxQyAzLjY2NDMgNS45MTY4OCAzLjA3NDQ0IDYuMDAxNTEgMi40OTgwNSA1LjkxMTcxQyAxLjkyMTY2IDUuODIxOSAxLjM4NDYzIDUuNTYxNyAwLjk1NDg5OCA1LjE2NDAxQyAwLjUyNTE3IDQuNzY2MzMgMC4yMjIwNTYgNC4yNDkwMyAwLjA4MzkwMzcgMy42Nzc1N0MgLTAuMDU0MjQ4MyAzLjEwNjExIC0wLjAyMTIzIDIuNTA2MTcgMC4xNzg3ODEgMS45NTM2NEMgMC4zNzg3OTMgMS40MDExIDAuNzM2ODA5IDAuOTIwODE3IDEuMjA3NTQgMC41NzM1MzhDIDEuNjc4MjYgMC4yMjYyNTkgMi4yNDA1NSAwLjAyNzU5MTkgMi44MjMyNiAwLjAwMjY3MjI5QyAzLjYwMzg5IC0wLjAzMDcxMTUgNC4zNjU3MyAwLjI0OTc4OSA0Ljk0MTQyIDAuNzgyNTUxQyA1LjUxNzExIDEuMzE1MzEgNS44NTk1NiAyLjA1Njc2IDUuODkzNTMgMi44NDRaIi8+CiAgICAgIDxwYXRoIHRyYW5zZm9ybT0idHJhbnNsYXRlKDE2MzkuOCAyMzIzLjgxKSIgZD0iTSA3LjQyNzg5IDMuNTgzMzhDIDcuNDYwMDggNC4zMjQzIDcuMjczNTUgNS4wNTgxOSA2Ljg5MTkzIDUuNjkyMTNDIDYuNTEwMzEgNi4zMjYwNyA1Ljk1MDc1IDYuODMxNTYgNS4yODQxMSA3LjE0NDZDIDQuNjE3NDcgNy40NTc2MyAzLjg3MzcxIDcuNTY0MTQgMy4xNDcwMiA3LjQ1MDYzQyAyLjQyMDMyIDcuMzM3MTIgMS43NDMzNiA3LjAwODcgMS4yMDE4NCA2LjUwNjk1QyAwLjY2MDMyOCA2LjAwNTIgMC4yNzg2MSA1LjM1MjY4IDAuMTA1MDE3IDQuNjMyMDJDIC0wLjA2ODU3NTcgMy45MTEzNSAtMC4wMjYyMzYxIDMuMTU0OTQgMC4yMjY2NzUgMi40NTg1NkMgMC40Nzk1ODcgMS43NjIxNyAwLjkzMTY5NyAxLjE1NzEzIDEuNTI1NzYgMC43MjAwMzNDIDIuMTE5ODMgMC4yODI5MzUgMi44MjkxNCAwLjAzMzQzOTUgMy41NjM4OSAwLjAwMzEzMzQ0QyA0LjU0NjY3IC0wLjAzNzQwMzMgNS41MDUyOSAwLjMxNjcwNiA2LjIyOTYxIDAuOTg3ODM1QyA2Ljk1MzkzIDEuNjU4OTYgNy4zODQ4NCAyLjU5MjM1IDcuNDI3ODkgMy41ODMzOEwgNy40Mjc4OSAzLjU4MzM4WiIvPgogICAgICA8cGF0aCB0cmFuc2Zvcm09InRyYW5zbGF0ZSgxNjM4LjM2IDIyODYuMDYpIiBkPSJNIDIuMjc0NzEgNC4zOTYyOUMgMS44NDM2MyA0LjQxNTA4IDEuNDE2NzEgNC4zMDQ0NSAxLjA0Nzk5IDQuMDc4NDNDIDAuNjc5MjY4IDMuODUyNCAwLjM4NTMyOCAzLjUyMTE0IDAuMjAzMzcxIDMuMTI2NTZDIDAuMDIxNDEzNiAyLjczMTk4IC0wLjA0MDM3OTggMi4yOTE4MyAwLjAyNTgxMTYgMS44NjE4MUMgMC4wOTIwMDMxIDEuNDMxOCAwLjI4MzIwNCAxLjAzMTI2IDAuNTc1MjEzIDAuNzEwODgzQyAwLjg2NzIyMiAwLjM5MDUxIDEuMjQ2OTEgMC4xNjQ3MDggMS42NjYyMiAwLjA2MjA1OTJDIDIuMDg1NTMgLTAuMDQwNTg5NyAyLjUyNTYxIC0wLjAxNTQ3MTQgMi45MzA3NiAwLjEzNDIzNUMgMy4zMzU5MSAwLjI4Mzk0MSAzLjY4NzkyIDAuNTUxNTA1IDMuOTQyMjIgMC45MDMwNkMgNC4xOTY1MiAxLjI1NDYyIDQuMzQxNjkgMS42NzQzNiA0LjM1OTM1IDIuMTA5MTZDIDQuMzgyOTkgMi42OTEwNyA0LjE3Njc4IDMuMjU4NjkgMy43ODU5NyAzLjY4NzQ2QyAzLjM5NTE2IDQuMTE2MjQgMi44NTE2NiA0LjM3MTE2IDIuMjc0NzEgNC4zOTYyOUwgMi4yNzQ3MSA0LjM5NjI5WiIvPgogICAgPC9nPgogIDwvZz4+Cjwvc3ZnPgo=);
  --jp-icon-jupyterlab-wordmark: url(data:image/svg+xml;base64,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);
  --jp-icon-kernel: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgZmlsbD0iIzYxNjE2MSIgZD0iTTE1IDlIOXY2aDZWOXptLTIgNGgtMnYtMmgydjJ6bTgtMlY5aC0yVjdjMC0xLjEtLjktMi0yLTJoLTJWM2gtMnYyaC0yVjNIOXYySDdjLTEuMSAwLTIgLjktMiAydjJIM3YyaDJ2MkgzdjJoMnYyYzAgMS4xLjkgMiAyIDJoMnYyaDJ2LTJoMnYyaDJ2LTJoMmMxLjEgMCAyLS45IDItMnYtMmgydi0yaC0ydi0yaDJ6bS00IDZIN1Y3aDEwdjEweiIvPgo8L3N2Zz4K);
  --jp-icon-keyboard: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMjAgNUg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMTdjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY3YzAtMS4xLS45LTItMi0yem0tOSAzaDJ2MmgtMlY4em0wIDNoMnYyaC0ydi0yek04IDhoMnYySDhWOHptMCAzaDJ2Mkg4di0yem0tMSAySDV2LTJoMnYyem0wLTNINVY4aDJ2MnptOSA3SDh2LTJoOHYyem0wLTRoLTJ2LTJoMnYyem0wLTNoLTJWOGgydjJ6bTMgM2gtMnYtMmgydjJ6bTAtM2gtMlY4aDJ2MnoiLz4KPC9zdmc+Cg==);
  --jp-icon-launch: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMzIgMzIiIHdpZHRoPSIzMiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik0yNiwyOEg2YTIuMDAyNywyLjAwMjcsMCwwLDEtMi0yVjZBMi4wMDI3LDIuMDAyNywwLDAsMSw2LDRIMTZWNkg2VjI2SDI2VjE2aDJWMjZBMi4wMDI3LDIuMDAyNywwLDAsMSwyNiwyOFoiLz4KICAgIDxwb2x5Z29uIHBvaW50cz0iMjAgMiAyMCA0IDI2LjU4NiA0IDE4IDEyLjU4NiAxOS40MTQgMTQgMjggNS40MTQgMjggMTIgMzAgMTIgMzAgMiAyMCAyIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-launcher: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkgMTlINVY1aDdWM0g1YTIgMiAwIDAwLTIgMnYxNGEyIDIgMCAwMDIgMmgxNGMxLjEgMCAyLS45IDItMnYtN2gtMnY3ek0xNCAzdjJoMy41OWwtOS44MyA5LjgzIDEuNDEgMS40MUwxOSA2LjQxVjEwaDJWM2gtN3oiLz4KPC9zdmc+Cg==);
  --jp-icon-line-form: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGZpbGw9IndoaXRlIiBkPSJNNS44OCA0LjEyTDEzLjc2IDEybC03Ljg4IDcuODhMOCAyMmwxMC0xMEw4IDJ6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-link: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTMuOSAxMmMwLTEuNzEgMS4zOS0zLjEgMy4xLTMuMWg0VjdIN2MtMi43NiAwLTUgMi4yNC01IDVzMi4yNCA1IDUgNWg0di0xLjlIN2MtMS43MSAwLTMuMS0xLjM5LTMuMS0zLjF6TTggMTNoOHYtMkg4djJ6bTktNmgtNHYxLjloNGMxLjcxIDAgMy4xIDEuMzkgMy4xIDMuMXMtMS4zOSAzLjEtMy4xIDMuMWgtNFYxN2g0YzIuNzYgMCA1LTIuMjQgNS01cy0yLjI0LTUtNS01eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-list: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xOSA1djE0SDVWNWgxNG0xLjEtMkgzLjljLS41IDAtLjkuNC0uOS45djE2LjJjMCAuNC40LjkuOS45aDE2LjJjLjQgMCAuOS0uNS45LS45VjMuOWMwLS41LS41LS45LS45LS45ek0xMSA3aDZ2MmgtNlY3em0wIDRoNnYyaC02di0yem0wIDRoNnYyaC02ek03IDdoMnYySDd6bTAgNGgydjJIN3ptMCA0aDJ2Mkg3eiIvPgo8L3N2Zz4K);
  --jp-icon-markdown: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDAganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjN0IxRkEyIiBkPSJNNSAxNC45aDEybC02LjEgNnptOS40LTYuOGMwLTEuMy0uMS0yLjktLjEtNC41LS40IDEuNC0uOSAyLjktMS4zIDQuM2wtMS4zIDQuM2gtMkw4LjUgNy45Yy0uNC0xLjMtLjctMi45LTEtNC4zLS4xIDEuNi0uMSAzLjItLjIgNC42TDcgMTIuNEg0LjhsLjctMTFoMy4zTDEwIDVjLjQgMS4yLjcgMi43IDEgMy45LjMtMS4yLjctMi42IDEtMy45bDEuMi0zLjdoMy4zbC42IDExaC0yLjRsLS4zLTQuMnoiLz4KPC9zdmc+Cg==);
  --jp-icon-move-down: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBkPSJNMTIuNDcxIDcuNTI4OTlDMTIuNzYzMiA3LjIzNjg0IDEyLjc2MzIgNi43NjMxNiAxMi40NzEgNi40NzEwMVY2LjQ3MTAxQzEyLjE3OSA2LjE3OTA1IDExLjcwNTcgNi4xNzg4NCAxMS40MTM1IDYuNDcwNTRMNy43NSAxMC4xMjc1VjEuNzVDNy43NSAxLjMzNTc5IDcuNDE0MjEgMSA3IDFWMUM2LjU4NTc5IDEgNi4yNSAxLjMzNTc5IDYuMjUgMS43NVYxMC4xMjc1TDIuNTk3MjYgNi40NjgyMkMyLjMwMzM4IDYuMTczODEgMS44MjY0MSA2LjE3MzU5IDEuNTMyMjYgNi40Njc3NFY2LjQ2Nzc0QzEuMjM4MyA2Ljc2MTcgMS4yMzgzIDcuMjM4MyAxLjUzMjI2IDcuNTMyMjZMNi4yOTI4OSAxMi4yOTI5QzYuNjgzNDIgMTIuNjgzNCA3LjMxNjU4IDEyLjY4MzQgNy43MDcxMSAxMi4yOTI5TDEyLjQ3MSA3LjUyODk5WiIgZmlsbD0iIzYxNjE2MSIvPgo8L3N2Zz4K);
  --jp-icon-move-up: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBkPSJNMS41Mjg5OSA2LjQ3MTAxQzEuMjM2ODQgNi43NjMxNiAxLjIzNjg0IDcuMjM2ODQgMS41Mjg5OSA3LjUyODk5VjcuNTI4OTlDMS44MjA5NSA3LjgyMDk1IDIuMjk0MjYgNy44MjExNiAyLjU4NjQ5IDcuNTI5NDZMNi4yNSAzLjg3MjVWMTIuMjVDNi4yNSAxMi42NjQyIDYuNTg1NzkgMTMgNyAxM1YxM0M3LjQxNDIxIDEzIDcuNzUgMTIuNjY0MiA3Ljc1IDEyLjI1VjMuODcyNUwxMS40MDI3IDcuNTMxNzhDMTEuNjk2NiA3LjgyNjE5IDEyLjE3MzYgNy44MjY0MSAxMi40Njc3IDcuNTMyMjZWNy41MzIyNkMxMi43NjE3IDcuMjM4MyAxMi43NjE3IDYuNzYxNyAxMi40Njc3IDYuNDY3NzRMNy43MDcxMSAxLjcwNzExQzcuMzE2NTggMS4zMTY1OCA2LjY4MzQyIDEuMzE2NTggNi4yOTI4OSAxLjcwNzExTDEuNTI4OTkgNi40NzEwMVoiIGZpbGw9IiM2MTYxNjEiLz4KPC9zdmc+Cg==);
  --jp-icon-new-folder: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwIDZoLThsLTItMkg0Yy0xLjExIDAtMS45OS44OS0xLjk5IDJMMiAxOGMwIDEuMTEuODkgMiAyIDJoMTZjMS4xMSAwIDItLjg5IDItMlY4YzAtMS4xMS0uODktMi0yLTJ6bS0xIDhoLTN2M2gtMnYtM2gtM3YtMmgzVjloMnYzaDN2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-not-trusted: url(data:image/svg+xml;base64,PHN2ZyBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI1IDI1Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDMgMykiIGQ9Ik0xLjg2MDk0IDExLjQ0MDlDMC44MjY0NDggOC43NzAyNyAwLjg2Mzc3OSA2LjA1NzY0IDEuMjQ5MDcgNC4xOTkzMkMyLjQ4MjA2IDMuOTMzNDcgNC4wODA2OCAzLjQwMzQ3IDUuNjAxMDIgMi44NDQ5QzcuMjM1NDkgMi4yNDQ0IDguODU2NjYgMS41ODE1IDkuOTg3NiAxLjA5NTM5QzExLjA1OTcgMS41ODM0MSAxMi42MDk0IDIuMjQ0NCAxNC4yMTggMi44NDMzOUMxNS43NTAzIDMuNDEzOTQgMTcuMzk5NSAzLjk1MjU4IDE4Ljc1MzkgNC4yMTM4NUMxOS4xMzY0IDYuMDcxNzcgMTkuMTcwOSA4Ljc3NzIyIDE4LjEzOSAxMS40NDA5QzE3LjAzMDMgMTQuMzAzMiAxNC42NjY4IDE3LjE4NDQgOS45OTk5OSAxOC45MzU0QzUuMzMzMTkgMTcuMTg0NCAyLjk2OTY4IDE0LjMwMzIgMS44NjA5NCAxMS40NDA5WiIvPgogICAgPHBhdGggY2xhc3M9ImpwLWljb24yIiBzdHJva2U9IiMzMzMzMzMiIHN0cm9rZS13aWR0aD0iMiIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoOS4zMTU5MiA5LjMyMDMxKSIgZD0iTTcuMzY4NDIgMEwwIDcuMzY0NzkiLz4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDkuMzE1OTIgMTYuNjgzNikgc2NhbGUoMSAtMSkiIGQ9Ik03LjM2ODQyIDBMMCA3LjM2NDc5Ii8+Cjwvc3ZnPgo=);
  --jp-icon-notebook: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtbm90ZWJvb2staWNvbi1jb2xvciBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNFRjZDMDAiPgogICAgPHBhdGggZD0iTTE4LjcgMy4zdjE1LjRIMy4zVjMuM2gxNS40bTEuNS0xLjVIMS44djE4LjNoMTguM2wuMS0xOC4zeiIvPgogICAgPHBhdGggZD0iTTE2LjUgMTYuNWwtNS40LTQuMy01LjYgNC4zdi0xMWgxMXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-numbering: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIiIGhlaWdodD0iMjIiIHZpZXdCb3g9IjAgMCAyOCAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTQgMTlINlYxOS41SDVWMjAuNUg2VjIxSDRWMjJIN1YxOEg0VjE5Wk01IDEwSDZWNkg0VjdINVYxMFpNNCAxM0g1LjhMNCAxNS4xVjE2SDdWMTVINS4yTDcgMTIuOVYxMkg0VjEzWk05IDdWOUgyM1Y3SDlaTTkgMjFIMjNWMTlIOVYyMVpNOSAxNUgyM1YxM0g5VjE1WiIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-offline-bolt: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjE2Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyIDIuMDJjLTUuNTEgMC05Ljk4IDQuNDctOS45OCA5Ljk4czQuNDcgOS45OCA5Ljk4IDkuOTggOS45OC00LjQ3IDkuOTgtOS45OFMxNy41MSAyLjAyIDEyIDIuMDJ6TTExLjQ4IDIwdi02LjI2SDhMMTMgNHY2LjI2aDMuMzVMMTEuNDggMjB6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-palette: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE4IDEzVjIwSDRWNkg5LjAyQzkuMDcgNS4yOSA5LjI0IDQuNjIgOS41IDRINEMyLjkgNCAyIDQuOSAyIDZWMjBDMiAyMS4xIDIuOSAyMiA0IDIySDE4QzE5LjEgMjIgMjAgMjEuMSAyMCAyMFYxNUwxOCAxM1pNMTkuMyA4Ljg5QzE5Ljc0IDguMTkgMjAgNy4zOCAyMCA2LjVDMjAgNC4wMSAxNy45OSAyIDE1LjUgMkMxMy4wMSAyIDExIDQuMDEgMTEgNi41QzExIDguOTkgMTMuMDEgMTEgMTUuNDkgMTFDMTYuMzcgMTEgMTcuMTkgMTAuNzQgMTcuODggMTAuM0wyMSAxMy40MkwyMi40MiAxMkwxOS4zIDguODlaTTE1LjUgOUMxNC4xMiA5IDEzIDcuODggMTMgNi41QzEzIDUuMTIgMTQuMTIgNCAxNS41IDRDMTYuODggNCAxOCA1LjEyIDE4IDYuNUMxOCA3Ljg4IDE2Ljg4IDkgMTUuNSA5WiIvPgogICAgPHBhdGggZmlsbC1ydWxlPSJldmVub2RkIiBjbGlwLXJ1bGU9ImV2ZW5vZGQiIGQ9Ik00IDZIOS4wMTg5NEM5LjAwNjM5IDYuMTY1MDIgOSA2LjMzMTc2IDkgNi41QzkgOC44MTU3NyAxMC4yMTEgMTAuODQ4NyAxMi4wMzQzIDEySDlWMTRIMTZWMTIuOTgxMUMxNi41NzAzIDEyLjkzNzcgMTcuMTIgMTIuODIwNyAxNy42Mzk2IDEyLjYzOTZMMTggMTNWMjBINFY2Wk04IDhINlYxMEg4VjhaTTYgMTJIOFYxNEg2VjEyWk04IDE2SDZWMThIOFYxNlpNOSAxNkgxNlYxOEg5VjE2WiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-paste: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTE5IDJoLTQuMThDMTQuNC44NCAxMy4zIDAgMTIgMGMtMS4zIDAtMi40Ljg0LTIuODIgMkg1Yy0xLjEgMC0yIC45LTIgMnYxNmMwIDEuMS45IDIgMiAyaDE0YzEuMSAwIDItLjkgMi0yVjRjMC0xLjEtLjktMi0yLTJ6bS03IDBjLjU1IDAgMSAuNDUgMSAxcy0uNDUgMS0xIDEtMS0uNDUtMS0xIC40NS0xIDEtMXptNyAxOEg1VjRoMnYzaDEwVjRoMnYxNnoiLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-pdf: url(data:image/svg+xml;base64,PHN2ZwogICB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyMiAyMiIgd2lkdGg9IjE2Ij4KICAgIDxwYXRoIHRyYW5zZm9ybT0icm90YXRlKDQ1KSIgY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI0ZGMkEyQSIKICAgICAgIGQ9Im0gMjIuMzQ0MzY5LC0zLjAxNjM2NDIgaCA1LjYzODYwNCB2IDEuNTc5MjQzMyBoIC0zLjU0OTIyNyB2IDEuNTA4NjkyOTkgaCAzLjMzNzU3NiBWIDEuNjUwODE1NCBoIC0zLjMzNzU3NiB2IDMuNDM1MjYxMyBoIC0yLjA4OTM3NyB6IG0gLTcuMTM2NDQ0LDEuNTc5MjQzMyB2IDQuOTQzOTU0MyBoIDAuNzQ4OTIgcSAxLjI4MDc2MSwwIDEuOTUzNzAzLC0wLjYzNDk1MzUgMC42NzgzNjksLTAuNjM0OTUzNSAwLjY3ODM2OSwtMS44NDUxNjQxIDAsLTEuMjA0NzgzNTUgLTAuNjcyOTQyLC0xLjgzNDMxMDExIC0wLjY3Mjk0MiwtMC42Mjk1MjY1OSAtMS45NTkxMywtMC42Mjk1MjY1OSB6IG0gLTIuMDg5Mzc3LC0xLjU3OTI0MzMgaCAyLjIwMzM0MyBxIDEuODQ1MTY0LDAgMi43NDYwMzksMC4yNjU5MjA3IDAuOTA2MzAxLDAuMjYwNDkzNyAxLjU1MjEwOCwwLjg5MDAyMDMgMC41Njk4MywwLjU0ODEyMjMgMC44NDY2MDUsMS4yNjQ0ODAwNiAwLjI3Njc3NCwwLjcxNjM1NzgxIDAuMjc2Nzc0LDEuNjIyNjU4OTQgMCwwLjkxNzE1NTEgLTAuMjc2Nzc0LDEuNjM4OTM5OSAtMC4yNzY3NzUsMC43MTYzNTc4IC0wLjg0NjYwNSwxLjI2NDQ4IC0wLjY1MTIzNCwwLjYyOTUyNjYgLTEuNTYyOTYyLDAuODk1NDQ3MyAtMC45MTE3MjgsMC4yNjA0OTM3IC0yLjczNTE4NSwwLjI2MDQ5MzcgaCAtMi4yMDMzNDMgeiBtIC04LjE0NTg1NjUsMCBoIDMuNDY3ODIzIHEgMS41NDY2ODE2LDAgMi4zNzE1Nzg1LDAuNjg5MjIzIDAuODMwMzI0LDAuNjgzNzk2MSAwLjgzMDMyNCwxLjk1MzcwMzE0IDAsMS4yNzUzMzM5NyAtMC44MzAzMjQsMS45NjQ1NTcwNiBRIDkuOTg3MTk2MSwyLjI3NDkxNSA4LjQ0MDUxNDUsMi4yNzQ5MTUgSCA3LjA2MjA2ODQgViA1LjA4NjA3NjcgSCA0Ljk3MjY5MTUgWiBtIDIuMDg5Mzc2OSwxLjUxNDExOTkgdiAyLjI2MzAzOTQzIGggMS4xNTU5NDEgcSAwLjYwNzgxODgsMCAwLjkzODg2MjksLTAuMjkzMDU1NDcgMC4zMzEwNDQxLC0wLjI5ODQ4MjQxIDAuMzMxMDQ0MSwtMC44NDExNzc3MiAwLC0wLjU0MjY5NTMxIC0wLjMzMTA0NDEsLTAuODM1NzUwNzQgLTAuMzMxMDQ0MSwtMC4yOTMwNTU1IC0wLjkzODg2MjksLTAuMjkzMDU1NSB6IgovPgo8L3N2Zz4K);
  --jp-icon-python: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iLTEwIC0xMCAxMzEuMTYxMzYxNjk0MzM1OTQgMTMyLjM4ODk5OTkzODk2NDg0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMzA2OTk4IiBkPSJNIDU0LjkxODc4NSw5LjE5Mjc0MjFlLTQgQyA1MC4zMzUxMzIsMC4wMjIyMTcyNyA0NS45NTc4NDYsMC40MTMxMzY5NyA0Mi4xMDYyODUsMS4wOTQ2NjkzIDMwLjc2MDA2OSwzLjA5OTE3MzEgMjguNzAwMDM2LDcuMjk0NzcxNCAyOC43MDAwMzUsMTUuMDMyMTY5IHYgMTAuMjE4NzUgaCAyNi44MTI1IHYgMy40MDYyNSBoIC0yNi44MTI1IC0xMC4wNjI1IGMgLTcuNzkyNDU5LDAgLTE0LjYxNTc1ODgsNC42ODM3MTcgLTE2Ljc0OTk5OTgsMTMuNTkzNzUgLTIuNDYxODE5OTgsMTAuMjEyOTY2IC0yLjU3MTAxNTA4LDE2LjU4NjAyMyAwLDI3LjI1IDEuOTA1OTI4Myw3LjkzNzg1MiA2LjQ1NzU0MzIsMTMuNTkzNzQ4IDE0LjI0OTk5OTgsMTMuNTkzNzUgaCA5LjIxODc1IHYgLTEyLjI1IGMgMCwtOC44NDk5MDIgNy42NTcxNDQsLTE2LjY1NjI0OCAxNi43NSwtMTYuNjU2MjUgaCAyNi43ODEyNSBjIDcuNDU0OTUxLDAgMTMuNDA2MjUzLC02LjEzODE2NCAxMy40MDYyNSwtMTMuNjI1IHYgLTI1LjUzMTI1IGMgMCwtNy4yNjYzMzg2IC02LjEyOTk4LC0xMi43MjQ3NzcxIC0xMy40MDYyNSwtMTMuOTM3NDk5NyBDIDY0LjI4MTU0OCwwLjMyNzk0Mzk3IDU5LjUwMjQzOCwtMC4wMjAzNzkwMyA1NC45MTg3ODUsOS4xOTI3NDIxZS00IFogbSAtMTQuNSw4LjIxODc1MDEyNTc5IGMgMi43Njk1NDcsMCA1LjAzMTI1LDIuMjk4NjQ1NiA1LjAzMTI1LDUuMTI0OTk5NiAtMmUtNiwyLjgxNjMzNiAtMi4yNjE3MDMsNS4wOTM3NSAtNS4wMzEyNSw1LjA5Mzc1IC0yLjc3OTQ3NiwtMWUtNiAtNS4wMzEyNSwtMi4yNzc0MTUgLTUuMDMxMjUsLTUuMDkzNzUgLTEwZS03LC0yLjgyNjM1MyAyLjI1MTc3NCwtNS4xMjQ5OTk2IDUuMDMxMjUsLTUuMTI0OTk5NiB6Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI2ZmZDQzYiIgZD0ibSA4NS42Mzc1MzUsMjguNjU3MTY5IHYgMTEuOTA2MjUgYyAwLDkuMjMwNzU1IC03LjgyNTg5NSwxNi45OTk5OTkgLTE2Ljc1LDE3IGggLTI2Ljc4MTI1IGMgLTcuMzM1ODMzLDAgLTEzLjQwNjI0OSw2LjI3ODQ4MyAtMTMuNDA2MjUsMTMuNjI1IHYgMjUuNTMxMjQ3IGMgMCw3LjI2NjM0NCA2LjMxODU4OCwxMS41NDAzMjQgMTMuNDA2MjUsMTMuNjI1MDA0IDguNDg3MzMxLDIuNDk1NjEgMTYuNjI2MjM3LDIuOTQ2NjMgMjYuNzgxMjUsMCA2Ljc1MDE1NSwtMS45NTQzOSAxMy40MDYyNTMsLTUuODg3NjEgMTMuNDA2MjUsLTEzLjYyNTAwNCBWIDg2LjUwMDkxOSBoIC0yNi43ODEyNSB2IC0zLjQwNjI1IGggMjYuNzgxMjUgMTMuNDA2MjU0IGMgNy43OTI0NjEsMCAxMC42OTYyNTEsLTUuNDM1NDA4IDEzLjQwNjI0MSwtMTMuNTkzNzUgMi43OTkzMywtOC4zOTg4ODYgMi42ODAyMiwtMTYuNDc1Nzc2IDAsLTI3LjI1IC0xLjkyNTc4LC03Ljc1NzQ0MSAtNS42MDM4NywtMTMuNTkzNzUgLTEzLjQwNjI0MSwtMTMuNTkzNzUgeiBtIC0xNS4wNjI1LDY0LjY1NjI1IGMgMi43Nzk0NzgsM2UtNiA1LjAzMTI1LDIuMjc3NDE3IDUuMDMxMjUsNS4wOTM3NDcgLTJlLTYsMi44MjYzNTQgLTIuMjUxNzc1LDUuMTI1MDA0IC01LjAzMTI1LDUuMTI1MDA0IC0yLjc2OTU1LDAgLTUuMDMxMjUsLTIuMjk4NjUgLTUuMDMxMjUsLTUuMTI1MDA0IDJlLTYsLTIuODE2MzMgMi4yNjE2OTcsLTUuMDkzNzQ3IDUuMDMxMjUsLTUuMDkzNzQ3IHoiLz4KPC9zdmc+Cg==);
  --jp-icon-r-kernel: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMjE5NkYzIiBkPSJNNC40IDIuNWMxLjItLjEgMi45LS4zIDQuOS0uMyAyLjUgMCA0LjEuNCA1LjIgMS4zIDEgLjcgMS41IDEuOSAxLjUgMy41IDAgMi0xLjQgMy41LTIuOSA0LjEgMS4yLjQgMS43IDEuNiAyLjIgMyAuNiAxLjkgMSAzLjkgMS4zIDQuNmgtMy44Yy0uMy0uNC0uOC0xLjctMS4yLTMuN3MtMS4yLTIuNi0yLjYtMi42aC0uOXY2LjRINC40VjIuNXptMy43IDYuOWgxLjRjMS45IDAgMi45LS45IDIuOS0yLjNzLTEtMi4zLTIuOC0yLjNjLS43IDAtMS4zIDAtMS42LjJ2NC41aC4xdi0uMXoiLz4KPC9zdmc+Cg==);
  --jp-icon-react: url(data:image/svg+xml;base64,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);
  --jp-icon-redo: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjE2Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgICA8cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIi8+PHBhdGggZD0iTTE4LjQgMTAuNkMxNi41NSA4Ljk5IDE0LjE1IDggMTEuNSA4Yy00LjY1IDAtOC41OCAzLjAzLTkuOTYgNy4yMkwzLjkgMTZjMS4wNS0zLjE5IDQuMDUtNS41IDcuNi01LjUgMS45NSAwIDMuNzMuNzIgNS4xMiAxLjg4TDEzIDE2aDlWN2wtMy42IDMuNnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-refresh: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTkgMTMuNWMtMi40OSAwLTQuNS0yLjAxLTQuNS00LjVTNi41MSA0LjUgOSA0LjVjMS4yNCAwIDIuMzYuNTIgMy4xNyAxLjMzTDEwIDhoNVYzbC0xLjc2IDEuNzZDMTIuMTUgMy42OCAxMC42NiAzIDkgMyA1LjY5IDMgMy4wMSA1LjY5IDMuMDEgOVM1LjY5IDE1IDkgMTVjMi45NyAwIDUuNDMtMi4xNiA1LjktNWgtMS41MmMtLjQ2IDItMi4yNCAzLjUtNC4zOCAzLjV6Ii8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-regex: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KICA8ZyBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiM0MTQxNDEiPgogICAgPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjE2IiBoZWlnaHQ9IjE2Ii8+CiAgPC9nPgoKICA8ZyBjbGFzcz0ianAtaWNvbi1hY2NlbnQyIiBmaWxsPSIjRkZGIj4KICAgIDxjaXJjbGUgY2xhc3M9InN0MiIgY3g9IjUuNSIgY3k9IjE0LjUiIHI9IjEuNSIvPgogICAgPHJlY3QgeD0iMTIiIHk9IjQiIGNsYXNzPSJzdDIiIHdpZHRoPSIxIiBoZWlnaHQ9IjgiLz4KICAgIDxyZWN0IHg9IjguNSIgeT0iNy41IiB0cmFuc2Zvcm09Im1hdHJpeCgwLjg2NiAtMC41IDAuNSAwLjg2NiAtMi4zMjU1IDcuMzIxOSkiIGNsYXNzPSJzdDIiIHdpZHRoPSI4IiBoZWlnaHQ9IjEiLz4KICAgIDxyZWN0IHg9IjEyIiB5PSI0IiB0cmFuc2Zvcm09Im1hdHJpeCgwLjUgLTAuODY2IDAuODY2IDAuNSAtMC42Nzc5IDE0LjgyNTIpIiBjbGFzcz0ic3QyIiB3aWR0aD0iMSIgaGVpZ2h0PSI4Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-run: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTggNXYxNGwxMS03eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-running: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDUxMiA1MTIiPgogIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICA8cGF0aCBkPSJNMjU2IDhDMTE5IDggOCAxMTkgOCAyNTZzMTExIDI0OCAyNDggMjQ4IDI0OC0xMTEgMjQ4LTI0OFMzOTMgOCAyNTYgOHptOTYgMzI4YzAgOC44LTcuMiAxNi0xNiAxNkgxNzZjLTguOCAwLTE2LTcuMi0xNi0xNlYxNzZjMC04LjggNy4yLTE2IDE2LTE2aDE2MGM4LjggMCAxNiA3LjIgMTYgMTZ2MTYweiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-save: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTE3IDNINWMtMS4xMSAwLTIgLjktMiAydjE0YzAgMS4xLjg5IDIgMiAyaDE0YzEuMSAwIDItLjkgMi0yVjdsLTQtNHptLTUgMTZjLTEuNjYgMC0zLTEuMzQtMy0zczEuMzQtMyAzLTMgMyAxLjM0IDMgMy0xLjM0IDMtMyAzem0zLTEwSDVWNWgxMHY0eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-search: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjEsMTAuOWgtMC43bC0wLjItMC4yYzAuOC0wLjksMS4zLTIuMiwxLjMtMy41YzAtMy0yLjQtNS40LTUuNC01LjRTMS44LDQuMiwxLjgsNy4xczIuNCw1LjQsNS40LDUuNCBjMS4zLDAsMi41LTAuNSwzLjUtMS4zbDAuMiwwLjJ2MC43bDQuMSw0LjFsMS4yLTEuMkwxMi4xLDEwLjl6IE03LjEsMTAuOWMtMi4xLDAtMy43LTEuNy0zLjctMy43czEuNy0zLjcsMy43LTMuN3MzLjcsMS43LDMuNywzLjcgUzkuMiwxMC45LDcuMSwxMC45eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-settings: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkuNDMgMTIuOThjLjA0LS4zMi4wNy0uNjQuMDctLjk4cy0uMDMtLjY2LS4wNy0uOThsMi4xMS0xLjY1Yy4xOS0uMTUuMjQtLjQyLjEyLS42NGwtMi0zLjQ2Yy0uMTItLjIyLS4zOS0uMy0uNjEtLjIybC0yLjQ5IDFjLS41Mi0uNC0xLjA4LS43My0xLjY5LS45OGwtLjM4LTIuNjVBLjQ4OC40ODggMCAwMDE0IDJoLTRjLS4yNSAwLS40Ni4xOC0uNDkuNDJsLS4zOCAyLjY1Yy0uNjEuMjUtMS4xNy41OS0xLjY5Ljk4bC0yLjQ5LTFjLS4yMy0uMDktLjQ5IDAtLjYxLjIybC0yIDMuNDZjLS4xMy4yMi0uMDcuNDkuMTIuNjRsMi4xMSAxLjY1Yy0uMDQuMzItLjA3LjY1LS4wNy45OHMuMDMuNjYuMDcuOThsLTIuMTEgMS42NWMtLjE5LjE1LS4yNC40Mi0uMTIuNjRsMiAzLjQ2Yy4xMi4yMi4zOS4zLjYxLjIybDIuNDktMWMuNTIuNCAxLjA4LjczIDEuNjkuOThsLjM4IDIuNjVjLjAzLjI0LjI0LjQyLjQ5LjQyaDRjLjI1IDAgLjQ2LS4xOC40OS0uNDJsLjM4LTIuNjVjLjYxLS4yNSAxLjE3LS41OSAxLjY5LS45OGwyLjQ5IDFjLjIzLjA5LjQ5IDAgLjYxLS4yMmwyLTMuNDZjLjEyLS4yMi4wNy0uNDktLjEyLS42NGwtMi4xMS0xLjY1ek0xMiAxNS41Yy0xLjkzIDAtMy41LTEuNTctMy41LTMuNXMxLjU3LTMuNSAzLjUtMy41IDMuNSAxLjU3IDMuNSAzLjUtMS41NyAzLjUtMy41IDMuNXoiLz4KPC9zdmc+Cg==);
  --jp-icon-share: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIHZpZXdCb3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTSAxOCAyIEMgMTYuMzU0OTkgMiAxNSAzLjM1NDk5MDQgMTUgNSBDIDE1IDUuMTkwOTUyOSAxNS4wMjE3OTEgNS4zNzcxMjI0IDE1LjA1NjY0MSA1LjU1ODU5MzggTCA3LjkyMTg3NSA5LjcyMDcwMzEgQyA3LjM5ODUzOTkgOS4yNzc4NTM5IDYuNzMyMDc3MSA5IDYgOSBDIDQuMzU0OTkwNCA5IDMgMTAuMzU0OTkgMyAxMiBDIDMgMTMuNjQ1MDEgNC4zNTQ5OTA0IDE1IDYgMTUgQyA2LjczMjA3NzEgMTUgNy4zOTg1Mzk5IDE0LjcyMjE0NiA3LjkyMTg3NSAxNC4yNzkyOTcgTCAxNS4wNTY2NDEgMTguNDM5NDUzIEMgMTUuMDIxNTU1IDE4LjYyMTUxNCAxNSAxOC44MDgzODYgMTUgMTkgQyAxNSAyMC42NDUwMSAxNi4zNTQ5OSAyMiAxOCAyMiBDIDE5LjY0NTAxIDIyIDIxIDIwLjY0NTAxIDIxIDE5IEMgMjEgMTcuMzU0OTkgMTkuNjQ1MDEgMTYgMTggMTYgQyAxNy4yNjc0OCAxNiAxNi42MDE1OTMgMTYuMjc5MzI4IDE2LjA3ODEyNSAxNi43MjI2NTYgTCA4Ljk0MzM1OTQgMTIuNTU4NTk0IEMgOC45NzgyMDk1IDEyLjM3NzEyMiA5IDEyLjE5MDk1MyA5IDEyIEMgOSAxMS44MDkwNDcgOC45NzgyMDk1IDExLjYyMjg3OCA4Ljk0MzM1OTQgMTEuNDQxNDA2IEwgMTYuMDc4MTI1IDcuMjc5Mjk2OSBDIDE2LjYwMTQ2IDcuNzIyMTQ2MSAxNy4yNjc5MjMgOCAxOCA4IEMgMTkuNjQ1MDEgOCAyMSA2LjY0NTAwOTYgMjEgNSBDIDIxIDMuMzU0OTkwNCAxOS42NDUwMSAyIDE4IDIgeiBNIDE4IDQgQyAxOC41NjQxMjkgNCAxOSA0LjQzNTg3MDYgMTkgNSBDIDE5IDUuNTY0MTI5NCAxOC41NjQxMjkgNiAxOCA2IEMgMTcuNDM1ODcxIDYgMTcgNS41NjQxMjk0IDE3IDUgQyAxNyA0LjQzNTg3MDYgMTcuNDM1ODcxIDQgMTggNCB6IE0gNiAxMSBDIDYuNTY0MTI5NCAxMSA3IDExLjQzNTg3MSA3IDEyIEMgNyAxMi41NjQxMjkgNi41NjQxMjk0IDEzIDYgMTMgQyA1LjQzNTg3MDYgMTMgNSAxMi41NjQxMjkgNSAxMiBDIDUgMTEuNDM1ODcxIDUuNDM1ODcwNiAxMSA2IDExIHogTSAxOCAxOCBDIDE4LjU2NDEyOSAxOCAxOSAxOC40MzU4NzEgMTkgMTkgQyAxOSAxOS41NjQxMjkgMTguNTY0MTI5IDIwIDE4IDIwIEMgMTcuNDM1ODcxIDIwIDE3IDE5LjU2NDEyOSAxNyAxOSBDIDE3IDE4LjQzNTg3MSAxNy40MzU4NzEgMTggMTggMTggeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-spreadsheet: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDEganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNENBRjUwIiBkPSJNMi4yIDIuMnYxNy42aDE3LjZWMi4ySDIuMnptMTUuNCA3LjdoLTUuNVY0LjRoNS41djUuNXpNOS45IDQuNHY1LjVINC40VjQuNGg1LjV6bS01LjUgNy43aDUuNXY1LjVINC40di01LjV6bTcuNyA1LjV2LTUuNWg1LjV2NS41aC01LjV6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-stop: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik02IDZoMTJ2MTJINnoiLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-tab: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIxIDNIM2MtMS4xIDAtMiAuOS0yIDJ2MTRjMCAxLjEuOSAyIDIgMmgxOGMxLjEgMCAyLS45IDItMlY1YzAtMS4xLS45LTItMi0yem0wIDE2SDNWNWgxMHY0aDh2MTB6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-table-rows: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik0yMSw4SDNWNGgxOFY4eiBNMjEsMTBIM3Y0aDE4VjEweiBNMjEsMTZIM3Y0aDE4VjE2eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-tag: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjgiIGhlaWdodD0iMjgiIHZpZXdCb3g9IjAgMCA0MyAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTI4LjgzMzIgMTIuMzM0TDMyLjk5OTggMTYuNTAwN0wzNy4xNjY1IDEyLjMzNEgyOC44MzMyWiIvPgoJCTxwYXRoIGQ9Ik0xNi4yMDk1IDIxLjYxMDRDMTUuNjg3MyAyMi4xMjk5IDE0Ljg0NDMgMjIuMTI5OSAxNC4zMjQ4IDIxLjYxMDRMNi45ODI5IDE0LjcyNDVDNi41NzI0IDE0LjMzOTQgNi4wODMxMyAxMy42MDk4IDYuMDQ3ODYgMTMuMDQ4MkM1Ljk1MzQ3IDExLjUyODggNi4wMjAwMiA4LjYxOTQ0IDYuMDY2MjEgNy4wNzY5NUM2LjA4MjgxIDYuNTE0NzcgNi41NTU0OCA2LjA0MzQ3IDcuMTE4MDQgNi4wMzA1NUM5LjA4ODYzIDUuOTg0NzMgMTMuMjYzOCA1LjkzNTc5IDEzLjY1MTggNi4zMjQyNUwyMS43MzY5IDEzLjYzOUMyMi4yNTYgMTQuMTU4NSAyMS43ODUxIDE1LjQ3MjQgMjEuMjYyIDE1Ljk5NDZMMTYuMjA5NSAyMS42MTA0Wk05Ljc3NTg1IDguMjY1QzkuMzM1NTEgNy44MjU2NiA4LjYyMzUxIDcuODI1NjYgOC4xODI4IDguMjY1QzcuNzQzNDYgOC43MDU3MSA3Ljc0MzQ2IDkuNDE3MzMgOC4xODI4IDkuODU2NjdDOC42MjM4MiAxMC4yOTY0IDkuMzM1ODIgMTAuMjk2NCA5Ljc3NTg1IDkuODU2NjdDMTAuMjE1NiA5LjQxNzMzIDEwLjIxNTYgOC43MDUzMyA5Ljc3NTg1IDguMjY1WiIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-terminal: url(data:image/svg+xml;base64,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);
  --jp-icon-text-editor: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtdGV4dC1lZGl0b3ItaWNvbi1jb2xvciBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xNSAxNUgzdjJoMTJ2LTJ6bTAtOEgzdjJoMTJWN3pNMyAxM2gxOHYtMkgzdjJ6bTAgOGgxOHYtMkgzdjJ6TTMgM3YyaDE4VjNIM3oiLz4KPC9zdmc+Cg==);
  --jp-icon-toc: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik03LDVIMjFWN0g3VjVNNywxM1YxMUgyMVYxM0g3TTQsNC41QTEuNSwxLjUgMCAwLDEgNS41LDZBMS41LDEuNSAwIDAsMSA0LDcuNUExLjUsMS41IDAgMCwxIDIuNSw2QTEuNSwxLjUgMCAwLDEgNCw0LjVNNCwxMC41QTEuNSwxLjUgMCAwLDEgNS41LDEyQTEuNSwxLjUgMCAwLDEgNCwxMy41QTEuNSwxLjUgMCAwLDEgMi41LDEyQTEuNSwxLjUgMCAwLDEgNCwxMC41TTcsMTlWMTdIMjFWMTlIN000LDE2LjVBMS41LDEuNSAwIDAsMSA1LjUsMThBMS41LDEuNSAwIDAsMSA0LDE5LjVBMS41LDEuNSAwIDAsMSAyLjUsMThBMS41LDEuNSAwIDAsMSA0LDE2LjVaIiAvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-tree-view: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik0yMiAxMVYzaC03djNIOVYzSDJ2OGg3VjhoMnYxMGg0djNoN3YtOGgtN3YzaC0yVjhoMnYzeiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-trusted: url(data:image/svg+xml;base64,PHN2ZyBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI1Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDIgMykiIGQ9Ik0xLjg2MDk0IDExLjQ0MDlDMC44MjY0NDggOC43NzAyNyAwLjg2Mzc3OSA2LjA1NzY0IDEuMjQ5MDcgNC4xOTkzMkMyLjQ4MjA2IDMuOTMzNDcgNC4wODA2OCAzLjQwMzQ3IDUuNjAxMDIgMi44NDQ5QzcuMjM1NDkgMi4yNDQ0IDguODU2NjYgMS41ODE1IDkuOTg3NiAxLjA5NTM5QzExLjA1OTcgMS41ODM0MSAxMi42MDk0IDIuMjQ0NCAxNC4yMTggMi44NDMzOUMxNS43NTAzIDMuNDEzOTQgMTcuMzk5NSAzLjk1MjU4IDE4Ljc1MzkgNC4yMTM4NUMxOS4xMzY0IDYuMDcxNzcgMTkuMTcwOSA4Ljc3NzIyIDE4LjEzOSAxMS40NDA5QzE3LjAzMDMgMTQuMzAzMiAxNC42NjY4IDE3LjE4NDQgOS45OTk5OSAxOC45MzU0QzUuMzMzMiAxNy4xODQ0IDIuOTY5NjggMTQuMzAzMiAxLjg2MDk0IDExLjQ0MDlaIi8+CiAgICA8cGF0aCBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiMzMzMzMzMiIHN0cm9rZT0iIzMzMzMzMyIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoOCA5Ljg2NzE5KSIgZD0iTTIuODYwMTUgNC44NjUzNUwwLjcyNjU0OSAyLjk5OTU5TDAgMy42MzA0NUwyLjg2MDE1IDYuMTMxNTdMOCAwLjYzMDg3Mkw3LjI3ODU3IDBMMi44NjAxNSA0Ljg2NTM1WiIvPgo8L3N2Zz4K);
  --jp-icon-undo: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjUgOGMtMi42NSAwLTUuMDUuOTktNi45IDIuNkwyIDd2OWg5bC0zLjYyLTMuNjJjMS4zOS0xLjE2IDMuMTYtMS44OCA1LjEyLTEuODggMy41NCAwIDYuNTUgMi4zMSA3LjYgNS41bDIuMzctLjc4QzIxLjA4IDExLjAzIDE3LjE1IDggMTIuNSA4eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-user: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIHZpZXdCb3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE2IDdhNCA0IDAgMTEtOCAwIDQgNCAwIDAxOCAwek0xMiAxNGE3IDcgMCAwMC03IDdoMTRhNyA3IDAgMDAtNy03eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-users: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZlcnNpb249IjEuMSIgdmlld0JveD0iMCAwIDM2IDI0IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgogPGcgY2xhc3M9ImpwLWljb24zIiB0cmFuc2Zvcm09Im1hdHJpeCgxLjczMjcgMCAwIDEuNzMyNyAtMy42MjgyIC4wOTk1NzcpIiBmaWxsPSIjNjE2MTYxIj4KICA8cGF0aCB0cmFuc2Zvcm09Im1hdHJpeCgxLjUsMCwwLDEuNSwwLC02KSIgZD0ibTEyLjE4NiA3LjUwOThjLTEuMDUzNSAwLTEuOTc1NyAwLjU2NjUtMi40Nzg1IDEuNDEwMiAwLjc1MDYxIDAuMzEyNzcgMS4zOTc0IDAuODI2NDggMS44NzMgMS40NzI3aDMuNDg2M2MwLTEuNTkyLTEuMjg4OS0yLjg4MjgtMi44ODA5LTIuODgyOHoiLz4KICA8cGF0aCBkPSJtMjAuNDY1IDIuMzg5NWEyLjE4ODUgMi4xODg1IDAgMCAxLTIuMTg4NCAyLjE4ODUgMi4xODg1IDIuMTg4NSAwIDAgMS0yLjE4ODUtMi4xODg1IDIuMTg4NSAyLjE4ODUgMCAwIDEgMi4xODg1LTIuMTg4NSAyLjE4ODUgMi4xODg1IDAgMCAxIDIuMTg4NCAyLjE4ODV6Ii8+CiAgPHBhdGggdHJhbnNmb3JtPSJtYXRyaXgoMS41LDAsMCwxLjUsMCwtNikiIGQ9Im0zLjU4OTggOC40MjE5Yy0xLjExMjYgMC0yLjAxMzcgMC45MDExMS0yLjAxMzcgMi4wMTM3aDIuODE0NWMwLjI2Nzk3LTAuMzczMDkgMC41OTA3LTAuNzA0MzUgMC45NTg5OC0wLjk3ODUyLTAuMzQ0MzMtMC42MTY4OC0xLjAwMzEtMS4wMzUyLTEuNzU5OC0xLjAzNTJ6Ii8+CiAgPHBhdGggZD0ibTYuOTE1NCA0LjYyM2ExLjUyOTQgMS41Mjk0IDAgMCAxLTEuNTI5NCAxLjUyOTQgMS41Mjk0IDEuNTI5NCAwIDAgMS0xLjUyOTQtMS41Mjk0IDEuNTI5NCAxLjUyOTQgMCAwIDEgMS41Mjk0LTEuNTI5NCAxLjUyOTQgMS41Mjk0IDAgMCAxIDEuNTI5NCAxLjUyOTR6Ii8+CiAgPHBhdGggZD0ibTYuMTM1IDEzLjUzNWMwLTMuMjM5MiAyLjYyNTktNS44NjUgNS44NjUtNS44NjUgMy4yMzkyIDAgNS44NjUgMi42MjU5IDUuODY1IDUuODY1eiIvPgogIDxjaXJjbGUgY3g9IjEyIiBjeT0iMy43Njg1IiByPSIyLjk2ODUiLz4KIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-vega: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbjEganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMjEyMTIxIj4KICAgIDxwYXRoIGQ9Ik0xMC42IDUuNGwyLjItMy4ySDIuMnY3LjNsNC02LjZ6Ii8+CiAgICA8cGF0aCBkPSJNMTUuOCAyLjJsLTQuNCA2LjZMNyA2LjNsLTQuOCA4djUuNWgxNy42VjIuMmgtNHptLTcgMTUuNEg1LjV2LTQuNGgzLjN2NC40em00LjQgMEg5LjhWOS44aDMuNHY3Ljh6bTQuNCAwaC0zLjRWNi41aDMuNHYxMS4xeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-word: url(data:image/svg+xml;base64,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);
  --jp-icon-yaml: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1jb250cmFzdDIganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjRDgxQjYwIj4KICAgIDxwYXRoIGQ9Ik03LjIgMTguNnYtNS40TDMgNS42aDMuM2wxLjQgMy4xYy4zLjkuNiAxLjYgMSAyLjUuMy0uOC42LTEuNiAxLTIuNWwxLjQtMy4xaDMuNGwtNC40IDcuNnY1LjVsLTIuOS0uMXoiLz4KICAgIDxjaXJjbGUgY2xhc3M9InN0MCIgY3g9IjE3LjYiIGN5PSIxNi41IiByPSIyLjEiLz4KICAgIDxjaXJjbGUgY2xhc3M9InN0MCIgY3g9IjE3LjYiIGN5PSIxMSIgcj0iMi4xIi8+CiAgPC9nPgo8L3N2Zz4K);
}

/* Icon CSS class declarations */

.jp-AddAboveIcon {
  background-image: var(--jp-icon-add-above);
}

.jp-AddBelowIcon {
  background-image: var(--jp-icon-add-below);
}

.jp-AddIcon {
  background-image: var(--jp-icon-add);
}

.jp-BellIcon {
  background-image: var(--jp-icon-bell);
}

.jp-BugDotIcon {
  background-image: var(--jp-icon-bug-dot);
}

.jp-BugIcon {
  background-image: var(--jp-icon-bug);
}

.jp-BuildIcon {
  background-image: var(--jp-icon-build);
}

.jp-CaretDownEmptyIcon {
  background-image: var(--jp-icon-caret-down-empty);
}

.jp-CaretDownEmptyThinIcon {
  background-image: var(--jp-icon-caret-down-empty-thin);
}

.jp-CaretDownIcon {
  background-image: var(--jp-icon-caret-down);
}

.jp-CaretLeftIcon {
  background-image: var(--jp-icon-caret-left);
}

.jp-CaretRightIcon {
  background-image: var(--jp-icon-caret-right);
}

.jp-CaretUpEmptyThinIcon {
  background-image: var(--jp-icon-caret-up-empty-thin);
}

.jp-CaretUpIcon {
  background-image: var(--jp-icon-caret-up);
}

.jp-CaseSensitiveIcon {
  background-image: var(--jp-icon-case-sensitive);
}

.jp-CheckIcon {
  background-image: var(--jp-icon-check);
}

.jp-CircleEmptyIcon {
  background-image: var(--jp-icon-circle-empty);
}

.jp-CircleIcon {
  background-image: var(--jp-icon-circle);
}

.jp-ClearIcon {
  background-image: var(--jp-icon-clear);
}

.jp-CloseIcon {
  background-image: var(--jp-icon-close);
}

.jp-CodeCheckIcon {
  background-image: var(--jp-icon-code-check);
}

.jp-CodeIcon {
  background-image: var(--jp-icon-code);
}

.jp-CollapseAllIcon {
  background-image: var(--jp-icon-collapse-all);
}

.jp-ConsoleIcon {
  background-image: var(--jp-icon-console);
}

.jp-CopyIcon {
  background-image: var(--jp-icon-copy);
}

.jp-CopyrightIcon {
  background-image: var(--jp-icon-copyright);
}

.jp-CutIcon {
  background-image: var(--jp-icon-cut);
}

.jp-DeleteIcon {
  background-image: var(--jp-icon-delete);
}

.jp-DownloadIcon {
  background-image: var(--jp-icon-download);
}

.jp-DuplicateIcon {
  background-image: var(--jp-icon-duplicate);
}

.jp-EditIcon {
  background-image: var(--jp-icon-edit);
}

.jp-EllipsesIcon {
  background-image: var(--jp-icon-ellipses);
}

.jp-ErrorIcon {
  background-image: var(--jp-icon-error);
}

.jp-ExpandAllIcon {
  background-image: var(--jp-icon-expand-all);
}

.jp-ExtensionIcon {
  background-image: var(--jp-icon-extension);
}

.jp-FastForwardIcon {
  background-image: var(--jp-icon-fast-forward);
}

.jp-FileIcon {
  background-image: var(--jp-icon-file);
}

.jp-FileUploadIcon {
  background-image: var(--jp-icon-file-upload);
}

.jp-FilterDotIcon {
  background-image: var(--jp-icon-filter-dot);
}

.jp-FilterIcon {
  background-image: var(--jp-icon-filter);
}

.jp-FilterListIcon {
  background-image: var(--jp-icon-filter-list);
}

.jp-FolderFavoriteIcon {
  background-image: var(--jp-icon-folder-favorite);
}

.jp-FolderIcon {
  background-image: var(--jp-icon-folder);
}

.jp-HomeIcon {
  background-image: var(--jp-icon-home);
}

.jp-Html5Icon {
  background-image: var(--jp-icon-html5);
}

.jp-ImageIcon {
  background-image: var(--jp-icon-image);
}

.jp-InfoIcon {
  background-image: var(--jp-icon-info);
}

.jp-InspectorIcon {
  background-image: var(--jp-icon-inspector);
}

.jp-JsonIcon {
  background-image: var(--jp-icon-json);
}

.jp-JuliaIcon {
  background-image: var(--jp-icon-julia);
}

.jp-JupyterFaviconIcon {
  background-image: var(--jp-icon-jupyter-favicon);
}

.jp-JupyterIcon {
  background-image: var(--jp-icon-jupyter);
}

.jp-JupyterlabWordmarkIcon {
  background-image: var(--jp-icon-jupyterlab-wordmark);
}

.jp-KernelIcon {
  background-image: var(--jp-icon-kernel);
}

.jp-KeyboardIcon {
  background-image: var(--jp-icon-keyboard);
}

.jp-LaunchIcon {
  background-image: var(--jp-icon-launch);
}

.jp-LauncherIcon {
  background-image: var(--jp-icon-launcher);
}

.jp-LineFormIcon {
  background-image: var(--jp-icon-line-form);
}

.jp-LinkIcon {
  background-image: var(--jp-icon-link);
}

.jp-ListIcon {
  background-image: var(--jp-icon-list);
}

.jp-MarkdownIcon {
  background-image: var(--jp-icon-markdown);
}

.jp-MoveDownIcon {
  background-image: var(--jp-icon-move-down);
}

.jp-MoveUpIcon {
  background-image: var(--jp-icon-move-up);
}

.jp-NewFolderIcon {
  background-image: var(--jp-icon-new-folder);
}

.jp-NotTrustedIcon {
  background-image: var(--jp-icon-not-trusted);
}

.jp-NotebookIcon {
  background-image: var(--jp-icon-notebook);
}

.jp-NumberingIcon {
  background-image: var(--jp-icon-numbering);
}

.jp-OfflineBoltIcon {
  background-image: var(--jp-icon-offline-bolt);
}

.jp-PaletteIcon {
  background-image: var(--jp-icon-palette);
}

.jp-PasteIcon {
  background-image: var(--jp-icon-paste);
}

.jp-PdfIcon {
  background-image: var(--jp-icon-pdf);
}

.jp-PythonIcon {
  background-image: var(--jp-icon-python);
}

.jp-RKernelIcon {
  background-image: var(--jp-icon-r-kernel);
}

.jp-ReactIcon {
  background-image: var(--jp-icon-react);
}

.jp-RedoIcon {
  background-image: var(--jp-icon-redo);
}

.jp-RefreshIcon {
  background-image: var(--jp-icon-refresh);
}

.jp-RegexIcon {
  background-image: var(--jp-icon-regex);
}

.jp-RunIcon {
  background-image: var(--jp-icon-run);
}

.jp-RunningIcon {
  background-image: var(--jp-icon-running);
}

.jp-SaveIcon {
  background-image: var(--jp-icon-save);
}

.jp-SearchIcon {
  background-image: var(--jp-icon-search);
}

.jp-SettingsIcon {
  background-image: var(--jp-icon-settings);
}

.jp-ShareIcon {
  background-image: var(--jp-icon-share);
}

.jp-SpreadsheetIcon {
  background-image: var(--jp-icon-spreadsheet);
}

.jp-StopIcon {
  background-image: var(--jp-icon-stop);
}

.jp-TabIcon {
  background-image: var(--jp-icon-tab);
}

.jp-TableRowsIcon {
  background-image: var(--jp-icon-table-rows);
}

.jp-TagIcon {
  background-image: var(--jp-icon-tag);
}

.jp-TerminalIcon {
  background-image: var(--jp-icon-terminal);
}

.jp-TextEditorIcon {
  background-image: var(--jp-icon-text-editor);
}

.jp-TocIcon {
  background-image: var(--jp-icon-toc);
}

.jp-TreeViewIcon {
  background-image: var(--jp-icon-tree-view);
}

.jp-TrustedIcon {
  background-image: var(--jp-icon-trusted);
}

.jp-UndoIcon {
  background-image: var(--jp-icon-undo);
}

.jp-UserIcon {
  background-image: var(--jp-icon-user);
}

.jp-UsersIcon {
  background-image: var(--jp-icon-users);
}

.jp-VegaIcon {
  background-image: var(--jp-icon-vega);
}

.jp-WordIcon {
  background-image: var(--jp-icon-word);
}

.jp-YamlIcon {
  background-image: var(--jp-icon-yaml);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * (DEPRECATED) Support for consuming icons as CSS background images
 */

.jp-Icon,
.jp-MaterialIcon {
  background-position: center;
  background-repeat: no-repeat;
  background-size: 16px;
  min-width: 16px;
  min-height: 16px;
}

.jp-Icon-cover {
  background-position: center;
  background-repeat: no-repeat;
  background-size: cover;
}

/**
 * (DEPRECATED) Support for specific CSS icon sizes
 */

.jp-Icon-16 {
  background-size: 16px;
  min-width: 16px;
  min-height: 16px;
}

.jp-Icon-18 {
  background-size: 18px;
  min-width: 18px;
  min-height: 18px;
}

.jp-Icon-20 {
  background-size: 20px;
  min-width: 20px;
  min-height: 20px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.lm-TabBar .lm-TabBar-addButton {
  align-items: center;
  display: flex;
  padding: 4px;
  padding-bottom: 5px;
  margin-right: 1px;
  background-color: var(--jp-layout-color2);
}

.lm-TabBar .lm-TabBar-addButton:hover {
  background-color: var(--jp-layout-color1);
}

.lm-DockPanel-tabBar .lm-TabBar-tab {
  width: var(--jp-private-horizontal-tab-width);
}

.lm-DockPanel-tabBar .lm-TabBar-content {
  flex: unset;
}

.lm-DockPanel-tabBar[data-orientation='horizontal'] {
  flex: 1 1 auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * Support for icons as inline SVG HTMLElements
 */

/* recolor the primary elements of an icon */
.jp-icon0[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon1[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon2[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon3[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon4[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon0[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon1[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon2[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon3[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon4[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/* recolor the accent elements of an icon */
.jp-icon-accent0[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-accent1[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-accent2[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-accent3[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-accent4[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-accent0[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-accent1[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-accent2[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-accent3[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-accent4[stroke] {
  stroke: var(--jp-layout-color4);
}

/* set the color of an icon to transparent */
.jp-icon-none[fill] {
  fill: none;
}

.jp-icon-none[stroke] {
  stroke: none;
}

/* brand icon colors. Same for light and dark */
.jp-icon-brand0[fill] {
  fill: var(--jp-brand-color0);
}

.jp-icon-brand1[fill] {
  fill: var(--jp-brand-color1);
}

.jp-icon-brand2[fill] {
  fill: var(--jp-brand-color2);
}

.jp-icon-brand3[fill] {
  fill: var(--jp-brand-color3);
}

.jp-icon-brand4[fill] {
  fill: var(--jp-brand-color4);
}

.jp-icon-brand0[stroke] {
  stroke: var(--jp-brand-color0);
}

.jp-icon-brand1[stroke] {
  stroke: var(--jp-brand-color1);
}

.jp-icon-brand2[stroke] {
  stroke: var(--jp-brand-color2);
}

.jp-icon-brand3[stroke] {
  stroke: var(--jp-brand-color3);
}

.jp-icon-brand4[stroke] {
  stroke: var(--jp-brand-color4);
}

/* warn icon colors. Same for light and dark */
.jp-icon-warn0[fill] {
  fill: var(--jp-warn-color0);
}

.jp-icon-warn1[fill] {
  fill: var(--jp-warn-color1);
}

.jp-icon-warn2[fill] {
  fill: var(--jp-warn-color2);
}

.jp-icon-warn3[fill] {
  fill: var(--jp-warn-color3);
}

.jp-icon-warn0[stroke] {
  stroke: var(--jp-warn-color0);
}

.jp-icon-warn1[stroke] {
  stroke: var(--jp-warn-color1);
}

.jp-icon-warn2[stroke] {
  stroke: var(--jp-warn-color2);
}

.jp-icon-warn3[stroke] {
  stroke: var(--jp-warn-color3);
}

/* icon colors that contrast well with each other and most backgrounds */
.jp-icon-contrast0[fill] {
  fill: var(--jp-icon-contrast-color0);
}

.jp-icon-contrast1[fill] {
  fill: var(--jp-icon-contrast-color1);
}

.jp-icon-contrast2[fill] {
  fill: var(--jp-icon-contrast-color2);
}

.jp-icon-contrast3[fill] {
  fill: var(--jp-icon-contrast-color3);
}

.jp-icon-contrast0[stroke] {
  stroke: var(--jp-icon-contrast-color0);
}

.jp-icon-contrast1[stroke] {
  stroke: var(--jp-icon-contrast-color1);
}

.jp-icon-contrast2[stroke] {
  stroke: var(--jp-icon-contrast-color2);
}

.jp-icon-contrast3[stroke] {
  stroke: var(--jp-icon-contrast-color3);
}

.jp-icon-dot[fill] {
  fill: var(--jp-warn-color0);
}

.jp-jupyter-icon-color[fill] {
  fill: var(--jp-jupyter-icon-color, var(--jp-warn-color0));
}

.jp-notebook-icon-color[fill] {
  fill: var(--jp-notebook-icon-color, var(--jp-warn-color0));
}

.jp-json-icon-color[fill] {
  fill: var(--jp-json-icon-color, var(--jp-warn-color1));
}

.jp-console-icon-color[fill] {
  fill: var(--jp-console-icon-color, white);
}

.jp-console-icon-background-color[fill] {
  fill: var(--jp-console-icon-background-color, var(--jp-brand-color1));
}

.jp-terminal-icon-color[fill] {
  fill: var(--jp-terminal-icon-color, var(--jp-layout-color2));
}

.jp-terminal-icon-background-color[fill] {
  fill: var(
    --jp-terminal-icon-background-color,
    var(--jp-inverse-layout-color2)
  );
}

.jp-text-editor-icon-color[fill] {
  fill: var(--jp-text-editor-icon-color, var(--jp-inverse-layout-color3));
}

.jp-inspector-icon-color[fill] {
  fill: var(--jp-inspector-icon-color, var(--jp-inverse-layout-color3));
}

/* CSS for icons in selected filebrowser listing items */
.jp-DirListing-item.jp-mod-selected .jp-icon-selectable[fill] {
  fill: #fff;
}

.jp-DirListing-item.jp-mod-selected .jp-icon-selectable-inverse[fill] {
  fill: var(--jp-brand-color1);
}

/* stylelint-disable selector-max-class, selector-max-compound-selectors */

/**
* TODO: come up with non css-hack solution for showing the busy icon on top
*  of the close icon
* CSS for complex behavior of close icon of tabs in the main area tabbar
*/
.lm-DockPanel-tabBar
  .lm-TabBar-tab.lm-mod-closable.jp-mod-dirty
  > .lm-TabBar-tabCloseIcon
  > :not(:hover)
  > .jp-icon3[fill] {
  fill: none;
}

.lm-DockPanel-tabBar
  .lm-TabBar-tab.lm-mod-closable.jp-mod-dirty
  > .lm-TabBar-tabCloseIcon
  > :not(:hover)
  > .jp-icon-busy[fill] {
  fill: var(--jp-inverse-layout-color3);
}

/* stylelint-enable selector-max-class, selector-max-compound-selectors */

/* CSS for icons in status bar */
#jp-main-statusbar .jp-mod-selected .jp-icon-selectable[fill] {
  fill: #fff;
}

#jp-main-statusbar .jp-mod-selected .jp-icon-selectable-inverse[fill] {
  fill: var(--jp-brand-color1);
}

/* special handling for splash icon CSS. While the theme CSS reloads during
   splash, the splash icon can loose theming. To prevent that, we set a
   default for its color variable */
:root {
  --jp-warn-color0: var(--md-orange-700);
}

/* not sure what to do with this one, used in filebrowser listing */
.jp-DragIcon {
  margin-right: 4px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * Support for alt colors for icons as inline SVG HTMLElements
 */

/* alt recolor the primary elements of an icon */
.jp-icon-alt .jp-icon0[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-alt .jp-icon1[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-alt .jp-icon2[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-alt .jp-icon3[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-alt .jp-icon4[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-alt .jp-icon0[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-alt .jp-icon1[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-alt .jp-icon2[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-alt .jp-icon3[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-alt .jp-icon4[stroke] {
  stroke: var(--jp-layout-color4);
}

/* alt recolor the accent elements of an icon */
.jp-icon-alt .jp-icon-accent0[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon-alt .jp-icon-accent1[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon-alt .jp-icon-accent2[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon-alt .jp-icon-accent3[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon-alt .jp-icon-accent4[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-alt .jp-icon-accent0[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon-alt .jp-icon-accent1[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon-alt .jp-icon-accent2[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon-alt .jp-icon-accent3[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon-alt .jp-icon-accent4[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-icon-hoverShow:not(:hover) .jp-icon-hoverShow-content {
  display: none !important;
}

/**
 * Support for hover colors for icons as inline SVG HTMLElements
 */

/**
 * regular colors
 */

/* recolor the primary elements of an icon */
.jp-icon-hover :hover .jp-icon0-hover[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon-hover :hover .jp-icon1-hover[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon-hover :hover .jp-icon2-hover[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon-hover :hover .jp-icon3-hover[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon-hover :hover .jp-icon4-hover[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-hover :hover .jp-icon0-hover[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon-hover :hover .jp-icon1-hover[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon-hover :hover .jp-icon2-hover[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon-hover :hover .jp-icon3-hover[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon-hover :hover .jp-icon4-hover[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/* recolor the accent elements of an icon */
.jp-icon-hover :hover .jp-icon-accent0-hover[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-hover :hover .jp-icon-accent1-hover[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-hover :hover .jp-icon-accent2-hover[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-hover :hover .jp-icon-accent3-hover[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-hover :hover .jp-icon-accent4-hover[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-hover :hover .jp-icon-accent0-hover[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-hover :hover .jp-icon-accent1-hover[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-hover :hover .jp-icon-accent2-hover[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-hover :hover .jp-icon-accent3-hover[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-hover :hover .jp-icon-accent4-hover[stroke] {
  stroke: var(--jp-layout-color4);
}

/* set the color of an icon to transparent */
.jp-icon-hover :hover .jp-icon-none-hover[fill] {
  fill: none;
}

.jp-icon-hover :hover .jp-icon-none-hover[stroke] {
  stroke: none;
}

/**
 * inverse colors
 */

/* inverse recolor the primary elements of an icon */
.jp-icon-hover.jp-icon-alt :hover .jp-icon0-hover[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon1-hover[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon2-hover[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon3-hover[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon4-hover[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon0-hover[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon1-hover[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon2-hover[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon3-hover[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon4-hover[stroke] {
  stroke: var(--jp-layout-color4);
}

/* inverse recolor the accent elements of an icon */
.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent0-hover[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent1-hover[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent2-hover[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent3-hover[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent4-hover[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent0-hover[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent1-hover[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent2-hover[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent3-hover[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent4-hover[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-IFrame {
  width: 100%;
  height: 100%;
}

.jp-IFrame > iframe {
  border: none;
}

/*
When drag events occur, `lm-mod-override-cursor` is added to the body.
Because iframes steal all cursor events, the following two rules are necessary
to suppress pointer events while resize drags are occurring. There may be a
better solution to this problem.
*/
body.lm-mod-override-cursor .jp-IFrame {
  position: relative;
}

body.lm-mod-override-cursor .jp-IFrame::before {
  content: '';
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-HoverBox {
  position: fixed;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-FormGroup-content fieldset {
  border: none;
  padding: 0;
  min-width: 0;
  width: 100%;
}

/* stylelint-disable selector-max-type */

.jp-FormGroup-content fieldset .jp-inputFieldWrapper input,
.jp-FormGroup-content fieldset .jp-inputFieldWrapper select,
.jp-FormGroup-content fieldset .jp-inputFieldWrapper textarea {
  font-size: var(--jp-content-font-size2);
  border-color: var(--jp-input-border-color);
  border-style: solid;
  border-radius: var(--jp-border-radius);
  border-width: 1px;
  padding: 6px 8px;
  background: none;
  color: var(--jp-ui-font-color0);
  height: inherit;
}

.jp-FormGroup-content fieldset input[type='checkbox'] {
  position: relative;
  top: 2px;
  margin-left: 0;
}

.jp-FormGroup-content button.jp-mod-styled {
  cursor: pointer;
}

.jp-FormGroup-content .checkbox label {
  cursor: pointer;
  font-size: var(--jp-content-font-size1);
}

.jp-FormGroup-content .jp-root > fieldset > legend {
  display: none;
}

.jp-FormGroup-content .jp-root > fieldset > p {
  display: none;
}

/** copy of `input.jp-mod-styled:focus` style */
.jp-FormGroup-content fieldset input:focus,
.jp-FormGroup-content fieldset select:focus {
  -moz-outline-radius: unset;
  outline: var(--jp-border-width) solid var(--md-blue-500);
  outline-offset: -1px;
  box-shadow: inset 0 0 4px var(--md-blue-300);
}

.jp-FormGroup-content fieldset input:hover:not(:focus),
.jp-FormGroup-content fieldset select:hover:not(:focus) {
  background-color: var(--jp-border-color2);
}

/* stylelint-enable selector-max-type */

.jp-FormGroup-content .checkbox .field-description {
  /* Disable default description field for checkbox:
   because other widgets do not have description fields,
   we add descriptions to each widget on the field level.
  */
  display: none;
}

.jp-FormGroup-content #root__description {
  display: none;
}

.jp-FormGroup-content .jp-modifiedIndicator {
  width: 5px;
  background-color: var(--jp-brand-color2);
  margin-top: 0;
  margin-left: calc(var(--jp-private-settingeditor-modifier-indent) * -1);
  flex-shrink: 0;
}

.jp-FormGroup-content .jp-modifiedIndicator.jp-errorIndicator {
  background-color: var(--jp-error-color0);
  margin-right: 0.5em;
}

/* RJSF ARRAY style */

.jp-arrayFieldWrapper legend {
  font-size: var(--jp-content-font-size2);
  color: var(--jp-ui-font-color0);
  flex-basis: 100%;
  padding: 4px 0;
  font-weight: var(--jp-content-heading-font-weight);
  border-bottom: 1px solid var(--jp-border-color2);
}

.jp-arrayFieldWrapper .field-description {
  padding: 4px 0;
  white-space: pre-wrap;
}

.jp-arrayFieldWrapper .array-item {
  width: 100%;
  border: 1px solid var(--jp-border-color2);
  border-radius: 4px;
  margin: 4px;
}

.jp-ArrayOperations {
  display: flex;
  margin-left: 8px;
}

.jp-ArrayOperationsButton {
  margin: 2px;
}

.jp-ArrayOperationsButton .jp-icon3[fill] {
  fill: var(--jp-ui-font-color0);
}

button.jp-ArrayOperationsButton.jp-mod-styled:disabled {
  cursor: not-allowed;
  opacity: 0.5;
}

/* RJSF form validation error */

.jp-FormGroup-content .validationErrors {
  color: var(--jp-error-color0);
}

/* Hide panel level error as duplicated the field level error */
.jp-FormGroup-content .panel.errors {
  display: none;
}

/* RJSF normal content (settings-editor) */

.jp-FormGroup-contentNormal {
  display: flex;
  align-items: center;
  flex-wrap: wrap;
}

.jp-FormGroup-contentNormal .jp-FormGroup-contentItem {
  margin-left: 7px;
  color: var(--jp-ui-font-color0);
}

.jp-FormGroup-contentNormal .jp-FormGroup-description {
  flex-basis: 100%;
  padding: 4px 7px;
}

.jp-FormGroup-contentNormal .jp-FormGroup-default {
  flex-basis: 100%;
  padding: 4px 7px;
}

.jp-FormGroup-contentNormal .jp-FormGroup-fieldLabel {
  font-size: var(--jp-content-font-size1);
  font-weight: normal;
  min-width: 120px;
}

.jp-FormGroup-contentNormal fieldset:not(:first-child) {
  margin-left: 7px;
}

.jp-FormGroup-contentNormal .field-array-of-string .array-item {
  /* Display `jp-ArrayOperations` buttons side-by-side with content except
    for small screens where flex-wrap will place them one below the other.
  */
  display: flex;
  align-items: center;
  flex-wrap: wrap;
}

.jp-FormGroup-contentNormal .jp-objectFieldWrapper .form-group {
  padding: 2px 8px 2px var(--jp-private-settingeditor-modifier-indent);
  margin-top: 2px;
}

/* RJSF compact content (metadata-form) */

.jp-FormGroup-content.jp-FormGroup-contentCompact {
  width: 100%;
}

.jp-FormGroup-contentCompact .form-group {
  display: flex;
  padding: 0.5em 0.2em 0.5em 0;
}

.jp-FormGroup-contentCompact
  .jp-FormGroup-compactTitle
  .jp-FormGroup-description {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color2);
}

.jp-FormGroup-contentCompact .jp-FormGroup-fieldLabel {
  padding-bottom: 0.3em;
}

.jp-FormGroup-contentCompact .jp-inputFieldWrapper .form-control {
  width: 100%;
  box-sizing: border-box;
}

.jp-FormGroup-contentCompact .jp-arrayFieldWrapper .jp-FormGroup-compactTitle {
  padding-bottom: 7px;
}

.jp-FormGroup-contentCompact
  .jp-objectFieldWrapper
  .jp-objectFieldWrapper
  .form-group {
  padding: 2px 8px 2px var(--jp-private-settingeditor-modifier-indent);
  margin-top: 2px;
}

.jp-FormGroup-contentCompact ul.error-detail {
  margin-block-start: 0.5em;
  margin-block-end: 0.5em;
  padding-inline-start: 1em;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-SidePanel {
  display: flex;
  flex-direction: column;
  min-width: var(--jp-sidebar-min-width);
  overflow-y: auto;
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);
  font-size: var(--jp-ui-font-size1);
}

.jp-SidePanel-header {
  flex: 0 0 auto;
  display: flex;
  border-bottom: var(--jp-border-width) solid var(--jp-border-color2);
  font-size: var(--jp-ui-font-size0);
  font-weight: 600;
  letter-spacing: 1px;
  margin: 0;
  padding: 2px;
  text-transform: uppercase;
}

.jp-SidePanel-toolbar {
  flex: 0 0 auto;
}

.jp-SidePanel-content {
  flex: 1 1 auto;
}

.jp-SidePanel-toolbar,
.jp-AccordionPanel-toolbar {
  height: var(--jp-private-toolbar-height);
}

.jp-SidePanel-toolbar.jp-Toolbar-micro {
  display: none;
}

.lm-AccordionPanel .jp-AccordionPanel-title {
  box-sizing: border-box;
  line-height: 25px;
  margin: 0;
  display: flex;
  align-items: center;
  background: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  box-shadow: var(--jp-toolbar-box-shadow);
  font-size: var(--jp-ui-font-size0);
}

.jp-AccordionPanel-title {
  cursor: pointer;
  user-select: none;
  -moz-user-select: none;
  -webkit-user-select: none;
  text-transform: uppercase;
}

.lm-AccordionPanel[data-orientation='horizontal'] > .jp-AccordionPanel-title {
  /* Title is rotated for horizontal accordion panel using CSS */
  display: block;
  transform-origin: top left;
  transform: rotate(-90deg) translate(-100%);
}

.jp-AccordionPanel-title .lm-AccordionPanel-titleLabel {
  user-select: none;
  text-overflow: ellipsis;
  white-space: nowrap;
  overflow: hidden;
}

.jp-AccordionPanel-title .lm-AccordionPanel-titleCollapser {
  transform: rotate(-90deg);
  margin: auto 0;
  height: 16px;
}

.jp-AccordionPanel-title.lm-mod-expanded .lm-AccordionPanel-titleCollapser {
  transform: rotate(0deg);
}

.lm-AccordionPanel .jp-AccordionPanel-toolbar {
  background: none;
  box-shadow: none;
  border: none;
  margin-left: auto;
}

.lm-AccordionPanel .lm-SplitPanel-handle:hover {
  background: var(--jp-layout-color3);
}

.jp-text-truncated {
  overflow: hidden;
  text-overflow: ellipsis;
  white-space: nowrap;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Spinner {
  position: absolute;
  display: flex;
  justify-content: center;
  align-items: center;
  z-index: 10;
  left: 0;
  top: 0;
  width: 100%;
  height: 100%;
  background: var(--jp-layout-color0);
  outline: none;
}

.jp-SpinnerContent {
  font-size: 10px;
  margin: 50px auto;
  text-indent: -9999em;
  width: 3em;
  height: 3em;
  border-radius: 50%;
  background: var(--jp-brand-color3);
  background: linear-gradient(
    to right,
    #f37626 10%,
    rgba(255, 255, 255, 0) 42%
  );
  position: relative;
  animation: load3 1s infinite linear, fadeIn 1s;
}

.jp-SpinnerContent::before {
  width: 50%;
  height: 50%;
  background: #f37626;
  border-radius: 100% 0 0;
  position: absolute;
  top: 0;
  left: 0;
  content: '';
}

.jp-SpinnerContent::after {
  background: var(--jp-layout-color0);
  width: 75%;
  height: 75%;
  border-radius: 50%;
  content: '';
  margin: auto;
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  right: 0;
}

@keyframes fadeIn {
  0% {
    opacity: 0;
  }

  100% {
    opacity: 1;
  }
}

@keyframes load3 {
  0% {
    transform: rotate(0deg);
  }

  100% {
    transform: rotate(360deg);
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

button.jp-mod-styled {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  border: none;
  box-sizing: border-box;
  text-align: center;
  line-height: 32px;
  height: 32px;
  padding: 0 12px;
  letter-spacing: 0.8px;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

input.jp-mod-styled {
  background: var(--jp-input-background);
  height: 28px;
  box-sizing: border-box;
  border: var(--jp-border-width) solid var(--jp-border-color1);
  padding-left: 7px;
  padding-right: 7px;
  font-size: var(--jp-ui-font-size2);
  color: var(--jp-ui-font-color0);
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

input[type='checkbox'].jp-mod-styled {
  appearance: checkbox;
  -webkit-appearance: checkbox;
  -moz-appearance: checkbox;
  height: auto;
}

input.jp-mod-styled:focus {
  border: var(--jp-border-width) solid var(--md-blue-500);
  box-shadow: inset 0 0 4px var(--md-blue-300);
}

.jp-select-wrapper {
  display: flex;
  position: relative;
  flex-direction: column;
  padding: 1px;
  background-color: var(--jp-layout-color1);
  box-sizing: border-box;
  margin-bottom: 12px;
}

.jp-select-wrapper:not(.multiple) {
  height: 28px;
}

.jp-select-wrapper.jp-mod-focused select.jp-mod-styled {
  border: var(--jp-border-width) solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
  background-color: var(--jp-input-active-background);
}

select.jp-mod-styled:hover {
  cursor: pointer;
  color: var(--jp-ui-font-color0);
  background-color: var(--jp-input-hover-background);
  box-shadow: inset 0 0 1px rgba(0, 0, 0, 0.5);
}

select.jp-mod-styled {
  flex: 1 1 auto;
  width: 100%;
  font-size: var(--jp-ui-font-size2);
  background: var(--jp-input-background);
  color: var(--jp-ui-font-color0);
  padding: 0 25px 0 8px;
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  border-radius: 0;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

select.jp-mod-styled:not([multiple]) {
  height: 32px;
}

select.jp-mod-styled[multiple] {
  max-height: 200px;
  overflow-y: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-switch {
  display: flex;
  align-items: center;
  padding-left: 4px;
  padding-right: 4px;
  font-size: var(--jp-ui-font-size1);
  background-color: transparent;
  color: var(--jp-ui-font-color1);
  border: none;
  height: 20px;
}

.jp-switch:hover {
  background-color: var(--jp-layout-color2);
}

.jp-switch-label {
  margin-right: 5px;
  font-family: var(--jp-ui-font-family);
}

.jp-switch-track {
  cursor: pointer;
  background-color: var(--jp-switch-color, var(--jp-border-color1));
  -webkit-transition: 0.4s;
  transition: 0.4s;
  border-radius: 34px;
  height: 16px;
  width: 35px;
  position: relative;
}

.jp-switch-track::before {
  content: '';
  position: absolute;
  height: 10px;
  width: 10px;
  margin: 3px;
  left: 0;
  background-color: var(--jp-ui-inverse-font-color1);
  -webkit-transition: 0.4s;
  transition: 0.4s;
  border-radius: 50%;
}

.jp-switch[aria-checked='true'] .jp-switch-track {
  background-color: var(--jp-switch-true-position-color, var(--jp-warn-color0));
}

.jp-switch[aria-checked='true'] .jp-switch-track::before {
  /* track width (35) - margins (3 + 3) - thumb width (10) */
  left: 19px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

:root {
  --jp-private-toolbar-height: calc(
    28px + var(--jp-border-width)
  ); /* leave 28px for content */
}

.jp-Toolbar {
  color: var(--jp-ui-font-color1);
  flex: 0 0 auto;
  display: flex;
  flex-direction: row;
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  box-shadow: var(--jp-toolbar-box-shadow);
  background: var(--jp-toolbar-background);
  min-height: var(--jp-toolbar-micro-height);
  padding: 2px;
  z-index: 8;
  overflow-x: hidden;
}

/* Toolbar items */

.jp-Toolbar > .jp-Toolbar-item.jp-Toolbar-spacer {
  flex-grow: 1;
  flex-shrink: 1;
}

.jp-Toolbar-item.jp-Toolbar-kernelStatus {
  display: inline-block;
  width: 32px;
  background-repeat: no-repeat;
  background-position: center;
  background-size: 16px;
}

.jp-Toolbar > .jp-Toolbar-item {
  flex: 0 0 auto;
  display: flex;
  padding-left: 1px;
  padding-right: 1px;
  font-size: var(--jp-ui-font-size1);
  line-height: var(--jp-private-toolbar-height);
  height: 100%;
}

/* Toolbar buttons */

/* This is the div we use to wrap the react component into a Widget */
div.jp-ToolbarButton {
  color: transparent;
  border: none;
  box-sizing: border-box;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
  padding: 0;
  margin: 0;
}

button.jp-ToolbarButtonComponent {
  background: var(--jp-layout-color1);
  border: none;
  box-sizing: border-box;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
  padding: 0 6px;
  margin: 0;
  height: 24px;
  border-radius: var(--jp-border-radius);
  display: flex;
  align-items: center;
  text-align: center;
  font-size: 14px;
  min-width: unset;
  min-height: unset;
}

button.jp-ToolbarButtonComponent:disabled {
  opacity: 0.4;
}

button.jp-ToolbarButtonComponent > span {
  padding: 0;
  flex: 0 0 auto;
}

button.jp-ToolbarButtonComponent .jp-ToolbarButtonComponent-label {
  font-size: var(--jp-ui-font-size1);
  line-height: 100%;
  padding-left: 2px;
  color: var(--jp-ui-font-color1);
  font-family: var(--jp-ui-font-family);
}

#jp-main-dock-panel[data-mode='single-document']
  .jp-MainAreaWidget
  > .jp-Toolbar.jp-Toolbar-micro {
  padding: 0;
  min-height: 0;
}

#jp-main-dock-panel[data-mode='single-document']
  .jp-MainAreaWidget
  > .jp-Toolbar {
  border: none;
  box-shadow: none;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-WindowedPanel-outer {
  position: relative;
  overflow-y: auto;
}

.jp-WindowedPanel-inner {
  position: relative;
}

.jp-WindowedPanel-window {
  position: absolute;
  left: 0;
  right: 0;
  overflow: visible;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* Sibling imports */

body {
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
}

/* Disable native link decoration styles everywhere outside of dialog boxes */
a {
  text-decoration: unset;
  color: unset;
}

a:hover {
  text-decoration: unset;
  color: unset;
}

/* Accessibility for links inside dialog box text */
.jp-Dialog-content a {
  text-decoration: revert;
  color: var(--jp-content-link-color);
}

.jp-Dialog-content a:hover {
  text-decoration: revert;
}

/* Styles for ui-components */
.jp-Button {
  color: var(--jp-ui-font-color2);
  border-radius: var(--jp-border-radius);
  padding: 0 12px;
  font-size: var(--jp-ui-font-size1);

  /* Copy from blueprint 3 */
  display: inline-flex;
  flex-direction: row;
  border: none;
  cursor: pointer;
  align-items: center;
  justify-content: center;
  text-align: left;
  vertical-align: middle;
  min-height: 30px;
  min-width: 30px;
}

.jp-Button:disabled {
  cursor: not-allowed;
}

.jp-Button:empty {
  padding: 0 !important;
}

.jp-Button.jp-mod-small {
  min-height: 24px;
  min-width: 24px;
  font-size: 12px;
  padding: 0 7px;
}

/* Use our own theme for hover styles */
.jp-Button.jp-mod-minimal:hover {
  background-color: var(--jp-layout-color2);
}

.jp-Button.jp-mod-minimal {
  background: none;
}

.jp-InputGroup {
  display: block;
  position: relative;
}

.jp-InputGroup input {
  box-sizing: border-box;
  border: none;
  border-radius: 0;
  background-color: transparent;
  color: var(--jp-ui-font-color0);
  box-shadow: inset 0 0 0 var(--jp-border-width) var(--jp-input-border-color);
  padding-bottom: 0;
  padding-top: 0;
  padding-left: 10px;
  padding-right: 28px;
  position: relative;
  width: 100%;
  -webkit-appearance: none;
  -moz-appearance: none;
  appearance: none;
  font-size: 14px;
  font-weight: 400;
  height: 30px;
  line-height: 30px;
  outline: none;
  vertical-align: middle;
}

.jp-InputGroup input:focus {
  box-shadow: inset 0 0 0 var(--jp-border-width)
      var(--jp-input-active-box-shadow-color),
    inset 0 0 0 3px var(--jp-input-active-box-shadow-color);
}

.jp-InputGroup input:disabled {
  cursor: not-allowed;
  resize: block;
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color2);
}

.jp-InputGroup input:disabled ~ span {
  cursor: not-allowed;
  color: var(--jp-ui-font-color2);
}

.jp-InputGroup input::placeholder,
input::placeholder {
  color: var(--jp-ui-font-color2);
}

.jp-InputGroupAction {
  position: absolute;
  bottom: 1px;
  right: 0;
  padding: 6px;
}

.jp-HTMLSelect.jp-DefaultStyle select {
  background-color: initial;
  border: none;
  border-radius: 0;
  box-shadow: none;
  color: var(--jp-ui-font-color0);
  display: block;
  font-size: var(--jp-ui-font-size1);
  font-family: var(--jp-ui-font-family);
  height: 24px;
  line-height: 14px;
  padding: 0 25px 0 10px;
  text-align: left;
  -moz-appearance: none;
  -webkit-appearance: none;
}

.jp-HTMLSelect.jp-DefaultStyle select:disabled {
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color2);
  cursor: not-allowed;
  resize: block;
}

.jp-HTMLSelect.jp-DefaultStyle select:disabled ~ span {
  cursor: not-allowed;
}

/* Use our own theme for hover and option styles */
/* stylelint-disable-next-line selector-max-type */
.jp-HTMLSelect.jp-DefaultStyle select:hover,
.jp-HTMLSelect.jp-DefaultStyle select > option {
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color0);
}

select {
  box-sizing: border-box;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Styles
|----------------------------------------------------------------------------*/

.jp-StatusBar-Widget {
  display: flex;
  align-items: center;
  background: var(--jp-layout-color2);
  min-height: var(--jp-statusbar-height);
  justify-content: space-between;
  padding: 0 10px;
}

.jp-StatusBar-Left {
  display: flex;
  align-items: center;
  flex-direction: row;
}

.jp-StatusBar-Middle {
  display: flex;
  align-items: center;
}

.jp-StatusBar-Right {
  display: flex;
  align-items: center;
  flex-direction: row-reverse;
}

.jp-StatusBar-Item {
  max-height: var(--jp-statusbar-height);
  margin: 0 2px;
  height: var(--jp-statusbar-height);
  white-space: nowrap;
  text-overflow: ellipsis;
  color: var(--jp-ui-font-color1);
  padding: 0 6px;
}

.jp-mod-highlighted:hover {
  background-color: var(--jp-layout-color3);
}

.jp-mod-clicked {
  background-color: var(--jp-brand-color1);
}

.jp-mod-clicked:hover {
  background-color: var(--jp-brand-color0);
}

.jp-mod-clicked .jp-StatusBar-TextItem {
  color: var(--jp-ui-inverse-font-color1);
}

.jp-StatusBar-HoverItem {
  box-shadow: '0px 4px 4px rgba(0, 0, 0, 0.25)';
}

.jp-StatusBar-TextItem {
  font-size: var(--jp-ui-font-size1);
  font-family: var(--jp-ui-font-family);
  line-height: 24px;
  color: var(--jp-ui-font-color1);
}

.jp-StatusBar-GroupItem {
  display: flex;
  align-items: center;
  flex-direction: row;
}

.jp-Statusbar-ProgressCircle svg {
  display: block;
  margin: 0 auto;
  width: 16px;
  height: 24px;
  align-self: normal;
}

.jp-Statusbar-ProgressCircle path {
  fill: var(--jp-inverse-layout-color3);
}

.jp-Statusbar-ProgressBar-progress-bar {
  height: 10px;
  width: 100px;
  border: solid 0.25px var(--jp-brand-color2);
  border-radius: 3px;
  overflow: hidden;
  align-self: center;
}

.jp-Statusbar-ProgressBar-progress-bar > div {
  background-color: var(--jp-brand-color2);
  background-image: linear-gradient(
    -45deg,
    rgba(255, 255, 255, 0.2) 25%,
    transparent 25%,
    transparent 50%,
    rgba(255, 255, 255, 0.2) 50%,
    rgba(255, 255, 255, 0.2) 75%,
    transparent 75%,
    transparent
  );
  background-size: 40px 40px;
  float: left;
  width: 0%;
  height: 100%;
  font-size: 12px;
  line-height: 14px;
  color: #fff;
  text-align: center;
  animation: jp-Statusbar-ExecutionTime-progress-bar 2s linear infinite;
}

.jp-Statusbar-ProgressBar-progress-bar p {
  color: var(--jp-ui-font-color1);
  font-family: var(--jp-ui-font-family);
  font-size: var(--jp-ui-font-size1);
  line-height: 10px;
  width: 100px;
}

@keyframes jp-Statusbar-ExecutionTime-progress-bar {
  0% {
    background-position: 0 0;
  }

  100% {
    background-position: 40px 40px;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-commandpalette-search-height: 28px;
}

/*-----------------------------------------------------------------------------
| Overall styles
|----------------------------------------------------------------------------*/

.lm-CommandPalette {
  padding-bottom: 0;
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);

  /* This is needed so that all font sizing of children done in ems is
   * relative to this base size */
  font-size: var(--jp-ui-font-size1);
}

/*-----------------------------------------------------------------------------
| Modal variant
|----------------------------------------------------------------------------*/

.jp-ModalCommandPalette {
  position: absolute;
  z-index: 10000;
  top: 38px;
  left: 30%;
  margin: 0;
  padding: 4px;
  width: 40%;
  box-shadow: var(--jp-elevation-z4);
  border-radius: 4px;
  background: var(--jp-layout-color0);
}

.jp-ModalCommandPalette .lm-CommandPalette {
  max-height: 40vh;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-close-icon::after {
  display: none;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-CommandPalette-header {
  display: none;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-CommandPalette-item {
  margin-left: 4px;
  margin-right: 4px;
}

.jp-ModalCommandPalette
  .lm-CommandPalette
  .lm-CommandPalette-item.lm-mod-disabled {
  display: none;
}

/*-----------------------------------------------------------------------------
| Search
|----------------------------------------------------------------------------*/

.lm-CommandPalette-search {
  padding: 4px;
  background-color: var(--jp-layout-color1);
  z-index: 2;
}

.lm-CommandPalette-wrapper {
  overflow: overlay;
  padding: 0 9px;
  background-color: var(--jp-input-active-background);
  height: 30px;
  box-shadow: inset 0 0 0 var(--jp-border-width) var(--jp-input-border-color);
}

.lm-CommandPalette.lm-mod-focused .lm-CommandPalette-wrapper {
  box-shadow: inset 0 0 0 1px var(--jp-input-active-box-shadow-color),
    inset 0 0 0 3px var(--jp-input-active-box-shadow-color);
}

.jp-SearchIconGroup {
  color: white;
  background-color: var(--jp-brand-color1);
  position: absolute;
  top: 4px;
  right: 4px;
  padding: 5px 5px 1px;
}

.jp-SearchIconGroup svg {
  height: 20px;
  width: 20px;
}

.jp-SearchIconGroup .jp-icon3[fill] {
  fill: var(--jp-layout-color0);
}

.lm-CommandPalette-input {
  background: transparent;
  width: calc(100% - 18px);
  float: left;
  border: none;
  outline: none;
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  line-height: var(--jp-private-commandpalette-search-height);
}

.lm-CommandPalette-input::-webkit-input-placeholder,
.lm-CommandPalette-input::-moz-placeholder,
.lm-CommandPalette-input:-ms-input-placeholder {
  color: var(--jp-ui-font-color2);
  font-size: var(--jp-ui-font-size1);
}

/*-----------------------------------------------------------------------------
| Results
|----------------------------------------------------------------------------*/

.lm-CommandPalette-header:first-child {
  margin-top: 0;
}

.lm-CommandPalette-header {
  border-bottom: solid var(--jp-border-width) var(--jp-border-color2);
  color: var(--jp-ui-font-color1);
  cursor: pointer;
  display: flex;
  font-size: var(--jp-ui-font-size0);
  font-weight: 600;
  letter-spacing: 1px;
  margin-top: 8px;
  padding: 8px 0 8px 12px;
  text-transform: uppercase;
}

.lm-CommandPalette-header.lm-mod-active {
  background: var(--jp-layout-color2);
}

.lm-CommandPalette-header > mark {
  background-color: transparent;
  font-weight: bold;
  color: var(--jp-ui-font-color1);
}

.lm-CommandPalette-item {
  padding: 4px 12px 4px 4px;
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  font-weight: 400;
  display: flex;
}

.lm-CommandPalette-item.lm-mod-disabled {
  color: var(--jp-ui-font-color2);
}

.lm-CommandPalette-item.lm-mod-active {
  color: var(--jp-ui-inverse-font-color1);
  background: var(--jp-brand-color1);
}

.lm-CommandPalette-item.lm-mod-active .lm-CommandPalette-itemLabel > mark {
  color: var(--jp-ui-inverse-font-color0);
}

.lm-CommandPalette-item.lm-mod-active .jp-icon-selectable[fill] {
  fill: var(--jp-layout-color0);
}

.lm-CommandPalette-item.lm-mod-active:hover:not(.lm-mod-disabled) {
  color: var(--jp-ui-inverse-font-color1);
  background: var(--jp-brand-color1);
}

.lm-CommandPalette-item:hover:not(.lm-mod-active):not(.lm-mod-disabled) {
  background: var(--jp-layout-color2);
}

.lm-CommandPalette-itemContent {
  overflow: hidden;
}

.lm-CommandPalette-itemLabel > mark {
  color: var(--jp-ui-font-color0);
  background-color: transparent;
  font-weight: bold;
}

.lm-CommandPalette-item.lm-mod-disabled mark {
  color: var(--jp-ui-font-color2);
}

.lm-CommandPalette-item .lm-CommandPalette-itemIcon {
  margin: 0 4px 0 0;
  position: relative;
  width: 16px;
  top: 2px;
  flex: 0 0 auto;
}

.lm-CommandPalette-item.lm-mod-disabled .lm-CommandPalette-itemIcon {
  opacity: 0.6;
}

.lm-CommandPalette-item .lm-CommandPalette-itemShortcut {
  flex: 0 0 auto;
}

.lm-CommandPalette-itemCaption {
  display: none;
}

.lm-CommandPalette-content {
  background-color: var(--jp-layout-color1);
}

.lm-CommandPalette-content:empty::after {
  content: 'No results';
  margin: auto;
  margin-top: 20px;
  width: 100px;
  display: block;
  font-size: var(--jp-ui-font-size2);
  font-family: var(--jp-ui-font-family);
  font-weight: lighter;
}

.lm-CommandPalette-emptyMessage {
  text-align: center;
  margin-top: 24px;
  line-height: 1.32;
  padding: 0 8px;
  color: var(--jp-content-font-color3);
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Dialog {
  position: absolute;
  z-index: 10000;
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  top: 0;
  left: 0;
  margin: 0;
  padding: 0;
  width: 100%;
  height: 100%;
  background: var(--jp-dialog-background);
}

.jp-Dialog-content {
  display: flex;
  flex-direction: column;
  margin-left: auto;
  margin-right: auto;
  background: var(--jp-layout-color1);
  padding: 24px 24px 12px;
  min-width: 300px;
  min-height: 150px;
  max-width: 1000px;
  max-height: 500px;
  box-sizing: border-box;
  box-shadow: var(--jp-elevation-z20);
  word-wrap: break-word;
  border-radius: var(--jp-border-radius);

  /* This is needed so that all font sizing of children done in ems is
   * relative to this base size */
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color1);
  resize: both;
}

.jp-Dialog-content.jp-Dialog-content-small {
  max-width: 500px;
}

.jp-Dialog-button {
  overflow: visible;
}

button.jp-Dialog-button:focus {
  outline: 1px solid var(--jp-brand-color1);
  outline-offset: 4px;
  -moz-outline-radius: 0;
}

button.jp-Dialog-button:focus::-moz-focus-inner {
  border: 0;
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-accept:focus,
button.jp-Dialog-button.jp-mod-styled.jp-mod-warn:focus,
button.jp-Dialog-button.jp-mod-styled.jp-mod-reject:focus {
  outline-offset: 4px;
  -moz-outline-radius: 0;
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-accept:focus {
  outline: 1px solid var(--jp-accept-color-normal, var(--jp-brand-color1));
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-warn:focus {
  outline: 1px solid var(--jp-warn-color-normal, var(--jp-error-color1));
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-reject:focus {
  outline: 1px solid var(--jp-reject-color-normal, var(--md-grey-600));
}

button.jp-Dialog-close-button {
  padding: 0;
  height: 100%;
  min-width: unset;
  min-height: unset;
}

.jp-Dialog-header {
  display: flex;
  justify-content: space-between;
  flex: 0 0 auto;
  padding-bottom: 12px;
  font-size: var(--jp-ui-font-size3);
  font-weight: 400;
  color: var(--jp-ui-font-color1);
}

.jp-Dialog-body {
  display: flex;
  flex-direction: column;
  flex: 1 1 auto;
  font-size: var(--jp-ui-font-size1);
  background: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  overflow: auto;
}

.jp-Dialog-footer {
  display: flex;
  flex-direction: row;
  justify-content: flex-end;
  align-items: center;
  flex: 0 0 auto;
  margin-left: -12px;
  margin-right: -12px;
  padding: 12px;
}

.jp-Dialog-checkbox {
  padding-right: 5px;
}

.jp-Dialog-checkbox > input:focus-visible {
  outline: 1px solid var(--jp-input-active-border-color);
  outline-offset: 1px;
}

.jp-Dialog-spacer {
  flex: 1 1 auto;
}

.jp-Dialog-title {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.jp-Dialog-body > .jp-select-wrapper {
  width: 100%;
}

.jp-Dialog-body > button {
  padding: 0 16px;
}

.jp-Dialog-body > label {
  line-height: 1.4;
  color: var(--jp-ui-font-color0);
}

.jp-Dialog-button.jp-mod-styled:not(:last-child) {
  margin-right: 12px;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-Input-Boolean-Dialog {
  flex-direction: row-reverse;
  align-items: end;
  width: 100%;
}

.jp-Input-Boolean-Dialog > label {
  flex: 1 1 auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-MainAreaWidget > :focus {
  outline: none;
}

.jp-MainAreaWidget .jp-MainAreaWidget-error {
  padding: 6px;
}

.jp-MainAreaWidget .jp-MainAreaWidget-error > pre {
  width: auto;
  padding: 10px;
  background: var(--jp-error-color3);
  border: var(--jp-border-width) solid var(--jp-error-color1);
  border-radius: var(--jp-border-radius);
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  white-space: pre-wrap;
  word-wrap: break-word;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/**
 * google-material-color v1.2.6
 * https://github.com/danlevan/google-material-color
 */
:root {
  --md-red-50: #ffebee;
  --md-red-100: #ffcdd2;
  --md-red-200: #ef9a9a;
  --md-red-300: #e57373;
  --md-red-400: #ef5350;
  --md-red-500: #f44336;
  --md-red-600: #e53935;
  --md-red-700: #d32f2f;
  --md-red-800: #c62828;
  --md-red-900: #b71c1c;
  --md-red-A100: #ff8a80;
  --md-red-A200: #ff5252;
  --md-red-A400: #ff1744;
  --md-red-A700: #d50000;
  --md-pink-50: #fce4ec;
  --md-pink-100: #f8bbd0;
  --md-pink-200: #f48fb1;
  --md-pink-300: #f06292;
  --md-pink-400: #ec407a;
  --md-pink-500: #e91e63;
  --md-pink-600: #d81b60;
  --md-pink-700: #c2185b;
  --md-pink-800: #ad1457;
  --md-pink-900: #880e4f;
  --md-pink-A100: #ff80ab;
  --md-pink-A200: #ff4081;
  --md-pink-A400: #f50057;
  --md-pink-A700: #c51162;
  --md-purple-50: #f3e5f5;
  --md-purple-100: #e1bee7;
  --md-purple-200: #ce93d8;
  --md-purple-300: #ba68c8;
  --md-purple-400: #ab47bc;
  --md-purple-500: #9c27b0;
  --md-purple-600: #8e24aa;
  --md-purple-700: #7b1fa2;
  --md-purple-800: #6a1b9a;
  --md-purple-900: #4a148c;
  --md-purple-A100: #ea80fc;
  --md-purple-A200: #e040fb;
  --md-purple-A400: #d500f9;
  --md-purple-A700: #a0f;
  --md-deep-purple-50: #ede7f6;
  --md-deep-purple-100: #d1c4e9;
  --md-deep-purple-200: #b39ddb;
  --md-deep-purple-300: #9575cd;
  --md-deep-purple-400: #7e57c2;
  --md-deep-purple-500: #673ab7;
  --md-deep-purple-600: #5e35b1;
  --md-deep-purple-700: #512da8;
  --md-deep-purple-800: #4527a0;
  --md-deep-purple-900: #311b92;
  --md-deep-purple-A100: #b388ff;
  --md-deep-purple-A200: #7c4dff;
  --md-deep-purple-A400: #651fff;
  --md-deep-purple-A700: #6200ea;
  --md-indigo-50: #e8eaf6;
  --md-indigo-100: #c5cae9;
  --md-indigo-200: #9fa8da;
  --md-indigo-300: #7986cb;
  --md-indigo-400: #5c6bc0;
  --md-indigo-500: #3f51b5;
  --md-indigo-600: #3949ab;
  --md-indigo-700: #303f9f;
  --md-indigo-800: #283593;
  --md-indigo-900: #1a237e;
  --md-indigo-A100: #8c9eff;
  --md-indigo-A200: #536dfe;
  --md-indigo-A400: #3d5afe;
  --md-indigo-A700: #304ffe;
  --md-blue-50: #e3f2fd;
  --md-blue-100: #bbdefb;
  --md-blue-200: #90caf9;
  --md-blue-300: #64b5f6;
  --md-blue-400: #42a5f5;
  --md-blue-500: #2196f3;
  --md-blue-600: #1e88e5;
  --md-blue-700: #1976d2;
  --md-blue-800: #1565c0;
  --md-blue-900: #0d47a1;
  --md-blue-A100: #82b1ff;
  --md-blue-A200: #448aff;
  --md-blue-A400: #2979ff;
  --md-blue-A700: #2962ff;
  --md-light-blue-50: #e1f5fe;
  --md-light-blue-100: #b3e5fc;
  --md-light-blue-200: #81d4fa;
  --md-light-blue-300: #4fc3f7;
  --md-light-blue-400: #29b6f6;
  --md-light-blue-500: #03a9f4;
  --md-light-blue-600: #039be5;
  --md-light-blue-700: #0288d1;
  --md-light-blue-800: #0277bd;
  --md-light-blue-900: #01579b;
  --md-light-blue-A100: #80d8ff;
  --md-light-blue-A200: #40c4ff;
  --md-light-blue-A400: #00b0ff;
  --md-light-blue-A700: #0091ea;
  --md-cyan-50: #e0f7fa;
  --md-cyan-100: #b2ebf2;
  --md-cyan-200: #80deea;
  --md-cyan-300: #4dd0e1;
  --md-cyan-400: #26c6da;
  --md-cyan-500: #00bcd4;
  --md-cyan-600: #00acc1;
  --md-cyan-700: #0097a7;
  --md-cyan-800: #00838f;
  --md-cyan-900: #006064;
  --md-cyan-A100: #84ffff;
  --md-cyan-A200: #18ffff;
  --md-cyan-A400: #00e5ff;
  --md-cyan-A700: #00b8d4;
  --md-teal-50: #e0f2f1;
  --md-teal-100: #b2dfdb;
  --md-teal-200: #80cbc4;
  --md-teal-300: #4db6ac;
  --md-teal-400: #26a69a;
  --md-teal-500: #009688;
  --md-teal-600: #00897b;
  --md-teal-700: #00796b;
  --md-teal-800: #00695c;
  --md-teal-900: #004d40;
  --md-teal-A100: #a7ffeb;
  --md-teal-A200: #64ffda;
  --md-teal-A400: #1de9b6;
  --md-teal-A700: #00bfa5;
  --md-green-50: #e8f5e9;
  --md-green-100: #c8e6c9;
  --md-green-200: #a5d6a7;
  --md-green-300: #81c784;
  --md-green-400: #66bb6a;
  --md-green-500: #4caf50;
  --md-green-600: #43a047;
  --md-green-700: #388e3c;
  --md-green-800: #2e7d32;
  --md-green-900: #1b5e20;
  --md-green-A100: #b9f6ca;
  --md-green-A200: #69f0ae;
  --md-green-A400: #00e676;
  --md-green-A700: #00c853;
  --md-light-green-50: #f1f8e9;
  --md-light-green-100: #dcedc8;
  --md-light-green-200: #c5e1a5;
  --md-light-green-300: #aed581;
  --md-light-green-400: #9ccc65;
  --md-light-green-500: #8bc34a;
  --md-light-green-600: #7cb342;
  --md-light-green-700: #689f38;
  --md-light-green-800: #558b2f;
  --md-light-green-900: #33691e;
  --md-light-green-A100: #ccff90;
  --md-light-green-A200: #b2ff59;
  --md-light-green-A400: #76ff03;
  --md-light-green-A700: #64dd17;
  --md-lime-50: #f9fbe7;
  --md-lime-100: #f0f4c3;
  --md-lime-200: #e6ee9c;
  --md-lime-300: #dce775;
  --md-lime-400: #d4e157;
  --md-lime-500: #cddc39;
  --md-lime-600: #c0ca33;
  --md-lime-700: #afb42b;
  --md-lime-800: #9e9d24;
  --md-lime-900: #827717;
  --md-lime-A100: #f4ff81;
  --md-lime-A200: #eeff41;
  --md-lime-A400: #c6ff00;
  --md-lime-A700: #aeea00;
  --md-yellow-50: #fffde7;
  --md-yellow-100: #fff9c4;
  --md-yellow-200: #fff59d;
  --md-yellow-300: #fff176;
  --md-yellow-400: #ffee58;
  --md-yellow-500: #ffeb3b;
  --md-yellow-600: #fdd835;
  --md-yellow-700: #fbc02d;
  --md-yellow-800: #f9a825;
  --md-yellow-900: #f57f17;
  --md-yellow-A100: #ffff8d;
  --md-yellow-A200: #ff0;
  --md-yellow-A400: #ffea00;
  --md-yellow-A700: #ffd600;
  --md-amber-50: #fff8e1;
  --md-amber-100: #ffecb3;
  --md-amber-200: #ffe082;
  --md-amber-300: #ffd54f;
  --md-amber-400: #ffca28;
  --md-amber-500: #ffc107;
  --md-amber-600: #ffb300;
  --md-amber-700: #ffa000;
  --md-amber-800: #ff8f00;
  --md-amber-900: #ff6f00;
  --md-amber-A100: #ffe57f;
  --md-amber-A200: #ffd740;
  --md-amber-A400: #ffc400;
  --md-amber-A700: #ffab00;
  --md-orange-50: #fff3e0;
  --md-orange-100: #ffe0b2;
  --md-orange-200: #ffcc80;
  --md-orange-300: #ffb74d;
  --md-orange-400: #ffa726;
  --md-orange-500: #ff9800;
  --md-orange-600: #fb8c00;
  --md-orange-700: #f57c00;
  --md-orange-800: #ef6c00;
  --md-orange-900: #e65100;
  --md-orange-A100: #ffd180;
  --md-orange-A200: #ffab40;
  --md-orange-A400: #ff9100;
  --md-orange-A700: #ff6d00;
  --md-deep-orange-50: #fbe9e7;
  --md-deep-orange-100: #ffccbc;
  --md-deep-orange-200: #ffab91;
  --md-deep-orange-300: #ff8a65;
  --md-deep-orange-400: #ff7043;
  --md-deep-orange-500: #ff5722;
  --md-deep-orange-600: #f4511e;
  --md-deep-orange-700: #e64a19;
  --md-deep-orange-800: #d84315;
  --md-deep-orange-900: #bf360c;
  --md-deep-orange-A100: #ff9e80;
  --md-deep-orange-A200: #ff6e40;
  --md-deep-orange-A400: #ff3d00;
  --md-deep-orange-A700: #dd2c00;
  --md-brown-50: #efebe9;
  --md-brown-100: #d7ccc8;
  --md-brown-200: #bcaaa4;
  --md-brown-300: #a1887f;
  --md-brown-400: #8d6e63;
  --md-brown-500: #795548;
  --md-brown-600: #6d4c41;
  --md-brown-700: #5d4037;
  --md-brown-800: #4e342e;
  --md-brown-900: #3e2723;
  --md-grey-50: #fafafa;
  --md-grey-100: #f5f5f5;
  --md-grey-200: #eee;
  --md-grey-300: #e0e0e0;
  --md-grey-400: #bdbdbd;
  --md-grey-500: #9e9e9e;
  --md-grey-600: #757575;
  --md-grey-700: #616161;
  --md-grey-800: #424242;
  --md-grey-900: #212121;
  --md-blue-grey-50: #eceff1;
  --md-blue-grey-100: #cfd8dc;
  --md-blue-grey-200: #b0bec5;
  --md-blue-grey-300: #90a4ae;
  --md-blue-grey-400: #78909c;
  --md-blue-grey-500: #607d8b;
  --md-blue-grey-600: #546e7a;
  --md-blue-grey-700: #455a64;
  --md-blue-grey-800: #37474f;
  --md-blue-grey-900: #263238;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| RenderedText
|----------------------------------------------------------------------------*/

:root {
  /* This is the padding value to fill the gaps between lines containing spans with background color. */
  --jp-private-code-span-padding: calc(
    (var(--jp-code-line-height) - 1) * var(--jp-code-font-size) / 2
  );
}

.jp-RenderedText {
  text-align: left;
  padding-left: var(--jp-code-padding);
  line-height: var(--jp-code-line-height);
  font-family: var(--jp-code-font-family);
}

.jp-RenderedText pre,
.jp-RenderedJavaScript pre,
.jp-RenderedHTMLCommon pre {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-code-font-size);
  border: none;
  margin: 0;
  padding: 0;
}

.jp-RenderedText pre a:link {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

.jp-RenderedText pre a:hover {
  text-decoration: underline;
  color: var(--jp-content-link-color);
}

.jp-RenderedText pre a:visited {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

/* console foregrounds and backgrounds */
.jp-RenderedText pre .ansi-black-fg {
  color: #3e424d;
}

.jp-RenderedText pre .ansi-red-fg {
  color: #e75c58;
}

.jp-RenderedText pre .ansi-green-fg {
  color: #00a250;
}

.jp-RenderedText pre .ansi-yellow-fg {
  color: #ddb62b;
}

.jp-RenderedText pre .ansi-blue-fg {
  color: #208ffb;
}

.jp-RenderedText pre .ansi-magenta-fg {
  color: #d160c4;
}

.jp-RenderedText pre .ansi-cyan-fg {
  color: #60c6c8;
}

.jp-RenderedText pre .ansi-white-fg {
  color: #c5c1b4;
}

.jp-RenderedText pre .ansi-black-bg {
  background-color: #3e424d;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-red-bg {
  background-color: #e75c58;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-green-bg {
  background-color: #00a250;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-yellow-bg {
  background-color: #ddb62b;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-blue-bg {
  background-color: #208ffb;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-magenta-bg {
  background-color: #d160c4;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-cyan-bg {
  background-color: #60c6c8;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-white-bg {
  background-color: #c5c1b4;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-black-intense-fg {
  color: #282c36;
}

.jp-RenderedText pre .ansi-red-intense-fg {
  color: #b22b31;
}

.jp-RenderedText pre .ansi-green-intense-fg {
  color: #007427;
}

.jp-RenderedText pre .ansi-yellow-intense-fg {
  color: #b27d12;
}

.jp-RenderedText pre .ansi-blue-intense-fg {
  color: #0065ca;
}

.jp-RenderedText pre .ansi-magenta-intense-fg {
  color: #a03196;
}

.jp-RenderedText pre .ansi-cyan-intense-fg {
  color: #258f8f;
}

.jp-RenderedText pre .ansi-white-intense-fg {
  color: #a1a6b2;
}

.jp-RenderedText pre .ansi-black-intense-bg {
  background-color: #282c36;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-red-intense-bg {
  background-color: #b22b31;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-green-intense-bg {
  background-color: #007427;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-yellow-intense-bg {
  background-color: #b27d12;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-blue-intense-bg {
  background-color: #0065ca;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-magenta-intense-bg {
  background-color: #a03196;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-cyan-intense-bg {
  background-color: #258f8f;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-white-intense-bg {
  background-color: #a1a6b2;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-default-inverse-fg {
  color: var(--jp-ui-inverse-font-color0);
}

.jp-RenderedText pre .ansi-default-inverse-bg {
  background-color: var(--jp-inverse-layout-color0);
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-bold {
  font-weight: bold;
}

.jp-RenderedText pre .ansi-underline {
  text-decoration: underline;
}

.jp-RenderedText[data-mime-type='application/vnd.jupyter.stderr'] {
  background: var(--jp-rendermime-error-background);
  padding-top: var(--jp-code-padding);
}

/*-----------------------------------------------------------------------------
| RenderedLatex
|----------------------------------------------------------------------------*/

.jp-RenderedLatex {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-content-font-size1);
  line-height: var(--jp-content-line-height);
}

/* Left-justify outputs.*/
.jp-OutputArea-output.jp-RenderedLatex {
  padding: var(--jp-code-padding);
  text-align: left;
}

/*-----------------------------------------------------------------------------
| RenderedHTML
|----------------------------------------------------------------------------*/

.jp-RenderedHTMLCommon {
  color: var(--jp-content-font-color1);
  font-family: var(--jp-content-font-family);
  font-size: var(--jp-content-font-size1);
  line-height: var(--jp-content-line-height);

  /* Give a bit more R padding on Markdown text to keep line lengths reasonable */
  padding-right: 20px;
}

.jp-RenderedHTMLCommon em {
  font-style: italic;
}

.jp-RenderedHTMLCommon strong {
  font-weight: bold;
}

.jp-RenderedHTMLCommon u {
  text-decoration: underline;
}

.jp-RenderedHTMLCommon a:link {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

.jp-RenderedHTMLCommon a:hover {
  text-decoration: underline;
  color: var(--jp-content-link-color);
}

.jp-RenderedHTMLCommon a:visited {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

/* Headings */

.jp-RenderedHTMLCommon h1,
.jp-RenderedHTMLCommon h2,
.jp-RenderedHTMLCommon h3,
.jp-RenderedHTMLCommon h4,
.jp-RenderedHTMLCommon h5,
.jp-RenderedHTMLCommon h6 {
  line-height: var(--jp-content-heading-line-height);
  font-weight: var(--jp-content-heading-font-weight);
  font-style: normal;
  margin: var(--jp-content-heading-margin-top) 0
    var(--jp-content-heading-margin-bottom) 0;
}

.jp-RenderedHTMLCommon h1:first-child,
.jp-RenderedHTMLCommon h2:first-child,
.jp-RenderedHTMLCommon h3:first-child,
.jp-RenderedHTMLCommon h4:first-child,
.jp-RenderedHTMLCommon h5:first-child,
.jp-RenderedHTMLCommon h6:first-child {
  margin-top: calc(0.5 * var(--jp-content-heading-margin-top));
}

.jp-RenderedHTMLCommon h1:last-child,
.jp-RenderedHTMLCommon h2:last-child,
.jp-RenderedHTMLCommon h3:last-child,
.jp-RenderedHTMLCommon h4:last-child,
.jp-RenderedHTMLCommon h5:last-child,
.jp-RenderedHTMLCommon h6:last-child {
  margin-bottom: calc(0.5 * var(--jp-content-heading-margin-bottom));
}

.jp-RenderedHTMLCommon h1 {
  font-size: var(--jp-content-font-size5);
}

.jp-RenderedHTMLCommon h2 {
  font-size: var(--jp-content-font-size4);
}

.jp-RenderedHTMLCommon h3 {
  font-size: var(--jp-content-font-size3);
}

.jp-RenderedHTMLCommon h4 {
  font-size: var(--jp-content-font-size2);
}

.jp-RenderedHTMLCommon h5 {
  font-size: var(--jp-content-font-size1);
}

.jp-RenderedHTMLCommon h6 {
  font-size: var(--jp-content-font-size0);
}

/* Lists */

/* stylelint-disable selector-max-type, selector-max-compound-selectors */

.jp-RenderedHTMLCommon ul:not(.list-inline),
.jp-RenderedHTMLCommon ol:not(.list-inline) {
  padding-left: 2em;
}

.jp-RenderedHTMLCommon ul {
  list-style: disc;
}

.jp-RenderedHTMLCommon ul ul {
  list-style: square;
}

.jp-RenderedHTMLCommon ul ul ul {
  list-style: circle;
}

.jp-RenderedHTMLCommon ol {
  list-style: decimal;
}

.jp-RenderedHTMLCommon ol ol {
  list-style: upper-alpha;
}

.jp-RenderedHTMLCommon ol ol ol {
  list-style: lower-alpha;
}

.jp-RenderedHTMLCommon ol ol ol ol {
  list-style: lower-roman;
}

.jp-RenderedHTMLCommon ol ol ol ol ol {
  list-style: decimal;
}

.jp-RenderedHTMLCommon ol,
.jp-RenderedHTMLCommon ul {
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon ul ul,
.jp-RenderedHTMLCommon ul ol,
.jp-RenderedHTMLCommon ol ul,
.jp-RenderedHTMLCommon ol ol {
  margin-bottom: 0;
}

/* stylelint-enable selector-max-type, selector-max-compound-selectors */

.jp-RenderedHTMLCommon hr {
  color: var(--jp-border-color2);
  background-color: var(--jp-border-color1);
  margin-top: 1em;
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon > pre {
  margin: 1.5em 2em;
}

.jp-RenderedHTMLCommon pre,
.jp-RenderedHTMLCommon code {
  border: 0;
  background-color: var(--jp-layout-color0);
  color: var(--jp-content-font-color1);
  font-family: var(--jp-code-font-family);
  font-size: inherit;
  line-height: var(--jp-code-line-height);
  padding: 0;
  white-space: pre-wrap;
}

.jp-RenderedHTMLCommon :not(pre) > code {
  background-color: var(--jp-layout-color2);
  padding: 1px 5px;
}

/* Tables */

.jp-RenderedHTMLCommon table {
  border-collapse: collapse;
  border-spacing: 0;
  border: none;
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  table-layout: fixed;
  margin-left: auto;
  margin-bottom: 1em;
  margin-right: auto;
}

.jp-RenderedHTMLCommon thead {
  border-bottom: var(--jp-border-width) solid var(--jp-border-color1);
  vertical-align: bottom;
}

.jp-RenderedHTMLCommon td,
.jp-RenderedHTMLCommon th,
.jp-RenderedHTMLCommon tr {
  vertical-align: middle;
  padding: 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}

.jp-RenderedMarkdown.jp-RenderedHTMLCommon td,
.jp-RenderedMarkdown.jp-RenderedHTMLCommon th {
  max-width: none;
}

:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon td,
:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon th,
:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon tr {
  text-align: right;
}

.jp-RenderedHTMLCommon th {
  font-weight: bold;
}

.jp-RenderedHTMLCommon tbody tr:nth-child(odd) {
  background: var(--jp-layout-color0);
}

.jp-RenderedHTMLCommon tbody tr:nth-child(even) {
  background: var(--jp-rendermime-table-row-background);
}

.jp-RenderedHTMLCommon tbody tr:hover {
  background: var(--jp-rendermime-table-row-hover-background);
}

.jp-RenderedHTMLCommon p {
  text-align: left;
  margin: 0;
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon img {
  -moz-force-broken-image-icon: 1;
}

/* Restrict to direct children as other images could be nested in other content. */
.jp-RenderedHTMLCommon > img {
  display: block;
  margin-left: 0;
  margin-right: 0;
  margin-bottom: 1em;
}

/* Change color behind transparent images if they need it... */
[data-jp-theme-light='false'] .jp-RenderedImage img.jp-needs-light-background {
  background-color: var(--jp-inverse-layout-color1);
}

[data-jp-theme-light='true'] .jp-RenderedImage img.jp-needs-dark-background {
  background-color: var(--jp-inverse-layout-color1);
}

.jp-RenderedHTMLCommon img,
.jp-RenderedImage img,
.jp-RenderedHTMLCommon svg,
.jp-RenderedSVG svg {
  max-width: 100%;
  height: auto;
}

.jp-RenderedHTMLCommon img.jp-mod-unconfined,
.jp-RenderedImage img.jp-mod-unconfined,
.jp-RenderedHTMLCommon svg.jp-mod-unconfined,
.jp-RenderedSVG svg.jp-mod-unconfined {
  max-width: none;
}

.jp-RenderedHTMLCommon .alert {
  padding: var(--jp-notebook-padding);
  border: var(--jp-border-width) solid transparent;
  border-radius: var(--jp-border-radius);
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon .alert-info {
  color: var(--jp-info-color0);
  background-color: var(--jp-info-color3);
  border-color: var(--jp-info-color2);
}

.jp-RenderedHTMLCommon .alert-info hr {
  border-color: var(--jp-info-color3);
}

.jp-RenderedHTMLCommon .alert-info > p:last-child,
.jp-RenderedHTMLCommon .alert-info > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-warning {
  color: var(--jp-warn-color0);
  background-color: var(--jp-warn-color3);
  border-color: var(--jp-warn-color2);
}

.jp-RenderedHTMLCommon .alert-warning hr {
  border-color: var(--jp-warn-color3);
}

.jp-RenderedHTMLCommon .alert-warning > p:last-child,
.jp-RenderedHTMLCommon .alert-warning > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-success {
  color: var(--jp-success-color0);
  background-color: var(--jp-success-color3);
  border-color: var(--jp-success-color2);
}

.jp-RenderedHTMLCommon .alert-success hr {
  border-color: var(--jp-success-color3);
}

.jp-RenderedHTMLCommon .alert-success > p:last-child,
.jp-RenderedHTMLCommon .alert-success > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-danger {
  color: var(--jp-error-color0);
  background-color: var(--jp-error-color3);
  border-color: var(--jp-error-color2);
}

.jp-RenderedHTMLCommon .alert-danger hr {
  border-color: var(--jp-error-color3);
}

.jp-RenderedHTMLCommon .alert-danger > p:last-child,
.jp-RenderedHTMLCommon .alert-danger > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon blockquote {
  margin: 1em 2em;
  padding: 0 1em;
  border-left: 5px solid var(--jp-border-color2);
}

a.jp-InternalAnchorLink {
  visibility: hidden;
  margin-left: 8px;
  color: var(--md-blue-800);
}

h1:hover .jp-InternalAnchorLink,
h2:hover .jp-InternalAnchorLink,
h3:hover .jp-InternalAnchorLink,
h4:hover .jp-InternalAnchorLink,
h5:hover .jp-InternalAnchorLink,
h6:hover .jp-InternalAnchorLink {
  visibility: visible;
}

.jp-RenderedHTMLCommon kbd {
  background-color: var(--jp-rendermime-table-row-background);
  border: 1px solid var(--jp-border-color0);
  border-bottom-color: var(--jp-border-color2);
  border-radius: 3px;
  box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.25);
  display: inline-block;
  font-size: var(--jp-ui-font-size0);
  line-height: 1em;
  padding: 0.2em 0.5em;
}

/* Most direct children of .jp-RenderedHTMLCommon have a margin-bottom of 1.0.
 * At the bottom of cells this is a bit too much as there is also spacing
 * between cells. Going all the way to 0 gets too tight between markdown and
 * code cells.
 */
.jp-RenderedHTMLCommon > *:last-child {
  margin-bottom: 0.5em;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-cursor-backdrop {
  position: fixed;
  width: 200px;
  height: 200px;
  margin-top: -100px;
  margin-left: -100px;
  will-change: transform;
  z-index: 100;
}

.lm-mod-drag-image {
  will-change: transform;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-lineFormSearch {
  padding: 4px 12px;
  background-color: var(--jp-layout-color2);
  box-shadow: var(--jp-toolbar-box-shadow);
  z-index: 2;
  font-size: var(--jp-ui-font-size1);
}

.jp-lineFormCaption {
  font-size: var(--jp-ui-font-size0);
  line-height: var(--jp-ui-font-size1);
  margin-top: 4px;
  color: var(--jp-ui-font-color0);
}

.jp-baseLineForm {
  border: none;
  border-radius: 0;
  position: absolute;
  background-size: 16px;
  background-repeat: no-repeat;
  background-position: center;
  outline: none;
}

.jp-lineFormButtonContainer {
  top: 4px;
  right: 8px;
  height: 24px;
  padding: 0 12px;
  width: 12px;
}

.jp-lineFormButtonIcon {
  top: 0;
  right: 0;
  background-color: var(--jp-brand-color1);
  height: 100%;
  width: 100%;
  box-sizing: border-box;
  padding: 4px 6px;
}

.jp-lineFormButton {
  top: 0;
  right: 0;
  background-color: transparent;
  height: 100%;
  width: 100%;
  box-sizing: border-box;
}

.jp-lineFormWrapper {
  overflow: hidden;
  padding: 0 8px;
  border: 1px solid var(--jp-border-color0);
  background-color: var(--jp-input-active-background);
  height: 22px;
}

.jp-lineFormWrapperFocusWithin {
  border: var(--jp-border-width) solid var(--md-blue-500);
  box-shadow: inset 0 0 4px var(--md-blue-300);
}

.jp-lineFormInput {
  background: transparent;
  width: 200px;
  height: 100%;
  border: none;
  outline: none;
  color: var(--jp-ui-font-color0);
  line-height: 28px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-JSONEditor {
  display: flex;
  flex-direction: column;
  width: 100%;
}

.jp-JSONEditor-host {
  flex: 1 1 auto;
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  border-radius: 0;
  background: var(--jp-layout-color0);
  min-height: 50px;
  padding: 1px;
}

.jp-JSONEditor.jp-mod-error .jp-JSONEditor-host {
  border-color: red;
  outline-color: red;
}

.jp-JSONEditor-header {
  display: flex;
  flex: 1 0 auto;
  padding: 0 0 0 12px;
}

.jp-JSONEditor-header label {
  flex: 0 0 auto;
}

.jp-JSONEditor-commitButton {
  height: 16px;
  width: 16px;
  background-size: 18px;
  background-repeat: no-repeat;
  background-position: center;
}

.jp-JSONEditor-host.jp-mod-focused {
  background-color: var(--jp-input-active-background);
  border: 1px solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
}

.jp-Editor.jp-mod-dropTarget {
  border: var(--jp-border-width) solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/
.jp-DocumentSearch-input {
  border: none;
  outline: none;
  color: var(--jp-ui-font-color0);
  font-size: var(--jp-ui-font-size1);
  background-color: var(--jp-layout-color0);
  font-family: var(--jp-ui-font-family);
  padding: 2px 1px;
  resize: none;
}

.jp-DocumentSearch-overlay {
  position: absolute;
  background-color: var(--jp-toolbar-background);
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  border-left: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  top: 0;
  right: 0;
  z-index: 7;
  min-width: 405px;
  padding: 2px;
  font-size: var(--jp-ui-font-size1);

  --jp-private-document-search-button-height: 20px;
}

.jp-DocumentSearch-overlay button {
  background-color: var(--jp-toolbar-background);
  outline: 0;
}

.jp-DocumentSearch-overlay button:hover {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-overlay button:active {
  background-color: var(--jp-layout-color3);
}

.jp-DocumentSearch-overlay-row {
  display: flex;
  align-items: center;
  margin-bottom: 2px;
}

.jp-DocumentSearch-button-content {
  display: inline-block;
  cursor: pointer;
  box-sizing: border-box;
  width: 100%;
  height: 100%;
}

.jp-DocumentSearch-button-content svg {
  width: 100%;
  height: 100%;
}

.jp-DocumentSearch-input-wrapper {
  border: var(--jp-border-width) solid var(--jp-border-color0);
  display: flex;
  background-color: var(--jp-layout-color0);
  margin: 2px;
}

.jp-DocumentSearch-input-wrapper:focus-within {
  border-color: var(--jp-cell-editor-active-border-color);
}

.jp-DocumentSearch-toggle-wrapper,
.jp-DocumentSearch-button-wrapper {
  all: initial;
  overflow: hidden;
  display: inline-block;
  border: none;
  box-sizing: border-box;
}

.jp-DocumentSearch-toggle-wrapper {
  width: 14px;
  height: 14px;
}

.jp-DocumentSearch-button-wrapper {
  width: var(--jp-private-document-search-button-height);
  height: var(--jp-private-document-search-button-height);
}

.jp-DocumentSearch-toggle-wrapper:focus,
.jp-DocumentSearch-button-wrapper:focus {
  outline: var(--jp-border-width) solid
    var(--jp-cell-editor-active-border-color);
  outline-offset: -1px;
}

.jp-DocumentSearch-toggle-wrapper,
.jp-DocumentSearch-button-wrapper,
.jp-DocumentSearch-button-content:focus {
  outline: none;
}

.jp-DocumentSearch-toggle-placeholder {
  width: 5px;
}

.jp-DocumentSearch-input-button::before {
  display: block;
  padding-top: 100%;
}

.jp-DocumentSearch-input-button-off {
  opacity: var(--jp-search-toggle-off-opacity);
}

.jp-DocumentSearch-input-button-off:hover {
  opacity: var(--jp-search-toggle-hover-opacity);
}

.jp-DocumentSearch-input-button-on {
  opacity: var(--jp-search-toggle-on-opacity);
}

.jp-DocumentSearch-index-counter {
  padding-left: 10px;
  padding-right: 10px;
  user-select: none;
  min-width: 35px;
  display: inline-block;
}

.jp-DocumentSearch-up-down-wrapper {
  display: inline-block;
  padding-right: 2px;
  margin-left: auto;
  white-space: nowrap;
}

.jp-DocumentSearch-spacer {
  margin-left: auto;
}

.jp-DocumentSearch-up-down-wrapper button {
  outline: 0;
  border: none;
  width: var(--jp-private-document-search-button-height);
  height: var(--jp-private-document-search-button-height);
  vertical-align: middle;
  margin: 1px 5px 2px;
}

.jp-DocumentSearch-up-down-button:hover {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-up-down-button:active {
  background-color: var(--jp-layout-color3);
}

.jp-DocumentSearch-filter-button {
  border-radius: var(--jp-border-radius);
}

.jp-DocumentSearch-filter-button:hover {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-filter-button-enabled {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-filter-button-enabled:hover {
  background-color: var(--jp-layout-color3);
}

.jp-DocumentSearch-search-options {
  padding: 0 8px;
  margin-left: 3px;
  width: 100%;
  display: grid;
  justify-content: start;
  grid-template-columns: 1fr 1fr;
  align-items: center;
  justify-items: stretch;
}

.jp-DocumentSearch-search-filter-disabled {
  color: var(--jp-ui-font-color2);
}

.jp-DocumentSearch-search-filter {
  display: flex;
  align-items: center;
  user-select: none;
}

.jp-DocumentSearch-regex-error {
  color: var(--jp-error-color0);
}

.jp-DocumentSearch-replace-button-wrapper {
  overflow: hidden;
  display: inline-block;
  box-sizing: border-box;
  border: var(--jp-border-width) solid var(--jp-border-color0);
  margin: auto 2px;
  padding: 1px 4px;
  height: calc(var(--jp-private-document-search-button-height) + 2px);
}

.jp-DocumentSearch-replace-button-wrapper:focus {
  border: var(--jp-border-width) solid var(--jp-cell-editor-active-border-color);
}

.jp-DocumentSearch-replace-button {
  display: inline-block;
  text-align: center;
  cursor: pointer;
  box-sizing: border-box;
  color: var(--jp-ui-font-color1);

  /* height - 2 * (padding of wrapper) */
  line-height: calc(var(--jp-private-document-search-button-height) - 2px);
  width: 100%;
  height: 100%;
}

.jp-DocumentSearch-replace-button:focus {
  outline: none;
}

.jp-DocumentSearch-replace-wrapper-class {
  margin-left: 14px;
  display: flex;
}

.jp-DocumentSearch-replace-toggle {
  border: none;
  background-color: var(--jp-toolbar-background);
  border-radius: var(--jp-border-radius);
}

.jp-DocumentSearch-replace-toggle:hover {
  background-color: var(--jp-layout-color2);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.cm-editor {
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  font-family: var(--jp-code-font-family);
  border: 0;
  border-radius: 0;
  height: auto;

  /* Changed to auto to autogrow */
}

.cm-editor pre {
  padding: 0 var(--jp-code-padding);
}

.jp-CodeMirrorEditor[data-type='inline'] .cm-dialog {
  background-color: var(--jp-layout-color0);
  color: var(--jp-content-font-color1);
}

.jp-CodeMirrorEditor {
  cursor: text;
}

/* When zoomed out 67% and 33% on a screen of 1440 width x 900 height */
@media screen and (min-width: 2138px) and (max-width: 4319px) {
  .jp-CodeMirrorEditor[data-type='inline'] .cm-cursor {
    border-left: var(--jp-code-cursor-width1) solid
      var(--jp-editor-cursor-color);
  }
}

/* When zoomed out less than 33% */
@media screen and (min-width: 4320px) {
  .jp-CodeMirrorEditor[data-type='inline'] .cm-cursor {
    border-left: var(--jp-code-cursor-width2) solid
      var(--jp-editor-cursor-color);
  }
}

.cm-editor.jp-mod-readOnly .cm-cursor {
  display: none;
}

.jp-CollaboratorCursor {
  border-left: 5px solid transparent;
  border-right: 5px solid transparent;
  border-top: none;
  border-bottom: 3px solid;
  background-clip: content-box;
  margin-left: -5px;
  margin-right: -5px;
}

.cm-searching,
.cm-searching span {
  /* `.cm-searching span`: we need to override syntax highlighting */
  background-color: var(--jp-search-unselected-match-background-color);
  color: var(--jp-search-unselected-match-color);
}

.cm-searching::selection,
.cm-searching span::selection {
  background-color: var(--jp-search-unselected-match-background-color);
  color: var(--jp-search-unselected-match-color);
}

.jp-current-match > .cm-searching,
.jp-current-match > .cm-searching span,
.cm-searching > .jp-current-match,
.cm-searching > .jp-current-match span {
  background-color: var(--jp-search-selected-match-background-color);
  color: var(--jp-search-selected-match-color);
}

.jp-current-match > .cm-searching::selection,
.cm-searching > .jp-current-match::selection,
.jp-current-match > .cm-searching span::selection {
  background-color: var(--jp-search-selected-match-background-color);
  color: var(--jp-search-selected-match-color);
}

.cm-trailingspace {
  background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAFCAYAAAB4ka1VAAAAsElEQVQIHQGlAFr/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA7+r3zKmT0/+pk9P/7+r3zAAAAAAAAAAABAAAAAAAAAAA6OPzM+/q9wAAAAAA6OPzMwAAAAAAAAAAAgAAAAAAAAAAGR8NiRQaCgAZIA0AGR8NiQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQyoYJ/SY80UAAAAASUVORK5CYII=);
  background-position: center left;
  background-repeat: repeat-x;
}

.jp-CollaboratorCursor-hover {
  position: absolute;
  z-index: 1;
  transform: translateX(-50%);
  color: white;
  border-radius: 3px;
  padding-left: 4px;
  padding-right: 4px;
  padding-top: 1px;
  padding-bottom: 1px;
  text-align: center;
  font-size: var(--jp-ui-font-size1);
  white-space: nowrap;
}

.jp-CodeMirror-ruler {
  border-left: 1px dashed var(--jp-border-color2);
}

/* Styles for shared cursors (remote cursor locations and selected ranges) */
.jp-CodeMirrorEditor .cm-ySelectionCaret {
  position: relative;
  border-left: 1px solid black;
  margin-left: -1px;
  margin-right: -1px;
  box-sizing: border-box;
}

.jp-CodeMirrorEditor .cm-ySelectionCaret > .cm-ySelectionInfo {
  white-space: nowrap;
  position: absolute;
  top: -1.15em;
  padding-bottom: 0.05em;
  left: -1px;
  font-size: 0.95em;
  font-family: var(--jp-ui-font-family);
  font-weight: bold;
  line-height: normal;
  user-select: none;
  color: white;
  padding-left: 2px;
  padding-right: 2px;
  z-index: 101;
  transition: opacity 0.3s ease-in-out;
}

.jp-CodeMirrorEditor .cm-ySelectionInfo {
  transition-delay: 0.7s;
  opacity: 0;
}

.jp-CodeMirrorEditor .cm-ySelectionCaret:hover > .cm-ySelectionInfo {
  opacity: 1;
  transition-delay: 0s;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-MimeDocument {
  outline: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-filebrowser-button-height: 28px;
  --jp-private-filebrowser-button-width: 48px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-FileBrowser .jp-SidePanel-content {
  display: flex;
  flex-direction: column;
}

.jp-FileBrowser-toolbar.jp-Toolbar {
  flex-wrap: wrap;
  row-gap: 12px;
  border-bottom: none;
  height: auto;
  margin: 8px 12px 0;
  box-shadow: none;
  padding: 0;
  justify-content: flex-start;
}

.jp-FileBrowser-Panel {
  flex: 1 1 auto;
  display: flex;
  flex-direction: column;
}

.jp-BreadCrumbs {
  flex: 0 0 auto;
  margin: 8px 12px;
}

.jp-BreadCrumbs-item {
  margin: 0 2px;
  padding: 0 2px;
  border-radius: var(--jp-border-radius);
  cursor: pointer;
}

.jp-BreadCrumbs-item:hover {
  background-color: var(--jp-layout-color2);
}

.jp-BreadCrumbs-item:first-child {
  margin-left: 0;
}

.jp-BreadCrumbs-item.jp-mod-dropTarget {
  background-color: var(--jp-brand-color2);
  opacity: 0.7;
}

/*-----------------------------------------------------------------------------
| Buttons
|----------------------------------------------------------------------------*/

.jp-FileBrowser-toolbar > .jp-Toolbar-item {
  flex: 0 0 auto;
  padding-left: 0;
  padding-right: 2px;
  align-items: center;
  height: unset;
}

.jp-FileBrowser-toolbar > .jp-Toolbar-item .jp-ToolbarButtonComponent {
  width: 40px;
}

/*-----------------------------------------------------------------------------
| Other styles
|----------------------------------------------------------------------------*/

.jp-FileDialog.jp-mod-conflict input {
  color: var(--jp-error-color1);
}

.jp-FileDialog .jp-new-name-title {
  margin-top: 12px;
}

.jp-LastModified-hidden {
  display: none;
}

.jp-FileSize-hidden {
  display: none;
}

.jp-FileBrowser .lm-AccordionPanel > h3:first-child {
  display: none;
}

/*-----------------------------------------------------------------------------
| DirListing
|----------------------------------------------------------------------------*/

.jp-DirListing {
  flex: 1 1 auto;
  display: flex;
  flex-direction: column;
  outline: 0;
}

.jp-DirListing-header {
  flex: 0 0 auto;
  display: flex;
  flex-direction: row;
  align-items: center;
  overflow: hidden;
  border-top: var(--jp-border-width) solid var(--jp-border-color2);
  border-bottom: var(--jp-border-width) solid var(--jp-border-color1);
  box-shadow: var(--jp-toolbar-box-shadow);
  z-index: 2;
}

.jp-DirListing-headerItem {
  padding: 4px 12px 2px;
  font-weight: 500;
}

.jp-DirListing-headerItem:hover {
  background: var(--jp-layout-color2);
}

.jp-DirListing-headerItem.jp-id-name {
  flex: 1 0 84px;
}

.jp-DirListing-headerItem.jp-id-modified {
  flex: 0 0 112px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
}

.jp-DirListing-headerItem.jp-id-filesize {
  flex: 0 0 75px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
}

.jp-id-narrow {
  display: none;
  flex: 0 0 5px;
  padding: 4px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
  color: var(--jp-border-color2);
}

.jp-DirListing-narrow .jp-id-narrow {
  display: block;
}

.jp-DirListing-narrow .jp-id-modified,
.jp-DirListing-narrow .jp-DirListing-itemModified {
  display: none;
}

.jp-DirListing-headerItem.jp-mod-selected {
  font-weight: 600;
}

/* increase specificity to override bundled default */
.jp-DirListing-content {
  flex: 1 1 auto;
  margin: 0;
  padding: 0;
  list-style-type: none;
  overflow: auto;
  background-color: var(--jp-layout-color1);
}

.jp-DirListing-content mark {
  color: var(--jp-ui-font-color0);
  background-color: transparent;
  font-weight: bold;
}

.jp-DirListing-content .jp-DirListing-item.jp-mod-selected mark {
  color: var(--jp-ui-inverse-font-color0);
}

/* Style the directory listing content when a user drops a file to upload */
.jp-DirListing.jp-mod-native-drop .jp-DirListing-content {
  outline: 5px dashed rgba(128, 128, 128, 0.5);
  outline-offset: -10px;
  cursor: copy;
}

.jp-DirListing-item {
  display: flex;
  flex-direction: row;
  align-items: center;
  padding: 4px 12px;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-DirListing-checkboxWrapper {
  /* Increases hit area of checkbox. */
  padding: 4px;
}

.jp-DirListing-header
  .jp-DirListing-checkboxWrapper
  + .jp-DirListing-headerItem {
  padding-left: 4px;
}

.jp-DirListing-content .jp-DirListing-checkboxWrapper {
  position: relative;
  left: -4px;
  margin: -4px 0 -4px -8px;
}

.jp-DirListing-checkboxWrapper.jp-mod-visible {
  visibility: visible;
}

/* For devices that support hovering, hide checkboxes until hovered, selected...
*/
@media (hover: hover) {
  .jp-DirListing-checkboxWrapper {
    visibility: hidden;
  }

  .jp-DirListing-item:hover .jp-DirListing-checkboxWrapper,
  .jp-DirListing-item.jp-mod-selected .jp-DirListing-checkboxWrapper {
    visibility: visible;
  }
}

.jp-DirListing-item[data-is-dot] {
  opacity: 75%;
}

.jp-DirListing-item.jp-mod-selected {
  color: var(--jp-ui-inverse-font-color1);
  background: var(--jp-brand-color1);
}

.jp-DirListing-item.jp-mod-dropTarget {
  background: var(--jp-brand-color3);
}

.jp-DirListing-item:hover:not(.jp-mod-selected) {
  background: var(--jp-layout-color2);
}

.jp-DirListing-itemIcon {
  flex: 0 0 20px;
  margin-right: 4px;
}

.jp-DirListing-itemText {
  flex: 1 0 64px;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
  user-select: none;
}

.jp-DirListing-itemText:focus {
  outline-width: 2px;
  outline-color: var(--jp-inverse-layout-color1);
  outline-style: solid;
  outline-offset: 1px;
}

.jp-DirListing-item.jp-mod-selected .jp-DirListing-itemText:focus {
  outline-color: var(--jp-layout-color1);
}

.jp-DirListing-itemModified {
  flex: 0 0 125px;
  text-align: right;
}

.jp-DirListing-itemFileSize {
  flex: 0 0 90px;
  text-align: right;
}

.jp-DirListing-editor {
  flex: 1 0 64px;
  outline: none;
  border: none;
  color: var(--jp-ui-font-color1);
  background-color: var(--jp-layout-color1);
}

.jp-DirListing-item.jp-mod-running .jp-DirListing-itemIcon::before {
  color: var(--jp-success-color1);
  content: '\25CF';
  font-size: 8px;
  position: absolute;
  left: -8px;
}

.jp-DirListing-item.jp-mod-running.jp-mod-selected
  .jp-DirListing-itemIcon::before {
  color: var(--jp-ui-inverse-font-color1);
}

.jp-DirListing-item.lm-mod-drag-image,
.jp-DirListing-item.jp-mod-selected.lm-mod-drag-image {
  font-size: var(--jp-ui-font-size1);
  padding-left: 4px;
  margin-left: 4px;
  width: 160px;
  background-color: var(--jp-ui-inverse-font-color2);
  box-shadow: var(--jp-elevation-z2);
  border-radius: 0;
  color: var(--jp-ui-font-color1);
  transform: translateX(-40%) translateY(-58%);
}

.jp-Document {
  min-width: 120px;
  min-height: 120px;
  outline: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Main OutputArea
| OutputArea has a list of Outputs
|----------------------------------------------------------------------------*/

.jp-OutputArea {
  overflow-y: auto;
}

.jp-OutputArea-child {
  display: table;
  table-layout: fixed;
  width: 100%;
  overflow: hidden;
}

.jp-OutputPrompt {
  width: var(--jp-cell-prompt-width);
  color: var(--jp-cell-outprompt-font-color);
  font-family: var(--jp-cell-prompt-font-family);
  padding: var(--jp-code-padding);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;
  opacity: var(--jp-cell-prompt-opacity);

  /* Right align prompt text, don't wrap to handle large prompt numbers */
  text-align: right;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;

  /* Disable text selection */
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-OutputArea-prompt {
  display: table-cell;
  vertical-align: top;
}

.jp-OutputArea-output {
  display: table-cell;
  width: 100%;
  height: auto;
  overflow: auto;
  user-select: text;
  -moz-user-select: text;
  -webkit-user-select: text;
  -ms-user-select: text;
}

.jp-OutputArea .jp-RenderedText {
  padding-left: 1ch;
}

/**
 * Prompt overlay.
 */

.jp-OutputArea-promptOverlay {
  position: absolute;
  top: 0;
  width: var(--jp-cell-prompt-width);
  height: 100%;
  opacity: 0.5;
}

.jp-OutputArea-promptOverlay:hover {
  background: var(--jp-layout-color2);
  box-shadow: inset 0 0 1px var(--jp-inverse-layout-color0);
  cursor: zoom-out;
}

.jp-mod-outputsScrolled .jp-OutputArea-promptOverlay:hover {
  cursor: zoom-in;
}

/**
 * Isolated output.
 */
.jp-OutputArea-output.jp-mod-isolated {
  width: 100%;
  display: block;
}

/*
When drag events occur, `lm-mod-override-cursor` is added to the body.
Because iframes steal all cursor events, the following two rules are necessary
to suppress pointer events while resize drags are occurring. There may be a
better solution to this problem.
*/
body.lm-mod-override-cursor .jp-OutputArea-output.jp-mod-isolated {
  position: relative;
}

body.lm-mod-override-cursor .jp-OutputArea-output.jp-mod-isolated::before {
  content: '';
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background: transparent;
}

/* pre */

.jp-OutputArea-output pre {
  border: none;
  margin: 0;
  padding: 0;
  overflow-x: auto;
  overflow-y: auto;
  word-break: break-all;
  word-wrap: break-word;
  white-space: pre-wrap;
}

/* tables */

.jp-OutputArea-output.jp-RenderedHTMLCommon table {
  margin-left: 0;
  margin-right: 0;
}

/* description lists */

.jp-OutputArea-output dl,
.jp-OutputArea-output dt,
.jp-OutputArea-output dd {
  display: block;
}

.jp-OutputArea-output dl {
  width: 100%;
  overflow: hidden;
  padding: 0;
  margin: 0;
}

.jp-OutputArea-output dt {
  font-weight: bold;
  float: left;
  width: 20%;
  padding: 0;
  margin: 0;
}

.jp-OutputArea-output dd {
  float: left;
  width: 80%;
  padding: 0;
  margin: 0;
}

.jp-TrimmedOutputs pre {
  background: var(--jp-layout-color3);
  font-size: calc(var(--jp-code-font-size) * 1.4);
  text-align: center;
  text-transform: uppercase;
}

/* Hide the gutter in case of
 *  - nested output areas (e.g. in the case of output widgets)
 *  - mirrored output areas
 */
.jp-OutputArea .jp-OutputArea .jp-OutputArea-prompt {
  display: none;
}

/* Hide empty lines in the output area, for instance due to cleared widgets */
.jp-OutputArea-prompt:empty {
  padding: 0;
  border: 0;
}

/*-----------------------------------------------------------------------------
| executeResult is added to any Output-result for the display of the object
| returned by a cell
|----------------------------------------------------------------------------*/

.jp-OutputArea-output.jp-OutputArea-executeResult {
  margin-left: 0;
  width: 100%;
}

/* Text output with the Out[] prompt needs a top padding to match the
 * alignment of the Out[] prompt itself.
 */
.jp-OutputArea-executeResult .jp-RenderedText.jp-OutputArea-output {
  padding-top: var(--jp-code-padding);
  border-top: var(--jp-border-width) solid transparent;
}

/*-----------------------------------------------------------------------------
| The Stdin output
|----------------------------------------------------------------------------*/

.jp-Stdin-prompt {
  color: var(--jp-content-font-color0);
  padding-right: var(--jp-code-padding);
  vertical-align: baseline;
  flex: 0 0 auto;
}

.jp-Stdin-input {
  font-family: var(--jp-code-font-family);
  font-size: inherit;
  color: inherit;
  background-color: inherit;
  width: 42%;
  min-width: 200px;

  /* make sure input baseline aligns with prompt */
  vertical-align: baseline;

  /* padding + margin = 0.5em between prompt and cursor */
  padding: 0 0.25em;
  margin: 0 0.25em;
  flex: 0 0 70%;
}

.jp-Stdin-input::placeholder {
  opacity: 0;
}

.jp-Stdin-input:focus {
  box-shadow: none;
}

.jp-Stdin-input:focus::placeholder {
  opacity: 1;
}

/*-----------------------------------------------------------------------------
| Output Area View
|----------------------------------------------------------------------------*/

.jp-LinkedOutputView .jp-OutputArea {
  height: 100%;
  display: block;
}

.jp-LinkedOutputView .jp-OutputArea-output:only-child {
  height: 100%;
}

/*-----------------------------------------------------------------------------
| Printing
|----------------------------------------------------------------------------*/

@media print {
  .jp-OutputArea-child {
    break-inside: avoid-page;
  }
}

/*-----------------------------------------------------------------------------
| Mobile
|----------------------------------------------------------------------------*/
@media only screen and (max-width: 760px) {
  .jp-OutputPrompt {
    display: table-row;
    text-align: left;
  }

  .jp-OutputArea-child .jp-OutputArea-output {
    display: table-row;
    margin-left: var(--jp-notebook-padding);
  }
}

/* Trimmed outputs warning */
.jp-TrimmedOutputs > a {
  margin: 10px;
  text-decoration: none;
  cursor: pointer;
}

.jp-TrimmedOutputs > a:hover {
  text-decoration: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Table of Contents
|----------------------------------------------------------------------------*/

:root {
  --jp-private-toc-active-width: 4px;
}

.jp-TableOfContents {
  display: flex;
  flex-direction: column;
  background: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  height: 100%;
}

.jp-TableOfContents-placeholder {
  text-align: center;
}

.jp-TableOfContents-placeholderContent {
  color: var(--jp-content-font-color2);
  padding: 8px;
}

.jp-TableOfContents-placeholderContent > h3 {
  margin-bottom: var(--jp-content-heading-margin-bottom);
}

.jp-TableOfContents .jp-SidePanel-content {
  overflow-y: auto;
}

.jp-TableOfContents-tree {
  margin: 4px;
}

.jp-TableOfContents ol {
  list-style-type: none;
}

/* stylelint-disable-next-line selector-max-type */
.jp-TableOfContents li > ol {
  /* Align left border with triangle icon center */
  padding-left: 11px;
}

.jp-TableOfContents-content {
  /* left margin for the active heading indicator */
  margin: 0 0 0 var(--jp-private-toc-active-width);
  padding: 0;
  background-color: var(--jp-layout-color1);
}

.jp-tocItem {
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-tocItem-heading {
  display: flex;
  cursor: pointer;
}

.jp-tocItem-heading:hover {
  background-color: var(--jp-layout-color2);
}

.jp-tocItem-content {
  display: block;
  padding: 4px 0;
  white-space: nowrap;
  text-overflow: ellipsis;
  overflow-x: hidden;
}

.jp-tocItem-collapser {
  height: 20px;
  margin: 2px 2px 0;
  padding: 0;
  background: none;
  border: none;
  cursor: pointer;
}

.jp-tocItem-collapser:hover {
  background-color: var(--jp-layout-color3);
}

/* Active heading indicator */

.jp-tocItem-heading::before {
  content: ' ';
  background: transparent;
  width: var(--jp-private-toc-active-width);
  height: 24px;
  position: absolute;
  left: 0;
  border-radius: var(--jp-border-radius);
}

.jp-tocItem-heading.jp-tocItem-active::before {
  background-color: var(--jp-brand-color1);
}

.jp-tocItem-heading:hover.jp-tocItem-active::before {
  background: var(--jp-brand-color0);
  opacity: 1;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Collapser {
  flex: 0 0 var(--jp-cell-collapser-width);
  padding: 0;
  margin: 0;
  border: none;
  outline: none;
  background: transparent;
  border-radius: var(--jp-border-radius);
  opacity: 1;
}

.jp-Collapser-child {
  display: block;
  width: 100%;
  box-sizing: border-box;

  /* height: 100% doesn't work because the height of its parent is computed from content */
  position: absolute;
  top: 0;
  bottom: 0;
}

/*-----------------------------------------------------------------------------
| Printing
|----------------------------------------------------------------------------*/

/*
Hiding collapsers in print mode.

Note: input and output wrappers have "display: block" propery in print mode.
*/

@media print {
  .jp-Collapser {
    display: none;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Header/Footer
|----------------------------------------------------------------------------*/

/* Hidden by zero height by default */
.jp-CellHeader,
.jp-CellFooter {
  height: 0;
  width: 100%;
  padding: 0;
  margin: 0;
  border: none;
  outline: none;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Input
|----------------------------------------------------------------------------*/

/* All input areas */
.jp-InputArea {
  display: table;
  table-layout: fixed;
  width: 100%;
  overflow: hidden;
}

.jp-InputArea-editor {
  display: table-cell;
  overflow: hidden;
  vertical-align: top;

  /* This is the non-active, default styling */
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  border-radius: 0;
  background: var(--jp-cell-editor-background);
}

.jp-InputPrompt {
  display: table-cell;
  vertical-align: top;
  width: var(--jp-cell-prompt-width);
  color: var(--jp-cell-inprompt-font-color);
  font-family: var(--jp-cell-prompt-font-family);
  padding: var(--jp-code-padding);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  opacity: var(--jp-cell-prompt-opacity);
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;

  /* Right align prompt text, don't wrap to handle large prompt numbers */
  text-align: right;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;

  /* Disable text selection */
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

/*-----------------------------------------------------------------------------
| Mobile
|----------------------------------------------------------------------------*/
@media only screen and (max-width: 760px) {
  .jp-InputArea-editor {
    display: table-row;
    margin-left: var(--jp-notebook-padding);
  }

  .jp-InputPrompt {
    display: table-row;
    text-align: left;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Placeholder
|----------------------------------------------------------------------------*/

.jp-Placeholder {
  display: table;
  table-layout: fixed;
  width: 100%;
}

.jp-Placeholder-prompt {
  display: table-cell;
  box-sizing: border-box;
}

.jp-Placeholder-content {
  display: table-cell;
  padding: 4px 6px;
  border: 1px solid transparent;
  border-radius: 0;
  background: none;
  box-sizing: border-box;
  cursor: pointer;
}

.jp-Placeholder-contentContainer {
  display: flex;
}

.jp-Placeholder-content:hover,
.jp-InputPlaceholder > .jp-Placeholder-content:hover {
  border-color: var(--jp-layout-color3);
}

.jp-Placeholder-content .jp-MoreHorizIcon {
  width: 32px;
  height: 16px;
  border: 1px solid transparent;
  border-radius: var(--jp-border-radius);
}

.jp-Placeholder-content .jp-MoreHorizIcon:hover {
  border: 1px solid var(--jp-border-color1);
  box-shadow: 0 0 2px 0 rgba(0, 0, 0, 0.25);
  background-color: var(--jp-layout-color0);
}

.jp-PlaceholderText {
  white-space: nowrap;
  overflow-x: hidden;
  color: var(--jp-inverse-layout-color3);
  font-family: var(--jp-code-font-family);
}

.jp-InputPlaceholder > .jp-Placeholder-content {
  border-color: var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Private CSS variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-cell-scrolling-output-offset: 5px;
}

/*-----------------------------------------------------------------------------
| Cell
|----------------------------------------------------------------------------*/

.jp-Cell {
  padding: var(--jp-cell-padding);
  margin: 0;
  border: none;
  outline: none;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Common input/output
|----------------------------------------------------------------------------*/

.jp-Cell-inputWrapper,
.jp-Cell-outputWrapper {
  display: flex;
  flex-direction: row;
  padding: 0;
  margin: 0;

  /* Added to reveal the box-shadow on the input and output collapsers. */
  overflow: visible;
}

/* Only input/output areas inside cells */
.jp-Cell-inputArea,
.jp-Cell-outputArea {
  flex: 1 1 auto;
}

/*-----------------------------------------------------------------------------
| Collapser
|----------------------------------------------------------------------------*/

/* Make the output collapser disappear when there is not output, but do so
 * in a manner that leaves it in the layout and preserves its width.
 */
.jp-Cell.jp-mod-noOutputs .jp-Cell-outputCollapser {
  border: none !important;
  background: transparent !important;
}

.jp-Cell:not(.jp-mod-noOutputs) .jp-Cell-outputCollapser {
  min-height: var(--jp-cell-collapser-min-height);
}

/*-----------------------------------------------------------------------------
| Output
|----------------------------------------------------------------------------*/

/* Put a space between input and output when there IS output */
.jp-Cell:not(.jp-mod-noOutputs) .jp-Cell-outputWrapper {
  margin-top: 5px;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea {
  overflow-y: auto;
  max-height: 24em;
  margin-left: var(--jp-private-cell-scrolling-output-offset);
  resize: vertical;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea[style*='height'] {
  max-height: unset;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea::after {
  content: ' ';
  box-shadow: inset 0 0 6px 2px rgb(0 0 0 / 30%);
  width: 100%;
  height: 100%;
  position: sticky;
  bottom: 0;
  top: 0;
  margin-top: -50%;
  float: left;
  display: block;
  pointer-events: none;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-child {
  padding-top: 6px;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-prompt {
  width: calc(
    var(--jp-cell-prompt-width) - var(--jp-private-cell-scrolling-output-offset)
  );
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-promptOverlay {
  left: calc(-1 * var(--jp-private-cell-scrolling-output-offset));
}

/*-----------------------------------------------------------------------------
| CodeCell
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| MarkdownCell
|----------------------------------------------------------------------------*/

.jp-MarkdownOutput {
  display: table-cell;
  width: 100%;
  margin-top: 0;
  margin-bottom: 0;
  padding-left: var(--jp-code-padding);
}

.jp-MarkdownOutput.jp-RenderedHTMLCommon {
  overflow: auto;
}

/* collapseHeadingButton (show always if hiddenCellsButton is _not_ shown) */
.jp-collapseHeadingButton {
  display: flex;
  min-height: var(--jp-cell-collapser-min-height);
  font-size: var(--jp-code-font-size);
  position: absolute;
  background-color: transparent;
  background-size: 25px;
  background-repeat: no-repeat;
  background-position-x: center;
  background-position-y: top;
  background-image: var(--jp-icon-caret-down);
  right: 0;
  top: 0;
  bottom: 0;
}

.jp-collapseHeadingButton.jp-mod-collapsed {
  background-image: var(--jp-icon-caret-right);
}

/*
 set the container font size to match that of content
 so that the nested collapse buttons have the right size
*/
.jp-MarkdownCell .jp-InputPrompt {
  font-size: var(--jp-content-font-size1);
}

/*
  Align collapseHeadingButton with cell top header
  The font sizes are identical to the ones in packages/rendermime/style/base.css
*/
.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='1'] {
  font-size: var(--jp-content-font-size5);
  background-position-y: calc(0.3 * var(--jp-content-font-size5));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='2'] {
  font-size: var(--jp-content-font-size4);
  background-position-y: calc(0.3 * var(--jp-content-font-size4));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='3'] {
  font-size: var(--jp-content-font-size3);
  background-position-y: calc(0.3 * var(--jp-content-font-size3));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='4'] {
  font-size: var(--jp-content-font-size2);
  background-position-y: calc(0.3 * var(--jp-content-font-size2));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='5'] {
  font-size: var(--jp-content-font-size1);
  background-position-y: top;
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='6'] {
  font-size: var(--jp-content-font-size0);
  background-position-y: top;
}

/* collapseHeadingButton (show only on (hover,active) if hiddenCellsButton is shown) */
.jp-Notebook.jp-mod-showHiddenCellsButton .jp-collapseHeadingButton {
  display: none;
}

.jp-Notebook.jp-mod-showHiddenCellsButton
  :is(.jp-MarkdownCell:hover, .jp-mod-active)
  .jp-collapseHeadingButton {
  display: flex;
}

/* showHiddenCellsButton (only show if jp-mod-showHiddenCellsButton is set, which
is a consequence of the showHiddenCellsButton option in Notebook Settings)*/
.jp-Notebook.jp-mod-showHiddenCellsButton .jp-showHiddenCellsButton {
  margin-left: calc(var(--jp-cell-prompt-width) + 2 * var(--jp-code-padding));
  margin-top: var(--jp-code-padding);
  border: 1px solid var(--jp-border-color2);
  background-color: var(--jp-border-color3) !important;
  color: var(--jp-content-font-color0) !important;
  display: flex;
}

.jp-Notebook.jp-mod-showHiddenCellsButton .jp-showHiddenCellsButton:hover {
  background-color: var(--jp-border-color2) !important;
}

.jp-showHiddenCellsButton {
  display: none;
}

/*-----------------------------------------------------------------------------
| Printing
|----------------------------------------------------------------------------*/

/*
Using block instead of flex to allow the use of the break-inside CSS property for
cell outputs.
*/

@media print {
  .jp-Cell-inputWrapper,
  .jp-Cell-outputWrapper {
    display: block;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-notebook-toolbar-padding: 2px 5px 2px 2px;
}

/*-----------------------------------------------------------------------------

/*-----------------------------------------------------------------------------
| Styles
|----------------------------------------------------------------------------*/

.jp-NotebookPanel-toolbar {
  padding: var(--jp-notebook-toolbar-padding);

  /* disable paint containment from lumino 2.0 default strict CSS containment */
  contain: style size !important;
}

.jp-Toolbar-item.jp-Notebook-toolbarCellType .jp-select-wrapper.jp-mod-focused {
  border: none;
  box-shadow: none;
}

.jp-Notebook-toolbarCellTypeDropdown select {
  height: 24px;
  font-size: var(--jp-ui-font-size1);
  line-height: 14px;
  border-radius: 0;
  display: block;
}

.jp-Notebook-toolbarCellTypeDropdown span {
  top: 5px !important;
}

.jp-Toolbar-responsive-popup {
  position: absolute;
  height: fit-content;
  display: flex;
  flex-direction: row;
  flex-wrap: wrap;
  justify-content: flex-end;
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  box-shadow: var(--jp-toolbar-box-shadow);
  background: var(--jp-toolbar-background);
  min-height: var(--jp-toolbar-micro-height);
  padding: var(--jp-notebook-toolbar-padding);
  z-index: 1;
  right: 0;
  top: 0;
}

.jp-Toolbar > .jp-Toolbar-responsive-opener {
  margin-left: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------

/*-----------------------------------------------------------------------------
| Styles
|----------------------------------------------------------------------------*/

.jp-Notebook-ExecutionIndicator {
  position: relative;
  display: inline-block;
  height: 100%;
  z-index: 9997;
}

.jp-Notebook-ExecutionIndicator-tooltip {
  visibility: hidden;
  height: auto;
  width: max-content;
  width: -moz-max-content;
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color1);
  text-align: justify;
  border-radius: 6px;
  padding: 0 5px;
  position: fixed;
  display: table;
}

.jp-Notebook-ExecutionIndicator-tooltip.up {
  transform: translateX(-50%) translateY(-100%) translateY(-32px);
}

.jp-Notebook-ExecutionIndicator-tooltip.down {
  transform: translateX(calc(-100% + 16px)) translateY(5px);
}

.jp-Notebook-ExecutionIndicator-tooltip.hidden {
  display: none;
}

.jp-Notebook-ExecutionIndicator:hover .jp-Notebook-ExecutionIndicator-tooltip {
  visibility: visible;
}

.jp-Notebook-ExecutionIndicator span {
  font-size: var(--jp-ui-font-size1);
  font-family: var(--jp-ui-font-family);
  color: var(--jp-ui-font-color1);
  line-height: 24px;
  display: block;
}

.jp-Notebook-ExecutionIndicator-progress-bar {
  display: flex;
  justify-content: center;
  height: 100%;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*
 * Execution indicator
 */
.jp-tocItem-content::after {
  content: '';

  /* Must be identical to form a circle */
  width: 12px;
  height: 12px;
  background: none;
  border: none;
  position: absolute;
  right: 0;
}

.jp-tocItem-content[data-running='0']::after {
  border-radius: 50%;
  border: var(--jp-border-width) solid var(--jp-inverse-layout-color3);
  background: none;
}

.jp-tocItem-content[data-running='1']::after {
  border-radius: 50%;
  border: var(--jp-border-width) solid var(--jp-inverse-layout-color3);
  background-color: var(--jp-inverse-layout-color3);
}

.jp-tocItem-content[data-running='0'],
.jp-tocItem-content[data-running='1'] {
  margin-right: 12px;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-Notebook-footer {
  height: 27px;
  margin-left: calc(
    var(--jp-cell-prompt-width) + var(--jp-cell-collapser-width) +
      var(--jp-cell-padding)
  );
  width: calc(
    100% -
      (
        var(--jp-cell-prompt-width) + var(--jp-cell-collapser-width) +
          var(--jp-cell-padding) + var(--jp-cell-padding)
      )
  );
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  color: var(--jp-ui-font-color3);
  margin-top: 6px;
  background: none;
  cursor: pointer;
}

.jp-Notebook-footer:focus {
  border-color: var(--jp-cell-editor-active-border-color);
}

/* For devices that support hovering, hide footer until hover */
@media (hover: hover) {
  .jp-Notebook-footer {
    opacity: 0;
  }

  .jp-Notebook-footer:focus,
  .jp-Notebook-footer:hover {
    opacity: 1;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Imports
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| CSS variables
|----------------------------------------------------------------------------*/

:root {
  --jp-side-by-side-output-size: 1fr;
  --jp-side-by-side-resized-cell: var(--jp-side-by-side-output-size);
  --jp-private-notebook-dragImage-width: 304px;
  --jp-private-notebook-dragImage-height: 36px;
  --jp-private-notebook-selected-color: var(--md-blue-400);
  --jp-private-notebook-active-color: var(--md-green-400);
}

/*-----------------------------------------------------------------------------
| Notebook
|----------------------------------------------------------------------------*/

/* stylelint-disable selector-max-class */

.jp-NotebookPanel {
  display: block;
  height: 100%;
}

.jp-NotebookPanel.jp-Document {
  min-width: 240px;
  min-height: 120px;
}

.jp-Notebook {
  padding: var(--jp-notebook-padding);
  outline: none;
  overflow: auto;
  background: var(--jp-layout-color0);
}

.jp-Notebook.jp-mod-scrollPastEnd::after {
  display: block;
  content: '';
  min-height: var(--jp-notebook-scroll-padding);
}

.jp-MainAreaWidget-ContainStrict .jp-Notebook * {
  contain: strict;
}

.jp-Notebook .jp-Cell {
  overflow: visible;
}

.jp-Notebook .jp-Cell .jp-InputPrompt {
  cursor: move;
}

/*-----------------------------------------------------------------------------
| Notebook state related styling
|
| The notebook and cells each have states, here are the possibilities:
|
| - Notebook
|   - Command
|   - Edit
| - Cell
|   - None
|   - Active (only one can be active)
|   - Selected (the cells actions are applied to)
|   - Multiselected (when multiple selected, the cursor)
|   - No outputs
|----------------------------------------------------------------------------*/

/* Command or edit modes */

.jp-Notebook .jp-Cell:not(.jp-mod-active) .jp-InputPrompt {
  opacity: var(--jp-cell-prompt-not-active-opacity);
  color: var(--jp-cell-prompt-not-active-font-color);
}

.jp-Notebook .jp-Cell:not(.jp-mod-active) .jp-OutputPrompt {
  opacity: var(--jp-cell-prompt-not-active-opacity);
  color: var(--jp-cell-prompt-not-active-font-color);
}

/* cell is active */
.jp-Notebook .jp-Cell.jp-mod-active .jp-Collapser {
  background: var(--jp-brand-color1);
}

/* cell is dirty */
.jp-Notebook .jp-Cell.jp-mod-dirty .jp-InputPrompt {
  color: var(--jp-warn-color1);
}

.jp-Notebook .jp-Cell.jp-mod-dirty .jp-InputPrompt::before {
  color: var(--jp-warn-color1);
  content: '';
}

.jp-Notebook .jp-Cell.jp-mod-active.jp-mod-dirty .jp-Collapser {
  background: var(--jp-warn-color1);
}

/* collapser is hovered */
.jp-Notebook .jp-Cell .jp-Collapser:hover {
  box-shadow: var(--jp-elevation-z2);
  background: var(--jp-brand-color1);
  opacity: var(--jp-cell-collapser-not-active-hover-opacity);
}

/* cell is active and collapser is hovered */
.jp-Notebook .jp-Cell.jp-mod-active .jp-Collapser:hover {
  background: var(--jp-brand-color0);
  opacity: 1;
}

/* Command mode */

.jp-Notebook.jp-mod-commandMode .jp-Cell.jp-mod-selected {
  background: var(--jp-notebook-multiselected-color);
}

.jp-Notebook.jp-mod-commandMode
  .jp-Cell.jp-mod-active.jp-mod-selected:not(.jp-mod-multiSelected) {
  background: transparent;
}

/* Edit mode */

.jp-Notebook.jp-mod-editMode .jp-Cell.jp-mod-active .jp-InputArea-editor {
  border: var(--jp-border-width) solid var(--jp-cell-editor-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
  background-color: var(--jp-cell-editor-active-background);
}

/*-----------------------------------------------------------------------------
| Notebook drag and drop
|----------------------------------------------------------------------------*/

.jp-Notebook-cell.jp-mod-dropSource {
  opacity: 0.5;
}

.jp-Notebook-cell.jp-mod-dropTarget,
.jp-Notebook.jp-mod-commandMode
  .jp-Notebook-cell.jp-mod-active.jp-mod-selected.jp-mod-dropTarget {
  border-top-color: var(--jp-private-notebook-selected-color);
  border-top-style: solid;
  border-top-width: 2px;
}

.jp-dragImage {
  display: block;
  flex-direction: row;
  width: var(--jp-private-notebook-dragImage-width);
  height: var(--jp-private-notebook-dragImage-height);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background);
  overflow: visible;
}

.jp-dragImage-singlePrompt {
  box-shadow: 2px 2px 4px 0 rgba(0, 0, 0, 0.12);
}

.jp-dragImage .jp-dragImage-content {
  flex: 1 1 auto;
  z-index: 2;
  font-size: var(--jp-code-font-size);
  font-family: var(--jp-code-font-family);
  line-height: var(--jp-code-line-height);
  padding: var(--jp-code-padding);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background-color);
  color: var(--jp-content-font-color3);
  text-align: left;
  margin: 4px 4px 4px 0;
}

.jp-dragImage .jp-dragImage-prompt {
  flex: 0 0 auto;
  min-width: 36px;
  color: var(--jp-cell-inprompt-font-color);
  padding: var(--jp-code-padding);
  padding-left: 12px;
  font-family: var(--jp-cell-prompt-font-family);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  line-height: 1.9;
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;
}

.jp-dragImage-multipleBack {
  z-index: -1;
  position: absolute;
  height: 32px;
  width: 300px;
  top: 8px;
  left: 8px;
  background: var(--jp-layout-color2);
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  box-shadow: 2px 2px 4px 0 rgba(0, 0, 0, 0.12);
}

/*-----------------------------------------------------------------------------
| Cell toolbar
|----------------------------------------------------------------------------*/

.jp-NotebookTools {
  display: block;
  min-width: var(--jp-sidebar-min-width);
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);

  /* This is needed so that all font sizing of children done in ems is
    * relative to this base size */
  font-size: var(--jp-ui-font-size1);
  overflow: auto;
}

.jp-ActiveCellTool {
  padding: 12px 0;
  display: flex;
}

.jp-ActiveCellTool-Content {
  flex: 1 1 auto;
}

.jp-ActiveCellTool .jp-ActiveCellTool-CellContent {
  background: var(--jp-cell-editor-background);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  border-radius: 0;
  min-height: 29px;
}

.jp-ActiveCellTool .jp-InputPrompt {
  min-width: calc(var(--jp-cell-prompt-width) * 0.75);
}

.jp-ActiveCellTool-CellContent > pre {
  padding: 5px 4px;
  margin: 0;
  white-space: normal;
}

.jp-MetadataEditorTool {
  flex-direction: column;
  padding: 12px 0;
}

.jp-RankedPanel > :not(:first-child) {
  margin-top: 12px;
}

.jp-KeySelector select.jp-mod-styled {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  border: var(--jp-border-width) solid var(--jp-border-color1);
}

.jp-KeySelector label,
.jp-MetadataEditorTool label,
.jp-NumberSetter label {
  line-height: 1.4;
}

.jp-NotebookTools .jp-select-wrapper {
  margin-top: 4px;
  margin-bottom: 0;
}

.jp-NumberSetter input {
  width: 100%;
  margin-top: 4px;
}

.jp-NotebookTools .jp-Collapse {
  margin-top: 16px;
}

/*-----------------------------------------------------------------------------
| Presentation Mode (.jp-mod-presentationMode)
|----------------------------------------------------------------------------*/

.jp-mod-presentationMode .jp-Notebook {
  --jp-content-font-size1: var(--jp-content-presentation-font-size1);
  --jp-code-font-size: var(--jp-code-presentation-font-size);
}

.jp-mod-presentationMode .jp-Notebook .jp-Cell .jp-InputPrompt,
.jp-mod-presentationMode .jp-Notebook .jp-Cell .jp-OutputPrompt {
  flex: 0 0 110px;
}

/*-----------------------------------------------------------------------------
| Side-by-side Mode (.jp-mod-sideBySide)
|----------------------------------------------------------------------------*/
.jp-mod-sideBySide.jp-Notebook .jp-Notebook-cell {
  margin-top: 3em;
  margin-bottom: 3em;
  margin-left: 5%;
  margin-right: 5%;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell {
  display: grid;
  grid-template-columns: minmax(0, 1fr) min-content minmax(
      0,
      var(--jp-side-by-side-output-size)
    );
  grid-template-rows: auto minmax(0, 1fr) auto;
  grid-template-areas:
    'header header header'
    'input handle output'
    'footer footer footer';
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell.jp-mod-resizedCell {
  grid-template-columns: minmax(0, 1fr) min-content minmax(
      0,
      var(--jp-side-by-side-resized-cell)
    );
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellHeader {
  grid-area: header;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-Cell-inputWrapper {
  grid-area: input;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-Cell-outputWrapper {
  /* overwrite the default margin (no vertical separation needed in side by side move */
  margin-top: 0;
  grid-area: output;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellFooter {
  grid-area: footer;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellResizeHandle {
  grid-area: handle;
  user-select: none;
  display: block;
  height: 100%;
  cursor: ew-resize;
  padding: 0 var(--jp-cell-padding);
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellResizeHandle::after {
  content: '';
  display: block;
  background: var(--jp-border-color2);
  height: 100%;
  width: 5px;
}

.jp-mod-sideBySide.jp-Notebook
  .jp-CodeCell.jp-mod-resizedCell
  .jp-CellResizeHandle::after {
  background: var(--jp-border-color0);
}

.jp-CellResizeHandle {
  display: none;
}

/*-----------------------------------------------------------------------------
| Placeholder
|----------------------------------------------------------------------------*/

.jp-Cell-Placeholder {
  padding-left: 55px;
}

.jp-Cell-Placeholder-wrapper {
  background: #fff;
  border: 1px solid;
  border-color: #e5e6e9 #dfe0e4 #d0d1d5;
  border-radius: 4px;
  -webkit-border-radius: 4px;
  margin: 10px 15px;
}

.jp-Cell-Placeholder-wrapper-inner {
  padding: 15px;
  position: relative;
}

.jp-Cell-Placeholder-wrapper-body {
  background-repeat: repeat;
  background-size: 50% auto;
}

.jp-Cell-Placeholder-wrapper-body div {
  background: #f6f7f8;
  background-image: -webkit-linear-gradient(
    left,
    #f6f7f8 0%,
    #edeef1 20%,
    #f6f7f8 40%,
    #f6f7f8 100%
  );
  background-repeat: no-repeat;
  background-size: 800px 104px;
  height: 104px;
  position: absolute;
  right: 15px;
  left: 15px;
  top: 15px;
}

div.jp-Cell-Placeholder-h1 {
  top: 20px;
  height: 20px;
  left: 15px;
  width: 150px;
}

div.jp-Cell-Placeholder-h2 {
  left: 15px;
  top: 50px;
  height: 10px;
  width: 100px;
}

div.jp-Cell-Placeholder-content-1,
div.jp-Cell-Placeholder-content-2,
div.jp-Cell-Placeholder-content-3 {
  left: 15px;
  right: 15px;
  height: 10px;
}

div.jp-Cell-Placeholder-content-1 {
  top: 100px;
}

div.jp-Cell-Placeholder-content-2 {
  top: 120px;
}

div.jp-Cell-Placeholder-content-3 {
  top: 140px;
}

</style>
<style type="text/css">
/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*
The following CSS variables define the main, public API for styling JupyterLab.
These variables should be used by all plugins wherever possible. In other
words, plugins should not define custom colors, sizes, etc unless absolutely
necessary. This enables users to change the visual theme of JupyterLab
by changing these variables.

Many variables appear in an ordered sequence (0,1,2,3). These sequences
are designed to work well together, so for example, `--jp-border-color1` should
be used with `--jp-layout-color1`. The numbers have the following meanings:

* 0: super-primary, reserved for special emphasis
* 1: primary, most important under normal situations
* 2: secondary, next most important under normal situations
* 3: tertiary, next most important under normal situations

Throughout JupyterLab, we are mostly following principles from Google's
Material Design when selecting colors. We are not, however, following
all of MD as it is not optimized for dense, information rich UIs.
*/

:root {
  /* Elevation
   *
   * We style box-shadows using Material Design's idea of elevation. These particular numbers are taken from here:
   *
   * https://github.com/material-components/material-components-web
   * https://material-components-web.appspot.com/elevation.html
   */

  --jp-shadow-base-lightness: 0;
  --jp-shadow-umbra-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.2
  );
  --jp-shadow-penumbra-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.14
  );
  --jp-shadow-ambient-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.12
  );
  --jp-elevation-z0: none;
  --jp-elevation-z1: 0 2px 1px -1px var(--jp-shadow-umbra-color),
    0 1px 1px 0 var(--jp-shadow-penumbra-color),
    0 1px 3px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z2: 0 3px 1px -2px var(--jp-shadow-umbra-color),
    0 2px 2px 0 var(--jp-shadow-penumbra-color),
    0 1px 5px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z4: 0 2px 4px -1px var(--jp-shadow-umbra-color),
    0 4px 5px 0 var(--jp-shadow-penumbra-color),
    0 1px 10px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z6: 0 3px 5px -1px var(--jp-shadow-umbra-color),
    0 6px 10px 0 var(--jp-shadow-penumbra-color),
    0 1px 18px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z8: 0 5px 5px -3px var(--jp-shadow-umbra-color),
    0 8px 10px 1px var(--jp-shadow-penumbra-color),
    0 3px 14px 2px var(--jp-shadow-ambient-color);
  --jp-elevation-z12: 0 7px 8px -4px var(--jp-shadow-umbra-color),
    0 12px 17px 2px var(--jp-shadow-penumbra-color),
    0 5px 22px 4px var(--jp-shadow-ambient-color);
  --jp-elevation-z16: 0 8px 10px -5px var(--jp-shadow-umbra-color),
    0 16px 24px 2px var(--jp-shadow-penumbra-color),
    0 6px 30px 5px var(--jp-shadow-ambient-color);
  --jp-elevation-z20: 0 10px 13px -6px var(--jp-shadow-umbra-color),
    0 20px 31px 3px var(--jp-shadow-penumbra-color),
    0 8px 38px 7px var(--jp-shadow-ambient-color);
  --jp-elevation-z24: 0 11px 15px -7px var(--jp-shadow-umbra-color),
    0 24px 38px 3px var(--jp-shadow-penumbra-color),
    0 9px 46px 8px var(--jp-shadow-ambient-color);

  /* Borders
   *
   * The following variables, specify the visual styling of borders in JupyterLab.
   */

  --jp-border-width: 1px;
  --jp-border-color0: var(--md-grey-400);
  --jp-border-color1: var(--md-grey-400);
  --jp-border-color2: var(--md-grey-300);
  --jp-border-color3: var(--md-grey-200);
  --jp-inverse-border-color: var(--md-grey-600);
  --jp-border-radius: 2px;

  /* UI Fonts
   *
   * The UI font CSS variables are used for the typography all of the JupyterLab
   * user interface elements that are not directly user generated content.
   *
   * The font sizing here is done assuming that the body font size of --jp-ui-font-size1
   * is applied to a parent element. When children elements, such as headings, are sized
   * in em all things will be computed relative to that body size.
   */

  --jp-ui-font-scale-factor: 1.2;
  --jp-ui-font-size0: 0.83333em;
  --jp-ui-font-size1: 13px; /* Base font size */
  --jp-ui-font-size2: 1.2em;
  --jp-ui-font-size3: 1.44em;
  --jp-ui-font-family: system-ui, -apple-system, blinkmacsystemfont, 'Segoe UI',
    helvetica, arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji',
    'Segoe UI Symbol';

  /*
   * Use these font colors against the corresponding main layout colors.
   * In a light theme, these go from dark to light.
   */

  /* Defaults use Material Design specification */
  --jp-ui-font-color0: rgba(0, 0, 0, 1);
  --jp-ui-font-color1: rgba(0, 0, 0, 0.87);
  --jp-ui-font-color2: rgba(0, 0, 0, 0.54);
  --jp-ui-font-color3: rgba(0, 0, 0, 0.38);

  /*
   * Use these against the brand/accent/warn/error colors.
   * These will typically go from light to darker, in both a dark and light theme.
   */

  --jp-ui-inverse-font-color0: rgba(255, 255, 255, 1);
  --jp-ui-inverse-font-color1: rgba(255, 255, 255, 1);
  --jp-ui-inverse-font-color2: rgba(255, 255, 255, 0.7);
  --jp-ui-inverse-font-color3: rgba(255, 255, 255, 0.5);

  /* Content Fonts
   *
   * Content font variables are used for typography of user generated content.
   *
   * The font sizing here is done assuming that the body font size of --jp-content-font-size1
   * is applied to a parent element. When children elements, such as headings, are sized
   * in em all things will be computed relative to that body size.
   */

  --jp-content-line-height: 1.6;
  --jp-content-font-scale-factor: 1.2;
  --jp-content-font-size0: 0.83333em;
  --jp-content-font-size1: 14px; /* Base font size */
  --jp-content-font-size2: 1.2em;
  --jp-content-font-size3: 1.44em;
  --jp-content-font-size4: 1.728em;
  --jp-content-font-size5: 2.0736em;

  /* This gives a magnification of about 125% in presentation mode over normal. */
  --jp-content-presentation-font-size1: 17px;
  --jp-content-heading-line-height: 1;
  --jp-content-heading-margin-top: 1.2em;
  --jp-content-heading-margin-bottom: 0.8em;
  --jp-content-heading-font-weight: 500;

  /* Defaults use Material Design specification */
  --jp-content-font-color0: rgba(0, 0, 0, 1);
  --jp-content-font-color1: rgba(0, 0, 0, 0.87);
  --jp-content-font-color2: rgba(0, 0, 0, 0.54);
  --jp-content-font-color3: rgba(0, 0, 0, 0.38);
  --jp-content-link-color: var(--md-blue-900);
  --jp-content-font-family: system-ui, -apple-system, blinkmacsystemfont,
    'Segoe UI', helvetica, arial, sans-serif, 'Apple Color Emoji',
    'Segoe UI Emoji', 'Segoe UI Symbol';

  /*
   * Code Fonts
   *
   * Code font variables are used for typography of code and other monospaces content.
   */

  --jp-code-font-size: 13px;
  --jp-code-line-height: 1.3077; /* 17px for 13px base */
  --jp-code-padding: 5px; /* 5px for 13px base, codemirror highlighting needs integer px value */
  --jp-code-font-family-default: menlo, consolas, 'DejaVu Sans Mono', monospace;
  --jp-code-font-family: var(--jp-code-font-family-default);

  /* This gives a magnification of about 125% in presentation mode over normal. */
  --jp-code-presentation-font-size: 16px;

  /* may need to tweak cursor width if you change font size */
  --jp-code-cursor-width0: 1.4px;
  --jp-code-cursor-width1: 2px;
  --jp-code-cursor-width2: 4px;

  /* Layout
   *
   * The following are the main layout colors use in JupyterLab. In a light
   * theme these would go from light to dark.
   */

  --jp-layout-color0: white;
  --jp-layout-color1: white;
  --jp-layout-color2: var(--md-grey-200);
  --jp-layout-color3: var(--md-grey-400);
  --jp-layout-color4: var(--md-grey-600);

  /* Inverse Layout
   *
   * The following are the inverse layout colors use in JupyterLab. In a light
   * theme these would go from dark to light.
   */

  --jp-inverse-layout-color0: #111;
  --jp-inverse-layout-color1: var(--md-grey-900);
  --jp-inverse-layout-color2: var(--md-grey-800);
  --jp-inverse-layout-color3: var(--md-grey-700);
  --jp-inverse-layout-color4: var(--md-grey-600);

  /* Brand/accent */

  --jp-brand-color0: var(--md-blue-900);
  --jp-brand-color1: var(--md-blue-700);
  --jp-brand-color2: var(--md-blue-300);
  --jp-brand-color3: var(--md-blue-100);
  --jp-brand-color4: var(--md-blue-50);
  --jp-accent-color0: var(--md-green-900);
  --jp-accent-color1: var(--md-green-700);
  --jp-accent-color2: var(--md-green-300);
  --jp-accent-color3: var(--md-green-100);

  /* State colors (warn, error, success, info) */

  --jp-warn-color0: var(--md-orange-900);
  --jp-warn-color1: var(--md-orange-700);
  --jp-warn-color2: var(--md-orange-300);
  --jp-warn-color3: var(--md-orange-100);
  --jp-error-color0: var(--md-red-900);
  --jp-error-color1: var(--md-red-700);
  --jp-error-color2: var(--md-red-300);
  --jp-error-color3: var(--md-red-100);
  --jp-success-color0: var(--md-green-900);
  --jp-success-color1: var(--md-green-700);
  --jp-success-color2: var(--md-green-300);
  --jp-success-color3: var(--md-green-100);
  --jp-info-color0: var(--md-cyan-900);
  --jp-info-color1: var(--md-cyan-700);
  --jp-info-color2: var(--md-cyan-300);
  --jp-info-color3: var(--md-cyan-100);

  /* Cell specific styles */

  --jp-cell-padding: 5px;
  --jp-cell-collapser-width: 8px;
  --jp-cell-collapser-min-height: 20px;
  --jp-cell-collapser-not-active-hover-opacity: 0.6;
  --jp-cell-editor-background: var(--md-grey-100);
  --jp-cell-editor-border-color: var(--md-grey-300);
  --jp-cell-editor-box-shadow: inset 0 0 2px var(--md-blue-300);
  --jp-cell-editor-active-background: var(--jp-layout-color0);
  --jp-cell-editor-active-border-color: var(--jp-brand-color1);
  --jp-cell-prompt-width: 64px;
  --jp-cell-prompt-font-family: var(--jp-code-font-family-default);
  --jp-cell-prompt-letter-spacing: 0;
  --jp-cell-prompt-opacity: 1;
  --jp-cell-prompt-not-active-opacity: 0.5;
  --jp-cell-prompt-not-active-font-color: var(--md-grey-700);

  /* A custom blend of MD grey and blue 600
   * See https://meyerweb.com/eric/tools/color-blend/#546E7A:1E88E5:5:hex */
  --jp-cell-inprompt-font-color: #307fc1;

  /* A custom blend of MD grey and orange 600
   * https://meyerweb.com/eric/tools/color-blend/#546E7A:F4511E:5:hex */
  --jp-cell-outprompt-font-color: #bf5b3d;

  /* Notebook specific styles */

  --jp-notebook-padding: 10px;
  --jp-notebook-select-background: var(--jp-layout-color1);
  --jp-notebook-multiselected-color: var(--md-blue-50);

  /* The scroll padding is calculated to fill enough space at the bottom of the
  notebook to show one single-line cell (with appropriate padding) at the top
  when the notebook is scrolled all the way to the bottom. We also subtract one
  pixel so that no scrollbar appears if we have just one single-line cell in the
  notebook. This padding is to enable a 'scroll past end' feature in a notebook.
  */
  --jp-notebook-scroll-padding: calc(
    100% - var(--jp-code-font-size) * var(--jp-code-line-height) -
      var(--jp-code-padding) - var(--jp-cell-padding) - 1px
  );

  /* Rendermime styles */

  --jp-rendermime-error-background: #fdd;
  --jp-rendermime-table-row-background: var(--md-grey-100);
  --jp-rendermime-table-row-hover-background: var(--md-light-blue-50);

  /* Dialog specific styles */

  --jp-dialog-background: rgba(0, 0, 0, 0.25);

  /* Console specific styles */

  --jp-console-padding: 10px;

  /* Toolbar specific styles */

  --jp-toolbar-border-color: var(--jp-border-color1);
  --jp-toolbar-micro-height: 8px;
  --jp-toolbar-background: var(--jp-layout-color1);
  --jp-toolbar-box-shadow: 0 0 2px 0 rgba(0, 0, 0, 0.24);
  --jp-toolbar-header-margin: 4px 4px 0 4px;
  --jp-toolbar-active-background: var(--md-grey-300);

  /* Statusbar specific styles */

  --jp-statusbar-height: 24px;

  /* Input field styles */

  --jp-input-box-shadow: inset 0 0 2px var(--md-blue-300);
  --jp-input-active-background: var(--jp-layout-color1);
  --jp-input-hover-background: var(--jp-layout-color1);
  --jp-input-background: var(--md-grey-100);
  --jp-input-border-color: var(--jp-inverse-border-color);
  --jp-input-active-border-color: var(--jp-brand-color1);
  --jp-input-active-box-shadow-color: rgba(19, 124, 189, 0.3);

  /* General editor styles */

  --jp-editor-selected-background: #d9d9d9;
  --jp-editor-selected-focused-background: #d7d4f0;
  --jp-editor-cursor-color: var(--jp-ui-font-color0);

  /* Code mirror specific styles */

  --jp-mirror-editor-keyword-color: #008000;
  --jp-mirror-editor-atom-color: #88f;
  --jp-mirror-editor-number-color: #080;
  --jp-mirror-editor-def-color: #00f;
  --jp-mirror-editor-variable-color: var(--md-grey-900);
  --jp-mirror-editor-variable-2-color: rgb(0, 54, 109);
  --jp-mirror-editor-variable-3-color: #085;
  --jp-mirror-editor-punctuation-color: #05a;
  --jp-mirror-editor-property-color: #05a;
  --jp-mirror-editor-operator-color: #a2f;
  --jp-mirror-editor-comment-color: #408080;
  --jp-mirror-editor-string-color: #ba2121;
  --jp-mirror-editor-string-2-color: #708;
  --jp-mirror-editor-meta-color: #a2f;
  --jp-mirror-editor-qualifier-color: #555;
  --jp-mirror-editor-builtin-color: #008000;
  --jp-mirror-editor-bracket-color: #997;
  --jp-mirror-editor-tag-color: #170;
  --jp-mirror-editor-attribute-color: #00c;
  --jp-mirror-editor-header-color: blue;
  --jp-mirror-editor-quote-color: #090;
  --jp-mirror-editor-link-color: #00c;
  --jp-mirror-editor-error-color: #f00;
  --jp-mirror-editor-hr-color: #999;

  /*
    RTC user specific colors.
    These colors are used for the cursor, username in the editor,
    and the icon of the user.
  */

  --jp-collaborator-color1: #ffad8e;
  --jp-collaborator-color2: #dac83d;
  --jp-collaborator-color3: #72dd76;
  --jp-collaborator-color4: #00e4d0;
  --jp-collaborator-color5: #45d4ff;
  --jp-collaborator-color6: #e2b1ff;
  --jp-collaborator-color7: #ff9de6;

  /* Vega extension styles */

  --jp-vega-background: white;

  /* Sidebar-related styles */

  --jp-sidebar-min-width: 250px;

  /* Search-related styles */

  --jp-search-toggle-off-opacity: 0.5;
  --jp-search-toggle-hover-opacity: 0.8;
  --jp-search-toggle-on-opacity: 1;
  --jp-search-selected-match-background-color: rgb(245, 200, 0);
  --jp-search-selected-match-color: black;
  --jp-search-unselected-match-background-color: var(
    --jp-inverse-layout-color0
  );
  --jp-search-unselected-match-color: var(--jp-ui-inverse-font-color0);

  /* Icon colors that work well with light or dark backgrounds */
  --jp-icon-contrast-color0: var(--md-purple-600);
  --jp-icon-contrast-color1: var(--md-green-600);
  --jp-icon-contrast-color2: var(--md-pink-600);
  --jp-icon-contrast-color3: var(--md-blue-600);

  /* Button colors */
  --jp-accept-color-normal: var(--md-blue-700);
  --jp-accept-color-hover: var(--md-blue-800);
  --jp-accept-color-active: var(--md-blue-900);
  --jp-warn-color-normal: var(--md-red-700);
  --jp-warn-color-hover: var(--md-red-800);
  --jp-warn-color-active: var(--md-red-900);
  --jp-reject-color-normal: var(--md-grey-600);
  --jp-reject-color-hover: var(--md-grey-700);
  --jp-reject-color-active: var(--md-grey-800);

  /* File or activity icons and switch semantic variables */
  --jp-jupyter-icon-color: #f37626;
  --jp-notebook-icon-color: #f37626;
  --jp-json-icon-color: var(--md-orange-700);
  --jp-console-icon-background-color: var(--md-blue-700);
  --jp-console-icon-color: white;
  --jp-terminal-icon-background-color: var(--md-grey-800);
  --jp-terminal-icon-color: var(--md-grey-200);
  --jp-text-editor-icon-color: var(--md-grey-700);
  --jp-inspector-icon-color: var(--md-grey-700);
  --jp-switch-color: var(--md-grey-400);
  --jp-switch-true-position-color: var(--md-orange-900);
}
</style>
<style type="text/css">
/* Force rendering true colors when outputing to pdf */
* {
  -webkit-print-color-adjust: exact;
}

/* Misc */
a.anchor-link {
  display: none;
}

/* Input area styling */
.jp-InputArea {
  overflow: hidden;
}

.jp-InputArea-editor {
  overflow: hidden;
}

.cm-editor.cm-s-jupyter .highlight pre {
/* weird, but --jp-code-padding defined to be 5px but 4px horizontal padding is hardcoded for pre.cm-line */
  padding: var(--jp-code-padding) 4px;
  margin: 0;

  font-family: inherit;
  font-size: inherit;
  line-height: inherit;
  color: inherit;

}

.jp-OutputArea-output pre {
  line-height: inherit;
  font-family: inherit;
}

.jp-RenderedText pre {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-code-font-size);
}

/* Hiding the collapser by default */
.jp-Collapser {
  display: none;
}

@page {
    margin: 0.5in; /* Margin for each printed piece of paper */
}

@media print {
  .jp-Cell-inputWrapper,
  .jp-Cell-outputWrapper {
    display: block;
  }
}
</style>
<!-- Load mathjax -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML-full,Safe"> </script>
<!-- MathJax configuration -->
<script type="text/x-mathjax-config">
    init_mathjax = function() {
        if (window.MathJax) {
        // MathJax loaded
            MathJax.Hub.Config({
                TeX: {
                    equationNumbers: {
                    autoNumber: "AMS",
                    useLabelIds: true
                    }
                },
                tex2jax: {
                    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                    processEscapes: true,
                    processEnvironments: true
                },
                displayAlign: 'center',
                messageStyle: 'none',
                CommonHTML: {
                    linebreaks: {
                    automatic: true
                    }
                }
            });

            MathJax.Hub.Queue(["Typeset", MathJax.Hub]);
        }
    }
    init_mathjax();
    </script>
<!-- End of mathjax configuration --><script type="module">
  document.addEventListener("DOMContentLoaded", async () => {
    const diagrams = document.querySelectorAll(".jp-Mermaid > pre.mermaid");
    // do not load mermaidjs if not needed
    if (!diagrams.length) {
      return;
    }
    const mermaid = (await import("https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.7.0/mermaid.esm.min.mjs")).default;
    const parser = new DOMParser();

    mermaid.initialize({
      maxTextSize: 100000,
      maxEdges: 100000,
      startOnLoad: false,
      fontFamily: window
        .getComputedStyle(document.body)
        .getPropertyValue("--jp-ui-font-family"),
      theme: document.querySelector("body[data-jp-theme-light='true']")
        ? "default"
        : "dark",
    });

    let _nextMermaidId = 0;

    function makeMermaidImage(svg) {
      const img = document.createElement("img");
      const doc = parser.parseFromString(svg, "image/svg+xml");
      const svgEl = doc.querySelector("svg");
      const { maxWidth } = svgEl?.style || {};
      const firstTitle = doc.querySelector("title");
      const firstDesc = doc.querySelector("desc");

      img.setAttribute("src", `data:image/svg+xml,${encodeURIComponent(svg)}`);
      if (maxWidth) {
        img.width = parseInt(maxWidth);
      }
      if (firstTitle) {
        img.setAttribute("alt", firstTitle.textContent);
      }
      if (firstDesc) {
        const caption = document.createElement("figcaption");
        caption.className = "sr-only";
        caption.textContent = firstDesc.textContent;
        return [img, caption];
      }
      return [img];
    }

    async function makeMermaidError(text) {
      let errorMessage = "";
      try {
        await mermaid.parse(text);
      } catch (err) {
        errorMessage = `${err}`;
      }

      const result = document.createElement("details");
      result.className = 'jp-RenderedMermaid-Details';
      const summary = document.createElement("summary");
      summary.className = 'jp-RenderedMermaid-Summary';
      const pre = document.createElement("pre");
      const code = document.createElement("code");
      code.innerText = text;
      pre.appendChild(code);
      summary.appendChild(pre);
      result.appendChild(summary);

      const warning = document.createElement("pre");
      warning.innerText = errorMessage;
      result.appendChild(warning);
      return [result];
    }

    async function renderOneMarmaid(src) {
      const id = `jp-mermaid-${_nextMermaidId++}`;
      const parent = src.parentNode;
      let raw = src.textContent.trim();
      const el = document.createElement("div");
      el.style.visibility = "hidden";
      document.body.appendChild(el);
      let results = null;
      let output = null;
      try {
        let { svg } = await mermaid.render(id, raw, el);
        svg = cleanMermaidSvg(svg);
        results = makeMermaidImage(svg);
        output = document.createElement("figure");
        results.map(output.appendChild, output);
      } catch (err) {
        parent.classList.add("jp-mod-warning");
        results = await makeMermaidError(raw);
        output = results[0];
      } finally {
        el.remove();
      }
      parent.classList.add("jp-RenderedMermaid");
      parent.appendChild(output);
    }


    /**
     * Post-process to ensure mermaid diagrams contain only valid SVG and XHTML.
     */
    function cleanMermaidSvg(svg) {
      return svg.replace(RE_VOID_ELEMENT, replaceVoidElement);
    }


    /**
     * A regular expression for all void elements, which may include attributes and
     * a slash.
     *
     * @see https://developer.mozilla.org/en-US/docs/Glossary/Void_element
     *
     * Of these, only `<br>` is generated by Mermaid in place of `\n`,
     * but _any_ "malformed" tag will break the SVG rendering entirely.
     */
    const RE_VOID_ELEMENT =
      /<\s*(area|base|br|col|embed|hr|img|input|link|meta|param|source|track|wbr)\s*([^>]*?)\s*>/gi;

    /**
     * Ensure a void element is closed with a slash, preserving any attributes.
     */
    function replaceVoidElement(match, tag, rest) {
      rest = rest.trim();
      if (!rest.endsWith('/')) {
        rest = `${rest} /`;
      }
      return `<${tag} ${rest}>`;
    }

    void Promise.all([...diagrams].map(renderOneMarmaid));
  });
</script>
<style>
  .jp-Mermaid:not(.jp-RenderedMermaid) {
    display: none;
  }

  .jp-RenderedMermaid {
    overflow: auto;
    display: flex;
  }

  .jp-RenderedMermaid.jp-mod-warning {
    width: auto;
    padding: 0.5em;
    margin-top: 0.5em;
    border: var(--jp-border-width) solid var(--jp-warn-color2);
    border-radius: var(--jp-border-radius);
    color: var(--jp-ui-font-color1);
    font-size: var(--jp-ui-font-size1);
    white-space: pre-wrap;
    word-wrap: break-word;
  }

  .jp-RenderedMermaid figure {
    margin: 0;
    overflow: auto;
    max-width: 100%;
  }

  .jp-RenderedMermaid img {
    max-width: 100%;
  }

  .jp-RenderedMermaid-Details > pre {
    margin-top: 1em;
  }

  .jp-RenderedMermaid-Summary {
    color: var(--jp-warn-color2);
  }

  .jp-RenderedMermaid:not(.jp-mod-warning) pre {
    display: none;
  }

  .jp-RenderedMermaid-Summary > pre {
    display: inline-block;
    white-space: normal;
  }
</style>
<!-- End of mermaid configuration --></head>
<body class="jp-Notebook" data-jp-theme-light="true" data-jp-theme-name="JupyterLab Light">
<main>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h1 id="Neural-Machine-Translation-with-Attention">Neural Machine Translation with Attention<a class="anchor-link" href="#Neural-Machine-Translation-with-Attention"></a></h1><p>Advanced Learning Fall 2024.<br/>
Last updated: 2025-01-12</p>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>For SUBMISSION:</p>
<p>Please upload the complete and executed <code>ipynb</code> to your git repository. Verify that all of your output can be viewed directly from github, and provide a link to that git file below.</p>
<pre><code>STUDENT ID: 209307396
</code></pre>
<pre><code>STUDENT GIT LINK: https://github.com/meirabar/Adv.-computational-learning-and-data-analysis---52025.git
</code></pre>
<p>In Addition, don't forget to add your ID to the files, and upload to moodle the html version:</p>
<p><code>PS3_Attention_2024_ID_[209307396].html</code></p>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>In this problem set we are going to jump into the depths of <code>seq2seq</code> and <code>attention</code> and build a couple of PyTorch translation mechanisms with some  twists.</p>
<ul>
<li>Part 1 consists of a somewhat unorthodox <code>seq2seq</code> model for simple arithmetics</li>
<li>Part 2 consists of an <code>seq2seq - attention</code> language translation model. We will use it for Hebrew and English.</li>
</ul>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<hr/>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>A <strong>seq2seq</strong> model (sequence-to-sequence model) is a type of neural network designed specifically to handle sequences of data. The model converts input sequences into other sequences of data. This makes them particularly useful for tasks involving language, where the input and output are naturally sequences of words.</p>
<p>Here's a breakdown of how <code>seq2seq</code> models work:</p>
<ul>
<li><p>The encoder takes the input sequence, like a sentence in English, and processes it to capture its meaning and context.</p>
</li>
<li><p>information is then passed to the decoder, which uses it to generate the output sequence, like a translation in French.</p>
</li>
<li><p>Attention mechanism (optional): Some <code>seq2seq</code> models also incorporate an attention mechanism. This allows the decoder to focus on specific parts of the input sequence that are most relevant to generating the next element in the output sequence.</p>
</li>
</ul>
<p><code>seq2seq</code> models are used in many natural language processing (NLP) tasks.</p>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>imports: (feel free to add)</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[25]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># from __future__ import unicode_literals, print_function, division</span>
<span class="c1"># from io import open</span>
<span class="c1"># import unicodedata</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">unicodedata</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">optim</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">TensorDataset</span><span class="p">,</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">RandomSampler</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="Part-1:-Seq2Seq-Arithmetic-model">Part 1: Seq2Seq Arithmetic model<a class="anchor-link" href="#Part-1:-Seq2Seq-Arithmetic-model"></a></h2>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p><strong>Using RNN <code>seq2seq</code> model to "learn" simple arithmetics!</strong></p>
<blockquote>
<p>Given the string "54-7", the model should return a prediction: "47".<br/>
Given the string "10+20", the model should return a prediction: "30".</p>
</blockquote>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<ul>
<li>Watch Lukas Biewald's short <a href="https://youtu.be/MqugtGD605k?si=rAH34ZTJyYDj-XJ1">video</a> explaining <code>seq2seq</code> models and his toy application (somewhat outdated).</li>
<li>You can find the code for his example <a href="https://github.com/lukas/ml-class/blob/master/videos/seq2seq/train.py">here</a>.</li>
</ul>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>1.1) Using Lukas' code, implement a <code>seq2seq</code> network that can learn how to solve <strong>addition AND substraction</strong> of two numbers of maximum length of 4, using the following steps (similar to the example):</p>
<ul>
<li>Generate data; X: queries (two numbers), and Y: answers</li>
<li>One-hot encode X and Y,</li>
<li>Build a <code>seq2seq</code> network (with LSTM, RepeatVector, and TimeDistributed layers)</li>
<li>Train the model.</li>
<li>While training, sample from the validation set at random so we can visualize the generated solutions against the true solutions.</li>
</ul>
<p>Notes:</p>
<ul>
<li>The code in the example is quite old and based on Keras. You might have to adapt some of the code to overcome methods/code that is not supported anymore. Hint: for the evaluation part, review the type and format of the "correct" output - this will help you fix the unsupported "model.predict_classes".</li>
<li>Please use the parameters in the code cell below to train the model.</li>
<li>Instead of using a <code>wandb.config</code> object, please use a simple dictionary instead.</li>
<li>You don't need to run the model for more than 50 iterations (epochs) to get a gist of what is happening and what the algorithm is doing.</li>
<li>Extra credit if you can implement the network in PyTorch (this is not difficult).</li>
<li>Extra credit if you are able to significantly improve the model.</li>
</ul>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[4]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Attention</span><span class="p">,</span> <span class="n">Input</span><span class="p">,</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">LSTM</span><span class="p">,</span> <span class="n">RepeatVector</span><span class="p">,</span> <span class="n">TimeDistributed</span><span class="p">,</span> <span class="n">Layer</span><span class="p">,</span> <span class="n">Concatenate</span>


<span class="k">class</span> <span class="nc">CharacterTable</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">chars</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">chars</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">chars</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">char_indices</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">((</span><span class="n">c</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">chars</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">indices_char</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">((</span><span class="n">i</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">chars</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">encode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">num_rows</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">num_rows</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">chars</span><span class="p">)))</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">C</span><span class="p">):</span>
            <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">char_indices</span><span class="p">[</span><span class="n">c</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">calc_argmax</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">calc_argmax</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="s1">''</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">indices_char</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">x</span><span class="p">)</span>

<span class="c1"># Configuration parameters</span>
<span class="n">config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">"training_size"</span><span class="p">:</span> <span class="mi">40000</span><span class="p">,</span>
    <span class="s2">"digits"</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
    <span class="s2">"hidden_size"</span><span class="p">:</span> <span class="mi">128</span><span class="p">,</span>
    <span class="s2">"batch_size"</span><span class="p">:</span> <span class="mi">128</span><span class="p">,</span>
    <span class="s2">"iterations"</span><span class="p">:</span> <span class="mi">50</span>
<span class="p">}</span>

<span class="c1"># Define character set and maximum length</span>
<span class="n">chars</span> <span class="o">=</span> <span class="s1">'0123456789-+ '</span>
<span class="n">maxlen</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s2">"digits"</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">config</span><span class="p">[</span><span class="s2">"digits"</span><span class="p">]</span>
<span class="n">ctable</span> <span class="o">=</span> <span class="n">CharacterTable</span><span class="p">(</span><span class="n">chars</span><span class="p">)</span>

<span class="c1"># Generate training data</span>
<span class="n">questions</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">expected</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">seen</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Generating data...'</span><span class="p">)</span>
<span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">questions</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">config</span><span class="p">[</span><span class="s2">"training_size"</span><span class="p">]:</span>
    <span class="n">f</span> <span class="o">=</span> <span class="k">lambda</span><span class="p">:</span> <span class="nb">int</span><span class="p">(</span><span class="s1">''</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="s1">'0123456789'</span><span class="p">))</span>
                    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">config</span><span class="p">[</span><span class="s2">"digits"</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))))</span>
    <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">f</span><span class="p">(),</span> <span class="n">f</span><span class="p">()</span>
    <span class="n">key</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="nb">sorted</span><span class="p">((</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)))</span>
    <span class="k">if</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">seen</span><span class="p">:</span>
        <span class="k">continue</span>
    <span class="n">seen</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
    <span class="n">q</span> <span class="o">=</span> <span class="s1">'</span><span class="si">{}</span><span class="s1">-</span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
    <span class="n">query</span> <span class="o">=</span> <span class="n">q</span> <span class="o">+</span> <span class="s1">' '</span> <span class="o">*</span> <span class="p">(</span><span class="n">maxlen</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">q</span><span class="p">))</span>
    <span class="n">ans</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">a</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span>
    <span class="n">ans</span> <span class="o">+=</span> <span class="s1">' '</span> <span class="o">*</span> <span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="s2">"digits"</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">ans</span><span class="p">))</span>

    <span class="n">questions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
    <span class="n">expected</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ans</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">'Total subtraction questions:'</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">questions</span><span class="p">))</span>

<span class="c1"># Vectorize the data</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Vectorization...'</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">questions</span><span class="p">),</span> <span class="n">maxlen</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">chars</span><span class="p">)),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">bool_</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">questions</span><span class="p">),</span> <span class="n">config</span><span class="p">[</span><span class="s2">"digits"</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">chars</span><span class="p">)),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">bool_</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">questions</span><span class="p">):</span>
    <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">ctable</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">sentence</span><span class="p">,</span> <span class="n">maxlen</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">expected</span><span class="p">):</span>
    <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">ctable</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">sentence</span><span class="p">,</span> <span class="n">config</span><span class="p">[</span><span class="s2">"digits"</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Shuffle the data</span>
<span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>

<span class="c1"># Split into training and validation sets</span>
<span class="n">split_at</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">//</span> <span class="mi">10</span>
<span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">x_val</span><span class="p">)</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:</span><span class="n">split_at</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="n">split_at</span><span class="p">:]</span>
<span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_val</span><span class="p">)</span> <span class="o">=</span> <span class="n">y</span><span class="p">[:</span><span class="n">split_at</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">split_at</span><span class="p">:]</span>

<span class="c1"># Build the model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">([</span>
    <span class="n">LSTM</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="s2">"hidden_size"</span><span class="p">],</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="n">maxlen</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">chars</span><span class="p">))),</span>
    <span class="n">RepeatVector</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="s2">"digits"</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span>
    <span class="n">LSTM</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="s2">"hidden_size"</span><span class="p">],</span> <span class="n">return_sequences</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
    <span class="n">TimeDistributed</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">chars</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'softmax'</span><span class="p">))</span>
<span class="p">])</span>

<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">'categorical_crossentropy'</span><span class="p">,</span>
             <span class="n">optimizer</span><span class="o">=</span><span class="s1">'adam'</span><span class="p">,</span>
             <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">'accuracy'</span><span class="p">])</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>

<span class="c1"># Training loop with updated prediction handling</span>
<span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="s2">"iterations"</span><span class="p">]):</span>
    <span class="nb">print</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'-'</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'Iteration'</span><span class="p">,</span> <span class="n">iteration</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span>
             <span class="n">batch_size</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s2">"batch_size"</span><span class="p">],</span>
             <span class="n">epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
             <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">x_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">))</span>

    <span class="c1"># Validate on 10 random samples</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="n">ind</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_val</span><span class="p">))</span>
        <span class="n">rowx</span><span class="p">,</span> <span class="n">rowy</span> <span class="o">=</span> <span class="n">x_val</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">ind</span><span class="p">])],</span> <span class="n">y_val</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">ind</span><span class="p">])]</span>

        <span class="c1"># Updated prediction handling - using argmax on the raw predictions</span>
        <span class="n">preds</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">rowx</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">pred_classes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">preds</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">q</span> <span class="o">=</span> <span class="n">ctable</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">rowx</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">correct</span> <span class="o">=</span> <span class="n">ctable</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">rowy</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">guess</span> <span class="o">=</span> <span class="s1">''</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">ctable</span><span class="o">.</span><span class="n">indices_char</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">pred_classes</span><span class="p">)</span>

        <span class="nb">print</span><span class="p">(</span><span class="s1">'Q:'</span><span class="p">,</span> <span class="n">q</span><span class="o">.</span><span class="n">strip</span><span class="p">(),</span> <span class="n">end</span><span class="o">=</span><span class="s1">' '</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">'T:'</span><span class="p">,</span> <span class="n">correct</span><span class="o">.</span><span class="n">strip</span><span class="p">(),</span> <span class="n">end</span><span class="o">=</span><span class="s1">' '</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">correct</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span> <span class="o">==</span> <span class="n">guess</span><span class="o">.</span><span class="n">strip</span><span class="p">():</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">''</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">' '</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">''</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">' '</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">guess</span><span class="o">.</span><span class="n">strip</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Generating data...
Total subtraction questions: 40000
Vectorization...
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">Model: "sequential"</span>
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">
<span style="font-weight: bold"> Layer (type)                         </span><span style="font-weight: bold"> Output Shape                </span><span style="font-weight: bold">         Param # </span>

 lstm (<span style="color: #0087ff; text-decoration-color: #0087ff">LSTM</span>)                           (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>)                           <span style="color: #00af00; text-decoration-color: #00af00">72,704</span> 

 repeat_vector (<span style="color: #0087ff; text-decoration-color: #0087ff">RepeatVector</span>)          (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">5</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>)                             <span style="color: #00af00; text-decoration-color: #00af00">0</span> 

 lstm_1 (<span style="color: #0087ff; text-decoration-color: #0087ff">LSTM</span>)                         (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">5</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>)                       <span style="color: #00af00; text-decoration-color: #00af00">131,584</span> 

 time_distributed (<span style="color: #0087ff; text-decoration-color: #0087ff">TimeDistributed</span>)    (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">5</span>, <span style="color: #00af00; text-decoration-color: #00af00">13</span>)                          <span style="color: #00af00; text-decoration-color: #00af00">1,677</span> 

</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Total params: </span><span style="color: #00af00; text-decoration-color: #00af00">205,965</span> (804.55 KB)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">205,965</span> (804.55 KB)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Non-trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">0</span> (0.00 B)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>
--------------------------------------------------
Iteration 1
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">8s</span> 9ms/step - accuracy: 0.2862 - loss: 2.1107 - val_accuracy: 0.3880 - val_loss: 1.6632
Q: 3370-6 T: 3364  669
Q: 5-765 T: -760  -55
Q: 937-33 T: 904  25
Q: 18-51 T: -33  -1
Q: 1171-985 T: 186  -76
Q: 8226-8 T: 8218  6699
Q: 2277-194 T: 2083  176
Q: 923-328 T: 595  -23
Q: 3050-2 T: 3048  669
Q: 667-22 T: 645  15

--------------------------------------------------
Iteration 2
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 11ms/step - accuracy: 0.4010 - loss: 1.6302 - val_accuracy: 0.4196 - val_loss: 1.5818
Q: 351-0 T: 351  311
Q: 7378-3999 T: 3379  -771
Q: 305-7943 T: -7638  -3911
Q: 6415-2493 T: 3922  -111
Q: 612-1112 T: -500  -111
Q: 77-3424 T: -3347  -2216
Q: 824-6 T: 818  877
Q: 388-1 T: 387  887
Q: 772-48 T: 724  777
Q: 4-510 T: -506  -41

--------------------------------------------------
Iteration 3
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.4308 - loss: 1.5495 - val_accuracy: 0.4438 - val_loss: 1.4984
Q: 443-6 T: 437  440
Q: 557-918 T: -361  -50
Q: 4898-37 T: 4861  8877
Q: 6567-189 T: 6378  6602
Q: 1677-251 T: 1426  111
Q: 929-57 T: 872  999
Q: 932-9925 T: -8993  -9905
Q: 255-8 T: 247  552
Q: 800-350 T: 450  110
Q: 1780-5 T: 1775  777

--------------------------------------------------
Iteration 4
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.4520 - loss: 1.4719 - val_accuracy: 0.4639 - val_loss: 1.4379
Q: 538-7604 T: -7066  -7477
Q: 9485-33 T: 9452  9988
Q: 5051-44 T: 5007  5443
Q: 2-23 T: -21  -28
Q: 346-1833 T: -1487  -3077
Q: 84-615 T: -531  -443
Q: 9036-4 T: 9032  9999
Q: 36-47 T: -11  -44
Q: 44-968 T: -924  -888
Q: 97-7214 T: -7117  -7277

--------------------------------------------------
Iteration 5
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 7ms/step - accuracy: 0.4783 - loss: 1.3933 - val_accuracy: 0.4963 - val_loss: 1.3359
Q: 8222-8546 T: -324  221
Q: 95-40 T: 55  44
Q: 4081-2363 T: 1718  310
Q: 362-947 T: -585  -323
Q: 77-6 T: 71  63
Q: 719-46 T: 673  733
Q: 98-91 T: 7  8
Q: 7008-9954 T: -2946  -133
Q: 73-2 T: 71  77
Q: 3760-530 T: 3230  3113

--------------------------------------------------
Iteration 6
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 7ms/step - accuracy: 0.5130 - loss: 1.3100 - val_accuracy: 0.5304 - val_loss: 1.2552
Q: 5883-89 T: 5794  8886
Q: 91-1302 T: -1211  -1044
Q: 1886-255 T: 1631  1666
Q: 4-9355 T: -9351  -9433
Q: 4-5460 T: -5456  -4400
Q: 5668-55 T: 5613  5556
Q: 8356-3 T: 8353  8226
Q: 3-3059 T: -3056  -3003
Q: 885-4 T: 881  886
Q: 481-51 T: 430  476

--------------------------------------------------
Iteration 7
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.5386 - loss: 1.2424 - val_accuracy: 0.5485 - val_loss: 1.2139
Q: 617-924 T: -307  -584
Q: 5546-1 T: 5545  5518
Q: 1226-32 T: 1194  1188
Q: 5030-89 T: 4941  5088
Q: 4-4176 T: -4172  -4110
Q: 505-59 T: 446  408
Q: 37-98 T: -61  -55
Q: 7-5874 T: -5867  -5750
Q: 647-46 T: 601  638
Q: 4274-326 T: 3948  3888

--------------------------------------------------
Iteration 8
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 7ms/step - accuracy: 0.5615 - loss: 1.1816 - val_accuracy: 0.5705 - val_loss: 1.1558
Q: 373-3 T: 370  370
Q: 590-9 T: 581  595
Q: 48-2703 T: -2655  -2790
Q: 4-399 T: -395  -397
Q: 7-543 T: -536  -545
Q: 7454-83 T: 7371  7411
Q: 7465-1 T: 7464  7470
Q: 84-766 T: -682  -650
Q: 6180-8 T: 6172  6000
Q: 9854-749 T: 9105  8033

--------------------------------------------------
Iteration 9
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 7ms/step - accuracy: 0.5787 - loss: 1.1365 - val_accuracy: 0.5889 - val_loss: 1.1059
Q: 36-684 T: -648  -635
Q: 8990-656 T: 8334  8043
Q: 2440-896 T: 1544  2455
Q: 37-109 T: -72  -10
Q: 5659-0 T: 5659  5685
Q: 356-6 T: 350  352
Q: 5099-77 T: 5022  5043
Q: 796-87 T: 709  704
Q: 558-933 T: -375  -374
Q: 44-992 T: -948  -944

--------------------------------------------------
Iteration 10
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.5927 - loss: 1.0957 - val_accuracy: 0.5929 - val_loss: 1.0827
Q: 6-976 T: -970  -962
Q: 517-7 T: 510  516
Q: 91-44 T: 47  41
Q: 61-1617 T: -1556  -1501
Q: 45-28 T: 17  2
Q: 315-569 T: -254  -222
Q: 35-36 T: -1  -
Q: 7-3687 T: -3680  -3676
Q: 40-924 T: -884  -894
Q: 63-808 T: -745  -782

--------------------------------------------------
Iteration 11
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 10ms/step - accuracy: 0.6061 - loss: 1.0597 - val_accuracy: 0.6062 - val_loss: 1.0603
Q: 912-3 T: 909  906
Q: 5-9785 T: -9780  -9768
Q: 7826-5 T: 7821  7828
Q: 69-9001 T: -8932  -8994
Q: 448-94 T: 354  365
Q: 3-7610 T: -7607  -7608
Q: 40-326 T: -286  -296
Q: 1725-42 T: 1683  1698
Q: 5567-148 T: 5419  5466
Q: 820-158 T: 662  666

--------------------------------------------------
Iteration 12
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.6172 - loss: 1.0328 - val_accuracy: 0.6042 - val_loss: 1.0482
Q: 9459-15 T: 9444  9451
Q: 170-907 T: -737  -690
Q: 6716-766 T: 5950  6099
Q: 931-66 T: 865  898
Q: 8-4375 T: -4367  -4365
Q: 34-7215 T: -7181  -7199
Q: 328-2263 T: -1935  -1954
Q: 1873-7866 T: -5993  -6990
Q: 32-52 T: -20  -19
Q: 6116-7 T: 6109  6150

--------------------------------------------------
Iteration 13
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.6259 - loss: 1.0030 - val_accuracy: 0.6259 - val_loss: 0.9996
Q: 9534-74 T: 9460  9460
Q: 1906-1849 T: 57  102
Q: 4-238 T: -234  -233
Q: 6-3204 T: -3198  -3204
Q: 755-720 T: 35  -5
Q: 921-476 T: 445  362
Q: 309-880 T: -571  -580
Q: 7455-74 T: 7381  7388
Q: 6298-6 T: 6292  6283
Q: 732-1125 T: -393  -45

--------------------------------------------------
Iteration 14
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 13ms/step - accuracy: 0.6369 - loss: 0.9770 - val_accuracy: 0.6247 - val_loss: 0.9939
Q: 99-762 T: -663  -634
Q: 5-6088 T: -6083  -6078
Q: 812-98 T: 714  735
Q: 2630-2 T: 2628  2626
Q: 719-0 T: 719  706
Q: 42-8725 T: -8683  -8678
Q: 6-683 T: -677  -674
Q: 321-3798 T: -3477  -3544
Q: 4-5045 T: -5041  -504
Q: 1307-855 T: 452  732

--------------------------------------------------
Iteration 15
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 11ms/step - accuracy: 0.6475 - loss: 0.9519 - val_accuracy: 0.6315 - val_loss: 0.9742
Q: 1-7442 T: -7441  -7440
Q: 952-33 T: 919  911
Q: 52-698 T: -646  -631
Q: 96-68 T: 28  11
Q: 8003-7 T: 7996  8001
Q: 3537-599 T: 2938  3161
Q: 17-102 T: -85  -92
Q: 43-1894 T: -1851  -1852
Q: 1-8667 T: -8666  -8665
Q: 9-833 T: -824  -828

--------------------------------------------------
Iteration 16
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 7ms/step - accuracy: 0.6547 - loss: 0.9298 - val_accuracy: 0.6517 - val_loss: 0.9282
Q: 52-4120 T: -4068  -4079
Q: 3962-74 T: 3888  3813
Q: 98-474 T: -376  -363
Q: 0-5245 T: -5245  -5241
Q: 372-23 T: 349  353
Q: 5108-6 T: 5102  5006
Q: 887-7 T: 880  870
Q: 270-9071 T: -8801  -8911
Q: 4-723 T: -719  -729
Q: 4060-7029 T: -2969  -2110

--------------------------------------------------
Iteration 17
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 13ms/step - accuracy: 0.6641 - loss: 0.9029 - val_accuracy: 0.6557 - val_loss: 0.9150
Q: 3446-144 T: 3302  3300
Q: 2588-1158 T: 1430  1377
Q: 131-3586 T: -3455  -3444
Q: 143-204 T: -61  -91
Q: 611-90 T: 521  510
Q: 113-67 T: 46  64
Q: 628-9 T: 619  614
Q: 9116-6 T: 9110  9163
Q: 12-442 T: -430  -429
Q: 9044-2118 T: 6926  7811

--------------------------------------------------
Iteration 18
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 15ms/step - accuracy: 0.6692 - loss: 0.8889 - val_accuracy: 0.6601 - val_loss: 0.8968
Q: 7465-1 T: 7464  7453
Q: 998-127 T: 871  966
Q: 3-7610 T: -7607  -7609
Q: 5051-44 T: 5007  5099
Q: 205-734 T: -529  -566
Q: 57-2565 T: -2508  -2500
Q: 841-3 T: 838  839
Q: 4558-670 T: 3888  3879
Q: 5659-0 T: 5659  5656
Q: 255-8 T: 247  248

--------------------------------------------------
Iteration 19
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">6s</span> 20ms/step - accuracy: 0.6776 - loss: 0.8642 - val_accuracy: 0.6598 - val_loss: 0.8907
Q: 886-9 T: 877  876
Q: 6724-70 T: 6654  6666
Q: 56-91 T: -35  -35
Q: 18-378 T: -360  -369
Q: 53-712 T: -659  -660
Q: 11-29 T: -18  -17
Q: 9255-874 T: 8381  8860
Q: 0-696 T: -696  -695
Q: 8-974 T: -966  -966
Q: 7976-293 T: 7683  7586

--------------------------------------------------
Iteration 20
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.6866 - loss: 0.8399 - val_accuracy: 0.6715 - val_loss: 0.8731
Q: 4-8061 T: -8057  -8055
Q: 6-9733 T: -9727  -9733
Q: 1028-24 T: 1004  1014
Q: 8658-3777 T: 4881  5099
Q: 98-91 T: 7  -
Q: 5-2296 T: -2291  -2292
Q: 3241-668 T: 2573  2655
Q: 2948-8332 T: -5384  -5955
Q: 796-607 T: 189  135
Q: 2075-8 T: 2067  2077

--------------------------------------------------
Iteration 21
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.6917 - loss: 0.8249 - val_accuracy: 0.6703 - val_loss: 0.8593
Q: 638-6 T: 632  635
Q: 326-8 T: 318  327
Q: 6-50 T: -44  -44
Q: 9080-808 T: 8272  8900
Q: 22-2937 T: -2915  -2943
Q: 163-9 T: 154  157
Q: 1906-1849 T: 57  125
Q: 472-137 T: 335  433
Q: 854-9 T: 845  856
Q: 9-5771 T: -5762  -5768

--------------------------------------------------
Iteration 22
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 10ms/step - accuracy: 0.6987 - loss: 0.8031 - val_accuracy: 0.6779 - val_loss: 0.8430
Q: 7-1815 T: -1808  -1856
Q: 5874-6 T: 5868  5840
Q: 8371-4 T: 8367  8366
Q: 415-167 T: 248  262
Q: 609-539 T: 70  12
Q: 2-7418 T: -7416  -7418
Q: 559-5669 T: -5110  -5000
Q: 9162-24 T: 9138  9119
Q: 3343-225 T: 3118  3100
Q: 31-415 T: -384  -389

--------------------------------------------------
Iteration 23
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.7062 - loss: 0.7860 - val_accuracy: 0.6896 - val_loss: 0.8152
Q: 862-5 T: 857  856
Q: 184-442 T: -258  -366
Q: 824-502 T: 322  369
Q: 221-9185 T: -8964  -8055
Q: 22-459 T: -437  -444
Q: 6963-4763 T: 2200  2099
Q: 88-186 T: -98  -10
Q: 5051-44 T: 5007  5099
Q: 4489-243 T: 4246  4244
Q: 5962-3500 T: 2462  2464

--------------------------------------------------
Iteration 24
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.7159 - loss: 0.7594 - val_accuracy: 0.6896 - val_loss: 0.7987
Q: 82-31 T: 51  50
Q: 647-46 T: 601  611
Q: 2546-1533 T: 1013  127
Q: 30-513 T: -483  -470
Q: 741-445 T: 296  372
Q: 548-884 T: -336  -336
Q: 8650-984 T: 7666  7755
Q: 5-102 T: -97  -90
Q: 3-51 T: -48  -47
Q: 4898-37 T: 4861  4852

--------------------------------------------------
Iteration 25
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - accuracy: 0.7202 - loss: 0.7446 - val_accuracy: 0.7022 - val_loss: 0.7755
Q: 5001-276 T: 4725  4544
Q: 2656-4161 T: -1505  -154
Q: 788-342 T: 446  445
Q: 27-919 T: -892  -885
Q: 26-245 T: -219  -218
Q: 94-31 T: 63  63
Q: 51-56 T: -5  -4
Q: 9-7928 T: -7919  -7910
Q: 1237-726 T: 511  665
Q: 5071-837 T: 4234  4235

--------------------------------------------------
Iteration 26
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.7287 - loss: 0.7228 - val_accuracy: 0.7013 - val_loss: 0.7782
Q: 336-0 T: 336  336
Q: 71-1602 T: -1531  -1551
Q: 9-6891 T: -6882  -6889
Q: 51-70 T: -19  -10
Q: 2760-6 T: 2754  2753
Q: 369-9121 T: -8752  -8777
Q: 115-7 T: 108  108
Q: 908-18 T: 890  898
Q: 1-3685 T: -3684  -3674
Q: 151-5 T: 146  147

--------------------------------------------------
Iteration 27
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.7353 - loss: 0.7069 - val_accuracy: 0.7103 - val_loss: 0.7493
Q: 455-2603 T: -2148  -2364
Q: 188-218 T: -30  -88
Q: 8-6034 T: -6026  -6026
Q: 5449-265 T: 5184  5174
Q: 5-1104 T: -1099  -1011
Q: 5883-89 T: 5794  5795
Q: 8233-948 T: 7285  7455
Q: 2-933 T: -931  -931
Q: 78-1389 T: -1311  -1310
Q: 3251-4078 T: -827  -1444

--------------------------------------------------
Iteration 28
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 7ms/step - accuracy: 0.7400 - loss: 0.6893 - val_accuracy: 0.7130 - val_loss: 0.7375
Q: 3704-0 T: 3704  3703
Q: 74-241 T: -167  -155
Q: 0-7158 T: -7158  -7177
Q: 9744-5 T: 9739  9740
Q: 606-84 T: 522  522
Q: 693-3247 T: -2554  -2653
Q: 9901-7 T: 9894  9894
Q: 4740-5628 T: -888  -1898
Q: 1-7442 T: -7441  -7440
Q: 5785-4908 T: 877  -10

--------------------------------------------------
Iteration 29
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 10ms/step - accuracy: 0.7497 - loss: 0.6662 - val_accuracy: 0.7262 - val_loss: 0.7023
Q: 8-81 T: -73  -73
Q: 570-474 T: 96  124
Q: 74-7261 T: -7187  -7104
Q: 6467-467 T: 6000  5000
Q: 5-903 T: -898  -898
Q: 64-1356 T: -1292  -1280
Q: 8630-192 T: 8438  8432
Q: 75-611 T: -536  -544
Q: 4-4176 T: -4172  -4172
Q: 717-242 T: 475  477

--------------------------------------------------
Iteration 30
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 10ms/step - accuracy: 0.7563 - loss: 0.6455 - val_accuracy: 0.7277 - val_loss: 0.6921
Q: 330-56 T: 274  265
Q: 45-250 T: -205  -206
Q: 5768-607 T: 5161  5999
Q: 30-88 T: -58  -68
Q: 2911-6 T: 2905  2916
Q: 7089-5 T: 7084  7074
Q: 60-265 T: -205  -215
Q: 1767-62 T: 1705  1613
Q: 7868-77 T: 7791  7899
Q: 186-4149 T: -3963  -4957

--------------------------------------------------
Iteration 31
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 9ms/step - accuracy: 0.7662 - loss: 0.6223 - val_accuracy: 0.7394 - val_loss: 0.6752
Q: 92-97 T: -5  -14
Q: 5668-55 T: 5613  5613
Q: 562-887 T: -325  -336
Q: 82-567 T: -485  -484
Q: 42-93 T: -51  -40
Q: 73-389 T: -316  -325
Q: 68-204 T: -136  -144
Q: 3977-6 T: 3971  3972
Q: 169-64 T: 105  12
Q: 606-71 T: 535  545

--------------------------------------------------
Iteration 32
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.7718 - loss: 0.6069 - val_accuracy: 0.7417 - val_loss: 0.6613
Q: 686-79 T: 607  697
Q: 906-3361 T: -2455  -2664
Q: 41-805 T: -764  -765
Q: 984-843 T: 141  110
Q: 3-3059 T: -3056  -3055
Q: 822-7181 T: -6359  -6278
Q: 885-4 T: 881  881
Q: 279-843 T: -564  -565
Q: 34-5 T: 29  28
Q: 0-8444 T: -8444  -8444

--------------------------------------------------
Iteration 33
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 10ms/step - accuracy: 0.7808 - loss: 0.5859 - val_accuracy: 0.7484 - val_loss: 0.6440
Q: 15-51 T: -36  -36
Q: 824-502 T: 322  321
Q: 34-4015 T: -3981  -3980
Q: 2-487 T: -485  -485
Q: 252-6 T: 246  247
Q: 5172-20 T: 5152  5263
Q: 6-5695 T: -5689  -5689
Q: 194-896 T: -702  -700
Q: 81-8664 T: -8583  -8694
Q: 9-9255 T: -9246  -9247

--------------------------------------------------
Iteration 34
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.7883 - loss: 0.5686 - val_accuracy: 0.7614 - val_loss: 0.6179
Q: 2-6661 T: -6659  -6659
Q: 8578-60 T: 8518  8491
Q: 6487-16 T: 6471  6470
Q: 1-122 T: -121  -122
Q: 8401-7 T: 8394  8393
Q: 5-1104 T: -1099  -1011
Q: 4798-399 T: 4399  4400
Q: 65-72 T: -7  -8
Q: 3690-4 T: 3686  3685
Q: 840-4633 T: -3793  -3729

--------------------------------------------------
Iteration 35
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 11ms/step - accuracy: 0.7979 - loss: 0.5460 - val_accuracy: 0.7686 - val_loss: 0.6053
Q: 9-48 T: -39  -49
Q: 45-1289 T: -1244  -1243
Q: 13-1223 T: -1210  -1209
Q: 87-248 T: -161  -161
Q: 631-54 T: 577  577
Q: 8162-33 T: 8129  8110
Q: 5091-6 T: 5085  5065
Q: 27-916 T: -889  -888
Q: 115-7 T: 108  108
Q: 3029-1 T: 3028  3008

--------------------------------------------------
Iteration 36
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.8040 - loss: 0.5313 - val_accuracy: 0.7650 - val_loss: 0.6036
Q: 6708-67 T: 6641  6642
Q: 7738-32 T: 7706  7616
Q: 128-528 T: -400  -490
Q: 53-16 T: 37  37
Q: 76-24 T: 52  52
Q: 2299-147 T: 2152  2208
Q: 588-85 T: 503  403
Q: 8-2609 T: -2601  -2691
Q: 473-562 T: -89  -18
Q: 6-50 T: -44  -44

--------------------------------------------------
Iteration 37
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.8142 - loss: 0.5091 - val_accuracy: 0.7784 - val_loss: 0.5747
Q: 68-6558 T: -6490  -6490
Q: 953-7 T: 946  946
Q: 219-2028 T: -1809  -1900
Q: 7581-7407 T: 174  152
Q: 0-414 T: -414  -414
Q: 4-2466 T: -2462  -2562
Q: 95-94 T: 1  0
Q: 8866-0 T: 8866  8864
Q: 866-300 T: 566  577
Q: 18-3925 T: -3907  -3917

--------------------------------------------------
Iteration 38
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.8174 - loss: 0.4967 - val_accuracy: 0.7819 - val_loss: 0.5655
Q: 88-73 T: 15  25
Q: 160-553 T: -393  -333
Q: 330-56 T: 274  264
Q: 635-1 T: 634  634
Q: 49-821 T: -772  -772
Q: 17-48 T: -31  -30
Q: 986-77 T: 909  919
Q: 16-179 T: -163  -163
Q: 820-158 T: 662  643
Q: 63-57 T: 6  4

--------------------------------------------------
Iteration 39
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 10ms/step - accuracy: 0.8266 - loss: 0.4794 - val_accuracy: 0.7957 - val_loss: 0.5396
Q: 2002-276 T: 1726  1656
Q: 49-5091 T: -5042  -5051
Q: 477-9534 T: -9057  -9066
Q: 406-9514 T: -9108  -9033
Q: 9-2752 T: -2743  -2733
Q: 487-0 T: 487  487
Q: 314-759 T: -445  -465
Q: 2793-7076 T: -4283  -4354
Q: 725-18 T: 707  707
Q: 5001-276 T: 4725  4534

--------------------------------------------------
Iteration 40
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.8331 - loss: 0.4634 - val_accuracy: 0.7962 - val_loss: 0.5324
Q: 5145-112 T: 5033  4023
Q: 5512-739 T: 4773  4733
Q: 7669-5 T: 7664  7664
Q: 241-50 T: 191  101
Q: 9717-5415 T: 4302  4653
Q: 792-406 T: 386  366
Q: 288-4734 T: -4446  -4345
Q: 85-139 T: -54  -54
Q: 1171-985 T: 186  975
Q: 9-117 T: -108  -108

--------------------------------------------------
Iteration 41
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.8408 - loss: 0.4440 - val_accuracy: 0.8040 - val_loss: 0.5157
Q: 5014-369 T: 4645  4545
Q: 2440-896 T: 1544  1454
Q: 382-835 T: -453  -453
Q: 444-55 T: 389  389
Q: 9-72 T: -63  -63
Q: 99-4942 T: -4843  -4893
Q: 35-267 T: -232  -232
Q: 6-3533 T: -3527  -3527
Q: 9752-555 T: 9197  9372
Q: 44-11 T: 33  33

--------------------------------------------------
Iteration 42
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 11ms/step - accuracy: 0.8472 - loss: 0.4287 - val_accuracy: 0.8104 - val_loss: 0.4997
Q: 24-915 T: -891  -891
Q: 517-7 T: 510  500
Q: 0-408 T: -408  -408
Q: 738-93 T: 645  645
Q: 8016-5225 T: 2791  3819
Q: 7-5672 T: -5665  -5665
Q: 611-0 T: 611  610
Q: 5-3476 T: -3471  -3471
Q: 10-94 T: -84  -85
Q: 1334-2772 T: -1438  -158

--------------------------------------------------
Iteration 43
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 10ms/step - accuracy: 0.8570 - loss: 0.4070 - val_accuracy: 0.8144 - val_loss: 0.4861
Q: 978-655 T: 323  323
Q: 358-71 T: 287  287
Q: 4033-2 T: 4031  4031
Q: 5659-0 T: 5659  5659
Q: 5276-4 T: 5272  5272
Q: 161-89 T: 72  72
Q: 7283-9322 T: -2039  -1499
Q: 3343-225 T: 3118  3202
Q: 152-523 T: -371  -381
Q: 195-8 T: 187  187

--------------------------------------------------
Iteration 44
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.8595 - loss: 0.4002 - val_accuracy: 0.8242 - val_loss: 0.4690
Q: 3208-0 T: 3208  3208
Q: 7939-38 T: 7901  7910
Q: 8878-5 T: 8873  8853
Q: 633-46 T: 587  587
Q: 3-7368 T: -7365  -7365
Q: 88-40 T: 48  48
Q: 8-349 T: -341  -341
Q: 7-5132 T: -5125  -5124
Q: 322-4182 T: -3860  -3880
Q: 1-1701 T: -1700  -1700

--------------------------------------------------
Iteration 45
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 14ms/step - accuracy: 0.8682 - loss: 0.3775 - val_accuracy: 0.8266 - val_loss: 0.4604
Q: 706-8685 T: -7979  -7090
Q: 3698-41 T: 3657  3657
Q: 20-3270 T: -3250  -3250
Q: 8-974 T: -966  -966
Q: 0-5260 T: -5260  -5261
Q: 1207-7 T: 1200  1190
Q: 88-63 T: 25  24
Q: 96-810 T: -714  -715
Q: 63-66 T: -3  -3
Q: 5962-3500 T: 2462  2491

--------------------------------------------------
Iteration 46
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.8718 - loss: 0.3686 - val_accuracy: 0.8282 - val_loss: 0.4580
Q: 7-295 T: -288  -288
Q: 4614-95 T: 4519  4529
Q: 32-5554 T: -5522  -5523
Q: 9-1046 T: -1037  -1037
Q: 77-5213 T: -5136  -5146
Q: 111-95 T: 16  1
Q: 124-6 T: 118  118
Q: 951-66 T: 885  885
Q: 0-6626 T: -6626  -6626
Q: 760-984 T: -224  -234

--------------------------------------------------
Iteration 47
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 7ms/step - accuracy: 0.8772 - loss: 0.3540 - val_accuracy: 0.8349 - val_loss: 0.4421
Q: 1238-931 T: 307  462
Q: 5833-57 T: 5776  5766
Q: 6219-471 T: 5748  5821
Q: 904-9 T: 895  894
Q: 79-693 T: -614  -614
Q: 5690-4190 T: 1500  1490
Q: 1851-35 T: 1816  1856
Q: 8376-5971 T: 2405  3654
Q: 35-907 T: -872  -862
Q: 450-60 T: 390  390

--------------------------------------------------
Iteration 48
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - accuracy: 0.8795 - loss: 0.3457 - val_accuracy: 0.8336 - val_loss: 0.4450
Q: 4863-2 T: 4861  4861
Q: 1137-8 T: 1129  1129
Q: 2-3272 T: -3270  -3270
Q: 8830-737 T: 8093  8023
Q: 285-56 T: 229  229
Q: 2681-4496 T: -1815  -1826
Q: 5898-9 T: 5889  5889
Q: 7743-19 T: 7724  7724
Q: 0-8444 T: -8444  -8445
Q: 7-599 T: -592  -592

--------------------------------------------------
Iteration 49
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.8876 - loss: 0.3308 - val_accuracy: 0.8409 - val_loss: 0.4264
Q: 5-313 T: -308  -308
Q: 254-55 T: 199  190
Q: 221-5 T: 216  216
Q: 85-139 T: -54  -54
Q: 501-9 T: 492  492
Q: 2-23 T: -21  -21
Q: 4247-81 T: 4166  4176
Q: 3379-8473 T: -5094  -5044
Q: 3-8272 T: -8269  -8270
Q: 642-50 T: 592  592

--------------------------------------------------
Iteration 50
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.8892 - loss: 0.3185 - val_accuracy: 0.8473 - val_loss: 0.4105
Q: 290-953 T: -663  -663
Q: 725-575 T: 150  189
Q: 4779-8 T: 4771  4760
Q: 8703-42 T: 8661  8671
Q: 1078-8 T: 1070  1060
Q: 3519-5 T: 3514  3514
Q: 20-8928 T: -8908  -8908
Q: 0-32 T: -32  -32
Q: 523-10 T: 513  513
Q: 989-41 T: 948  948
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>1.2).</p>
<p>a) Do you think this model performs well?  Why or why not?<br/>
b) What are its limitations?<br/>
c) What would you do to improve it?<br/>
d) Can you apply an attention mechanism to this model? Why or why not?</p>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="1.2)-my-answer:">1.2) my answer:<a class="anchor-link" href="#1.2)-my-answer:"></a></h2><p>a) The model demonstrates moderate but not exceptional performance in handling arithmetic subtraction tasks. With a validation accuracy of around 85%, it successfully handles basic subtractions and some complex cases, but struggles with situations requiring precise calculations, especially those involving carrying operations or close number differences. While it can reliably handle simple cases and negative numbers, its accuracy drops when dealing with more nuanced calculations, particularly those involving large numbers with small differences or problems requiring multiple carrying operations. This level of performance suggests that while the model has learned the basic patterns of subtraction, it hasn't fully mastered the more intricate aspects of arithmetic operations that would be necessary for real-world applications.</p>
<p>b)</p>
<p>Fixed Input Length: The model is constrained to handle only numbers up to a certain digit length (4 digits in this case)
Single Operation: It's specialized for subtraction only and can't handle other arithmetic operations
No Explicit Carrying Mechanism: The LSTM has to learn carrying/borrowing implicitly
Limited Generalization: The validation accuracy suggests it may not generalize perfectly to unseen number combinations</p>
<p>c)</p>
<p>The architecture could be strengthened by adding residual connections and attention mechanisms, helping the model better handle complex calculations. The training process could be improved through curriculum learning, starting with simple problems and gradually increasing difficulty, while also expanding the training data to include more challenging edge cases like numbers requiring multiple carries or those with similar digits.</p>
<p>d)</p>
<p>Yes, an attention mechanism can be applied to this model, and it would be particularly beneficial for arithmetic operations.</p>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>1.3).</p>
<p>Add attention to the model. Evaluate the performance against the <code>seq2seq</code> you trained above. Which one is performing better?</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[9]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">attention_model</span><span class="p">(</span><span class="n">maxlen</span><span class="p">,</span> <span class="n">n_chars</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>

    <span class="n">input_layer</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">maxlen</span><span class="p">,</span> <span class="n">n_chars</span><span class="p">))</span>

    <span class="c1"># Encoder</span>
    <span class="n">encoder</span> <span class="o">=</span> <span class="n">LSTM</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="s2">"hidden_size"</span><span class="p">],</span> <span class="n">return_sequences</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_state</span><span class="o">=</span><span class="kc">True</span><span class="p">)(</span><span class="n">input_layer</span><span class="p">)</span>
    <span class="n">encoder_outputs</span><span class="p">,</span> <span class="n">state_h</span><span class="p">,</span> <span class="n">state_c</span> <span class="o">=</span> <span class="n">encoder</span>

    <span class="c1"># Decoder</span>
    <span class="n">decoder</span> <span class="o">=</span> <span class="n">RepeatVector</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="s2">"digits"</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)(</span><span class="n">state_h</span><span class="p">)</span>
    <span class="n">decoder</span> <span class="o">=</span> <span class="n">LSTM</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="s2">"hidden_size"</span><span class="p">],</span> <span class="n">return_sequences</span><span class="o">=</span><span class="kc">True</span><span class="p">)(</span><span class="n">decoder</span><span class="p">)</span>

    <span class="n">attention</span> <span class="o">=</span> <span class="n">Attention</span><span class="p">()([</span><span class="n">decoder</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">])</span>

    <span class="n">decoder_combined</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Concatenate</span><span class="p">()([</span><span class="n">decoder</span><span class="p">,</span> <span class="n">attention</span><span class="p">])</span>

    <span class="n">output</span> <span class="o">=</span> <span class="n">TimeDistributed</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">n_chars</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'softmax'</span><span class="p">))(</span><span class="n">decoder_combined</span><span class="p">)</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">input_layer</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">output</span><span class="p">)</span>

    <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">'categorical_crossentropy'</span><span class="p">,</span>
                 <span class="n">optimizer</span><span class="o">=</span><span class="s1">'adam'</span><span class="p">,</span>
                 <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">'accuracy'</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">attention_model</span><span class="p">(</span>
    <span class="n">maxlen</span><span class="o">=</span><span class="n">maxlen</span><span class="p">,</span>
    <span class="n">n_chars</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">chars</span><span class="p">),</span>
    <span class="n">config</span><span class="o">=</span><span class="n">config</span>
<span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>

<span class="c1"># Training loop with updated prediction handling</span>
<span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="s2">"iterations"</span><span class="p">]):</span>
    <span class="nb">print</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'-'</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'Iteration'</span><span class="p">,</span> <span class="n">iteration</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span>
             <span class="n">batch_size</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s2">"batch_size"</span><span class="p">],</span>
             <span class="n">epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
             <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">x_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">))</span>

    <span class="c1"># Validate on 10 random samples</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="n">ind</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_val</span><span class="p">))</span>
        <span class="n">rowx</span><span class="p">,</span> <span class="n">rowy</span> <span class="o">=</span> <span class="n">x_val</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">ind</span><span class="p">])],</span> <span class="n">y_val</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">ind</span><span class="p">])]</span>

        <span class="c1"># Updated prediction handling - using argmax on the raw predictions</span>
        <span class="n">preds</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">rowx</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">pred_classes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">preds</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">q</span> <span class="o">=</span> <span class="n">ctable</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">rowx</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">correct</span> <span class="o">=</span> <span class="n">ctable</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">rowy</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">guess</span> <span class="o">=</span> <span class="s1">''</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">ctable</span><span class="o">.</span><span class="n">indices_char</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">pred_classes</span><span class="p">)</span>

        <span class="nb">print</span><span class="p">(</span><span class="s1">'Q:'</span><span class="p">,</span> <span class="n">q</span><span class="o">.</span><span class="n">strip</span><span class="p">(),</span> <span class="n">end</span><span class="o">=</span><span class="s1">' '</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">'T:'</span><span class="p">,</span> <span class="n">correct</span><span class="o">.</span><span class="n">strip</span><span class="p">(),</span> <span class="n">end</span><span class="o">=</span><span class="s1">' '</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">correct</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span> <span class="o">==</span> <span class="n">guess</span><span class="o">.</span><span class="n">strip</span><span class="p">():</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">''</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">' '</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">''</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">' '</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">guess</span><span class="o">.</span><span class="n">strip</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">Model: "functional_1"</span>
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">
<span style="font-weight: bold"> Layer (type)              </span><span style="font-weight: bold"> Output Shape           </span><span style="font-weight: bold">        Param # </span><span style="font-weight: bold"> Connected to           </span>

 input_layer_3              (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">9</span>, <span style="color: #00af00; text-decoration-color: #00af00">13</span>)                        <span style="color: #00af00; text-decoration-color: #00af00">0</span>  -                      
 (<span style="color: #0087ff; text-decoration-color: #0087ff">InputLayer</span>)                                                                              

 lstm_4 (<span style="color: #0087ff; text-decoration-color: #0087ff">LSTM</span>)              [(<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">9</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>),                <span style="color: #00af00; text-decoration-color: #00af00">72,704</span>  input_layer_3[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]    
                            (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>), (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>,                                            
                            <span style="color: #00af00; text-decoration-color: #00af00">128</span>)]                                                          

 repeat_vector_2            (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">5</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>)                       <span style="color: #00af00; text-decoration-color: #00af00">0</span>  lstm_4[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">1</span>]           
 (<span style="color: #0087ff; text-decoration-color: #0087ff">RepeatVector</span>)                                                                            

 lstm_5 (<span style="color: #0087ff; text-decoration-color: #0087ff">LSTM</span>)              (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">5</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>)                 <span style="color: #00af00; text-decoration-color: #00af00">131,584</span>  repeat_vector_2[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]  

 attention_1 (<span style="color: #0087ff; text-decoration-color: #0087ff">Attention</span>)    (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">5</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>)                       <span style="color: #00af00; text-decoration-color: #00af00">0</span>  lstm_5[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>],          
                                                                    lstm_4[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]           

 concatenate (<span style="color: #0087ff; text-decoration-color: #0087ff">Concatenate</span>)  (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">5</span>, <span style="color: #00af00; text-decoration-color: #00af00">256</span>)                       <span style="color: #00af00; text-decoration-color: #00af00">0</span>  lstm_5[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>],          
                                                                    attention_1[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]      

 time_distributed_1         (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">5</span>, <span style="color: #00af00; text-decoration-color: #00af00">13</span>)                    <span style="color: #00af00; text-decoration-color: #00af00">3,341</span>  concatenate[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]      
 (<span style="color: #0087ff; text-decoration-color: #0087ff">TimeDistributed</span>)                                                                         

</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Total params: </span><span style="color: #00af00; text-decoration-color: #00af00">207,629</span> (811.05 KB)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">207,629</span> (811.05 KB)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Non-trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">0</span> (0.00 B)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>
--------------------------------------------------
Iteration 1
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">6s</span> 11ms/step - accuracy: 0.3007 - loss: 2.0927 - val_accuracy: 0.4025 - val_loss: 1.7654
Q: 568-26 T: 542  566
Q: 6978-50 T: 6928  666
Q: 5160-322 T: 4838  250
Q: 2-3873 T: -3871  -2222
Q: 4523-9 T: 4514  540
Q: 186-4149 T: -3963  -100
Q: 2-5897 T: -5895  -2222
Q: 3760-530 T: 3230  266
Q: 557-33 T: 524  550
Q: 4-8061 T: -8057  -1620

--------------------------------------------------
Iteration 2
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 11ms/step - accuracy: 0.4218 - loss: 1.6356 - val_accuracy: 0.4406 - val_loss: 1.5686
Q: 93-46 T: 47  33
Q: 6724-70 T: 6654  6666
Q: 8878-5 T: 8873  8888
Q: 83-2584 T: -2501  -2208
Q: 48-7204 T: -7156  -4708
Q: 2411-97 T: 2314  1112
Q: 794-3326 T: -2532  -309
Q: 8692-67 T: 8625  6888
Q: 805-8504 T: -7699  -500
Q: 43-44 T: -1  33

--------------------------------------------------
Iteration 3
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.4463 - loss: 1.5381 - val_accuracy: 0.4561 - val_loss: 1.4884
Q: 153-6179 T: -6026  -6000
Q: 94-55 T: 39  44
Q: 28-1997 T: -1969  -8802
Q: 64-103 T: -39  -30
Q: 65-438 T: -373  -358
Q: 63-632 T: -569  -208
Q: 212-74 T: 138  21
Q: 46-7461 T: -7415  -6644
Q: 5987-96 T: 5891  8885
Q: 1269-3839 T: -2570  -2211

--------------------------------------------------
Iteration 4
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 11ms/step - accuracy: 0.4588 - loss: 1.4701 - val_accuracy: 0.4721 - val_loss: 1.4239
Q: 20-408 T: -388  -407
Q: 493-281 T: 212  349
Q: 115-471 T: -356  -11
Q: 22-2937 T: -2915  -2222
Q: 8929-20 T: 8909  8889
Q: 163-4740 T: -4577  -4666
Q: 526-925 T: -399  -555
Q: 339-68 T: 271  33
Q: 4371-267 T: 4104  3477
Q: 58-7341 T: -7283  -8555

--------------------------------------------------
Iteration 5
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.4791 - loss: 1.4040 - val_accuracy: 0.4878 - val_loss: 1.3676
Q: 966-24 T: 942  666
Q: 68-932 T: -864  -866
Q: 55-4859 T: -4804  -5485
Q: 4-6998 T: -6994  -9993
Q: 296-9202 T: -8906  -9096
Q: 950-7 T: 943  999
Q: 246-3 T: 243  222
Q: 13-362 T: -349  -336
Q: 4642-9207 T: -4565  -4444
Q: 216-2766 T: -2550  -2566

--------------------------------------------------
Iteration 6
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.5012 - loss: 1.3418 - val_accuracy: 0.5145 - val_loss: 1.3166
Q: 84-980 T: -896  -800
Q: 6-6334 T: -6328  -6320
Q: 7-837 T: -830  -870
Q: 3992-2421 T: 1571  299
Q: 67-4982 T: -4915  -4766
Q: 46-3746 T: -3700  -3340
Q: 2-487 T: -485  -442
Q: 97-7214 T: -7117  -7000
Q: 33-195 T: -162  -103
Q: 97-2745 T: -2648  -2299

--------------------------------------------------
Iteration 7
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.5224 - loss: 1.2892 - val_accuracy: 0.5337 - val_loss: 1.2579
Q: 312-3524 T: -3212  -3099
Q: 42-92 T: -50  -44
Q: 444-55 T: 389  444
Q: 234-8 T: 226  222
Q: 2940-1034 T: 1906  2999
Q: 9189-4718 T: 4471  1999
Q: 67-244 T: -177  -266
Q: 161-89 T: 72  11
Q: 5-349 T: -344  -330
Q: 4416-338 T: 4078  4009

--------------------------------------------------
Iteration 8
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 11ms/step - accuracy: 0.5445 - loss: 1.2325 - val_accuracy: 0.5506 - val_loss: 1.2200
Q: 122-58 T: 64  14
Q: 0-6533 T: -6533  -6330
Q: 1-3535 T: -3534  -3531
Q: 388-9092 T: -8704  -8833
Q: 5-7792 T: -7787  -7740
Q: 8878-5 T: 8873  8888
Q: 25-242 T: -217  -110
Q: 2781-273 T: 2508  2744
Q: 524-69 T: 455  478
Q: 241-50 T: 191  144

--------------------------------------------------
Iteration 9
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.5588 - loss: 1.1976 - val_accuracy: 0.5627 - val_loss: 1.1680
Q: 130-50 T: 80  19
Q: 167-65 T: 102  111
Q: 3884-89 T: 3795  3783
Q: 5575-726 T: 4849  4709
Q: 9-4410 T: -4401  -4429
Q: 4670-1 T: 4669  4666
Q: 5512-739 T: 4773  5508
Q: 3-9722 T: -9719  -9935
Q: 5-766 T: -761  -761
Q: 924-59 T: 865  999

--------------------------------------------------
Iteration 10
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.5729 - loss: 1.1505 - val_accuracy: 0.5783 - val_loss: 1.1409
Q: 7794-524 T: 7270  7449
Q: 706-640 T: 66  14
Q: 8601-8 T: 8593  8600
Q: 22-2937 T: -2915  -2988
Q: 7051-69 T: 6982  7099
Q: 167-8452 T: -8285  -8268
Q: 412-51 T: 361  366
Q: 7-6598 T: -6591  -6588
Q: 420-1212 T: -792  -100
Q: 5950-61 T: 5889  5777

--------------------------------------------------
Iteration 11
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 10ms/step - accuracy: 0.5864 - loss: 1.1183 - val_accuracy: 0.5944 - val_loss: 1.0974
Q: 3613-9350 T: -5737  -5333
Q: 6613-3 T: 6610  6613
Q: 4476-84 T: 4392  4413
Q: 8880-445 T: 8435  8388
Q: 3045-39 T: 3006  3006
Q: 5892-1 T: 5891  5882
Q: 59-65 T: -6  11
Q: 11-930 T: -919  -918
Q: 6160-41 T: 6119  6161
Q: 5-744 T: -739  -745

--------------------------------------------------
Iteration 12
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.5978 - loss: 1.0890 - val_accuracy: 0.5943 - val_loss: 1.0857
Q: 76-803 T: -727  -722
Q: 9-9353 T: -9344  -9354
Q: 1-125 T: -124  -122
Q: 2-5609 T: -5607  -5599
Q: 9155-2 T: 9153  9184
Q: 894-6241 T: -5347  -5444
Q: 7455-74 T: 7381  7366
Q: 82-10 T: 72  70
Q: 237-9293 T: -9056  -9095
Q: 4-5688 T: -5684  -5666

--------------------------------------------------
Iteration 13
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.6058 - loss: 1.0620 - val_accuracy: 0.6059 - val_loss: 1.0533
Q: 714-566 T: 148  118
Q: 5705-6 T: 5699  5772
Q: 686-54 T: 632  615
Q: 3156-2596 T: 560  108
Q: 199-7 T: 192  184
Q: 1298-6 T: 1292  1188
Q: 6-976 T: -970  -969
Q: 820-158 T: 662  672
Q: 58-464 T: -406  -408
Q: 659-7574 T: -6915  -6188

--------------------------------------------------
Iteration 14
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.6177 - loss: 1.0349 - val_accuracy: 0.6042 - val_loss: 1.0465
Q: 7-9290 T: -9283  -9283
Q: 1586-77 T: 1509  1537
Q: 4134-52 T: 4082  4126
Q: 3433-9138 T: -5705  -5600
Q: 221-5 T: 216  227
Q: 3156-2596 T: 560  100
Q: 304-4 T: 300  309
Q: 6647-99 T: 6548  6463
Q: 41-812 T: -771  -766
Q: 33-329 T: -296  -296

--------------------------------------------------
Iteration 15
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 11ms/step - accuracy: 0.6240 - loss: 1.0123 - val_accuracy: 0.6219 - val_loss: 1.0132
Q: 2-443 T: -441  -442
Q: 6702-4 T: 6698  6600
Q: 111-95 T: 16  15
Q: 7592-2013 T: 5579  5699
Q: 5-850 T: -845  -840
Q: 28-220 T: -192  -199
Q: 44-3049 T: -3005  -3990
Q: 2-645 T: -643  -643
Q: 1366-70 T: 1296  1335
Q: 439-858 T: -419  -480

--------------------------------------------------
Iteration 16
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.6346 - loss: 0.9879 - val_accuracy: 0.6251 - val_loss: 0.9979
Q: 500-7289 T: -6789  -6888
Q: 1-660 T: -659  -660
Q: 4467-455 T: 4012  4903
Q: 36-136 T: -100  -10
Q: 5258-525 T: 4733  4965
Q: 470-541 T: -71  -15
Q: 20-408 T: -388  -399
Q: 9233-1492 T: 7741  8880
Q: 0-5007 T: -5007  -5009
Q: 1463-8 T: 1455  1458

--------------------------------------------------
Iteration 17
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.6410 - loss: 0.9700 - val_accuracy: 0.6302 - val_loss: 0.9843
Q: 2404-6191 T: -3787  -2522
Q: 55-948 T: -893  -895
Q: 97-954 T: -857  -856
Q: 8512-63 T: 8449  8416
Q: 3-12 T: -9  -1
Q: 482-5822 T: -5340  -5977
Q: 4241-4047 T: 194  -76
Q: 72-597 T: -525  -511
Q: 6383-4398 T: 1985  2863
Q: 162-417 T: -255  -266

--------------------------------------------------
Iteration 18
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.6488 - loss: 0.9471 - val_accuracy: 0.6394 - val_loss: 0.9615
Q: 874-923 T: -49  -1
Q: 53-9831 T: -9778  -9884
Q: 8171-3852 T: 4319  4699
Q: 3749-463 T: 3286  3112
Q: 7-3226 T: -3219  -3222
Q: 1262-9752 T: -8490  -7585
Q: 3-253 T: -250  -251
Q: 44-9758 T: -9714  -9624
Q: 230-70 T: 160  252
Q: 65-946 T: -881  -880

--------------------------------------------------
Iteration 19
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - accuracy: 0.6554 - loss: 0.9307 - val_accuracy: 0.6441 - val_loss: 0.9502
Q: 3760-530 T: 3230  3169
Q: 3059-204 T: 2855  2966
Q: 41-805 T: -764  -752
Q: 9891-6253 T: 3638  3099
Q: 221-9185 T: -8964  -9064
Q: 6166-3690 T: 2476  2889
Q: 26-91 T: -65  -65
Q: 7772-344 T: 7428  7319
Q: 1-1069 T: -1068  -1055
Q: 2-8436 T: -8434  -8444

--------------------------------------------------
Iteration 20
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 7ms/step - accuracy: 0.6600 - loss: 0.9158 - val_accuracy: 0.6485 - val_loss: 0.9325
Q: 21-527 T: -506  -505
Q: 124-6 T: 118  124
Q: 6761-4 T: 6757  6651
Q: 4523-9 T: 4514  4496
Q: 2825-4262 T: -1437  -144
Q: 0-8984 T: -8984  -8897
Q: 374-8645 T: -8271  -8292
Q: 575-2 T: 573  575
Q: 7331-91 T: 7240  7245
Q: 901-9325 T: -8424  -8525

--------------------------------------------------
Iteration 21
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.6688 - loss: 0.8924 - val_accuracy: 0.6539 - val_loss: 0.9182
Q: 672-9412 T: -8740  -8766
Q: 552-8841 T: -8289  -8251
Q: 45-971 T: -926  -935
Q: 724-2724 T: -2000  -2902
Q: 340-229 T: 111  104
Q: 2295-155 T: 2140  2148
Q: 4389-5 T: 4384  4384
Q: 9901-622 T: 9279  9080
Q: 14-28 T: -14  -14
Q: 79-164 T: -85  -88

--------------------------------------------------
Iteration 22
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.6732 - loss: 0.8801 - val_accuracy: 0.6560 - val_loss: 0.9148
Q: 7709-4 T: 7705  7704
Q: 173-56 T: 117  111
Q: 7520-629 T: 6891  6793
Q: 6-5695 T: -5689  -5681
Q: 85-6606 T: -6521  -6500
Q: 59-2 T: 57  57
Q: 3387-5 T: 3382  3389
Q: 50-4531 T: -4481  -4409
Q: 558-933 T: -375  -345
Q: 8299-8 T: 8291  8289

--------------------------------------------------
Iteration 23
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - accuracy: 0.6805 - loss: 0.8608 - val_accuracy: 0.6632 - val_loss: 0.8903
Q: 8-3032 T: -3024  -3032
Q: 578-6974 T: -6396  -6222
Q: 753-3 T: 750  752
Q: 65-438 T: -373  -379
Q: 8565-1 T: 8564  8556
Q: 7379-5862 T: 1517  2199
Q: 896-0 T: 896  886
Q: 9-2752 T: -2743  -2729
Q: 9135-35 T: 9100  9176
Q: 2-3222 T: -3220  -3229

--------------------------------------------------
Iteration 24
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.6843 - loss: 0.8489 - val_accuracy: 0.6590 - val_loss: 0.8947
Q: 953-7 T: 946  946
Q: 1307-855 T: 452  465
Q: 692-69 T: 623  613
Q: 7-6443 T: -6436  -6437
Q: 3-83 T: -80  -70
Q: 8163-4 T: 8159  8153
Q: 57-99 T: -42  -43
Q: 53-57 T: -4  -1
Q: 9-8239 T: -8230  -8219
Q: 30-4326 T: -4296  -4203

--------------------------------------------------
Iteration 25
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.6892 - loss: 0.8364 - val_accuracy: 0.6703 - val_loss: 0.8752
Q: 8-7890 T: -7882  -7889
Q: 4-709 T: -705  -706
Q: 61-377 T: -316  -321
Q: 552-385 T: 167  15
Q: 5172-3 T: 5169  5150
Q: 43-7398 T: -7355  -7338
Q: 85-61 T: 24  23
Q: 2-5897 T: -5895  -5887
Q: 6-894 T: -888  -889
Q: 24-910 T: -886  -887

--------------------------------------------------
Iteration 26
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - accuracy: 0.6964 - loss: 0.8182 - val_accuracy: 0.6744 - val_loss: 0.8605
Q: 7729-4288 T: 3441  3499
Q: 15-3709 T: -3694  -3699
Q: 732-6586 T: -5854  -5999
Q: 9228-848 T: 8380  8433
Q: 7881-26 T: 7855  7838
Q: 156-9 T: 147  158
Q: 2609-877 T: 1732  1725
Q: 71-99 T: -28  -22
Q: 304-4 T: 300  397
Q: 295-0 T: 295  295

--------------------------------------------------
Iteration 27
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.6989 - loss: 0.8084 - val_accuracy: 0.6775 - val_loss: 0.8450
Q: 5091-6 T: 5085  5092
Q: 9-603 T: -594  -590
Q: 549-89 T: 460  458
Q: 688-567 T: 121  200
Q: 5-3762 T: -3757  -3761
Q: 442-17 T: 425  424
Q: 289-0 T: 289  285
Q: 549-89 T: 460  458
Q: 75-992 T: -917  -915
Q: 9379-0 T: 9379  9364

--------------------------------------------------
Iteration 28
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.7084 - loss: 0.7861 - val_accuracy: 0.6807 - val_loss: 0.8414
Q: 617-65 T: 552  558
Q: 4-1878 T: -1874  -1874
Q: 870-13 T: 857  858
Q: 3830-1660 T: 2170  2226
Q: 583-758 T: -175  -189
Q: 470-541 T: -71  -84
Q: 46-773 T: -727  -738
Q: 4283-8 T: 4275  4276
Q: 5527-96 T: 5431  5464
Q: 4102-5024 T: -922  -1122

--------------------------------------------------
Iteration 29
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 10ms/step - accuracy: 0.7115 - loss: 0.7775 - val_accuracy: 0.6723 - val_loss: 0.8633
Q: 4-834 T: -830  -831
Q: 103-660 T: -557  -556
Q: 505-59 T: 446  445
Q: 75-720 T: -645  -648
Q: 2-2922 T: -2920  -2919
Q: 4947-4 T: 4943  4941
Q: 49-941 T: -892  -880
Q: 7-118 T: -111  -111
Q: 86-720 T: -634  -633
Q: 8830-737 T: 8093  8069

--------------------------------------------------
Iteration 30
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.7146 - loss: 0.7662 - val_accuracy: 0.6893 - val_loss: 0.8192
Q: 5741-8122 T: -2381  -2433
Q: 3169-42 T: 3127  3103
Q: 0-12 T: -12  -11
Q: 82-567 T: -485  -496
Q: 504-7 T: 497  497
Q: 66-4703 T: -4637  -4643
Q: 6852-5292 T: 1560  1044
Q: 29-8196 T: -8167  -8162
Q: 122-9 T: 113  115
Q: 354-503 T: -149  -167

--------------------------------------------------
Iteration 31
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.7213 - loss: 0.7501 - val_accuracy: 0.6935 - val_loss: 0.8071
Q: 5-8430 T: -8425  -8428
Q: 1317-2 T: 1315  1336
Q: 0-870 T: -870  -870
Q: 9-383 T: -374  -375
Q: 400-92 T: 308  312
Q: 98-20 T: 78  77
Q: 460-4 T: 456  455
Q: 9271-1472 T: 7799  8820
Q: 628-8 T: 620  611
Q: 5430-552 T: 4878  4880

--------------------------------------------------
Iteration 32
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 12ms/step - accuracy: 0.7255 - loss: 0.7400 - val_accuracy: 0.6965 - val_loss: 0.7937
Q: 1780-5 T: 1775  1770
Q: 258-27 T: 231  230
Q: 672-72 T: 600  603
Q: 80-2009 T: -1929  -1904
Q: 85-965 T: -880  -880
Q: 386-895 T: -509  -596
Q: 3503-5 T: 3498  3406
Q: 1207-7 T: 1200  1109
Q: 9589-530 T: 9059  9933
Q: 8164-7166 T: 998  113

--------------------------------------------------
Iteration 33
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.7303 - loss: 0.7250 - val_accuracy: 0.6958 - val_loss: 0.8007
Q: 377-500 T: -123  -216
Q: 9351-5 T: 9346  9344
Q: 2-883 T: -881  -880
Q: 760-984 T: -224  -206
Q: 3863-31 T: 3832  3802
Q: 3-9722 T: -9719  -9720
Q: 388-9092 T: -8704  -8909
Q: 713-153 T: 560  569
Q: 6611-9590 T: -2979  -3108
Q: 7283-9322 T: -2039  -2199

--------------------------------------------------
Iteration 34
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.7349 - loss: 0.7150 - val_accuracy: 0.7006 - val_loss: 0.7814
Q: 4-6185 T: -6181  -6184
Q: 192-5 T: 187  186
Q: 4682-60 T: 4622  4613
Q: 36-11 T: 25  35
Q: 82-9216 T: -9134  -9157
Q: 27-919 T: -892  -997
Q: 7049-11 T: 7038  7021
Q: 4752-8029 T: -3277  -3367
Q: 1-1701 T: -1700  -1700
Q: 3-1572 T: -1569  -1571

--------------------------------------------------
Iteration 35
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 13ms/step - accuracy: 0.7381 - loss: 0.7047 - val_accuracy: 0.7032 - val_loss: 0.7775
Q: 924-59 T: 865  877
Q: 28-1997 T: -1969  -1964
Q: 297-3 T: 294  295
Q: 896-0 T: 896  895
Q: 5633-554 T: 5079  5980
Q: 2-6661 T: -6659  -6650
Q: 3872-857 T: 3015  3066
Q: 5-3476 T: -3471  -3471
Q: 952-238 T: 714  715
Q: 2603-66 T: 2537  2566

--------------------------------------------------
Iteration 36
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.7433 - loss: 0.6916 - val_accuracy: 0.7043 - val_loss: 0.7811
Q: 1354-215 T: 1139  1223
Q: 315-87 T: 228  225
Q: 6-120 T: -114  -105
Q: 402-69 T: 333  344
Q: 9-9003 T: -8994  -8001
Q: 84-1582 T: -1498  -1572
Q: 400-341 T: 59  66
Q: 10-3639 T: -3629  -3622
Q: 965-296 T: 669  678
Q: 31-4841 T: -4810  -4818

--------------------------------------------------
Iteration 37
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.7441 - loss: 0.6899 - val_accuracy: 0.7122 - val_loss: 0.7497
Q: 8878-5 T: 8873  8873
Q: 30-705 T: -675  -672
Q: 2465-4 T: 2461  2452
Q: 3415-10 T: 3405  3404
Q: 0-817 T: -817  -817
Q: 5789-534 T: 5255  5244
Q: 9320-3226 T: 6094  5994
Q: 269-8 T: 261  260
Q: 9373-2 T: 9371  9371
Q: 17-102 T: -85  -91

--------------------------------------------------
Iteration 38
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.7519 - loss: 0.6682 - val_accuracy: 0.7038 - val_loss: 0.7785
Q: 331-9328 T: -8997  -9011
Q: 99-762 T: -663  -669
Q: 150-8194 T: -8044  -8011
Q: 5748-583 T: 5165  5074
Q: 738-697 T: 41  -2
Q: 183-8223 T: -8040  -8041
Q: 175-58 T: 117  118
Q: 7481-895 T: 6586  6666
Q: 1332-87 T: 1245  1254
Q: 965-296 T: 669  671

--------------------------------------------------
Iteration 39
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 10ms/step - accuracy: 0.7554 - loss: 0.6587 - val_accuracy: 0.7127 - val_loss: 0.7430
Q: 21-9109 T: -9088  -9098
Q: 1-125 T: -124  -124
Q: 38-8168 T: -8130  -8129
Q: 80-602 T: -522  -522
Q: 678-417 T: 261  249
Q: 54-8602 T: -8548  -8564
Q: 268-7 T: 261  260
Q: 33-40 T: -7  -7
Q: 5898-9 T: 5889  5880
Q: 3953-20 T: 3933  3934

--------------------------------------------------
Iteration 40
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.7593 - loss: 0.6490 - val_accuracy: 0.7166 - val_loss: 0.7398
Q: 48-385 T: -337  -338
Q: 7426-82 T: 7344  7364
Q: 561-6 T: 555  555
Q: 812-98 T: 714  714
Q: 1-7442 T: -7441  -7440
Q: 1871-8218 T: -6347  -6390
Q: 5510-1 T: 5509  5516
Q: 574-8 T: 566  566
Q: 611-0 T: 611  612
Q: 85-178 T: -93  -90

--------------------------------------------------
Iteration 41
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.7643 - loss: 0.6370 - val_accuracy: 0.7264 - val_loss: 0.7227
Q: 61-885 T: -824  -825
Q: 218-4341 T: -4123  -4144
Q: 1514-99 T: 1415  1451
Q: 45-5303 T: -5258  -5268
Q: 367-4576 T: -4209  -4200
Q: 5-658 T: -653  -653
Q: 80-999 T: -919  -920
Q: 1725-42 T: 1683  1682
Q: 12-3840 T: -3828  -3828
Q: 8862-65 T: 8797  8708

--------------------------------------------------
Iteration 42
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 10ms/step - accuracy: 0.7680 - loss: 0.6273 - val_accuracy: 0.7208 - val_loss: 0.7277
Q: 917-18 T: 899  909
Q: 951-5254 T: -4303  -4390
Q: 202-98 T: 104  111
Q: 659-670 T: -11  -10
Q: 43-7398 T: -7355  -7346
Q: 8371-4 T: 8367  8374
Q: 918-113 T: 805  796
Q: 6-9903 T: -9897  -9896
Q: 42-92 T: -50  -50
Q: 629-4 T: 625  624

--------------------------------------------------
Iteration 43
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.7717 - loss: 0.6150 - val_accuracy: 0.7176 - val_loss: 0.7255
Q: 4341-3314 T: 1027  100
Q: 5941-363 T: 5578  5661
Q: 7976-293 T: 7683  7573
Q: 12-5877 T: -5865  -5867
Q: 8388-981 T: 7407  7400
Q: 8064-5 T: 8059  8050
Q: 6060-2 T: 6058  6056
Q: 7-619 T: -612  -613
Q: 145-4515 T: -4370  -4383
Q: 10-16 T: -6  -6

--------------------------------------------------
Iteration 44
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.7748 - loss: 0.6035 - val_accuracy: 0.7300 - val_loss: 0.7026
Q: 4041-374 T: 3667  3781
Q: 293-3072 T: -2779  -2825
Q: 696-8016 T: -7320  -7325
Q: 172-618 T: -446  -433
Q: 5-599 T: -594  -593
Q: 775-774 T: 1  18
Q: 17-64 T: -47  -47
Q: 5-5503 T: -5498  -5498
Q: 6837-89 T: 6748  6756
Q: 10-16 T: -6  -4

--------------------------------------------------
Iteration 45
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 12ms/step - accuracy: 0.7802 - loss: 0.5914 - val_accuracy: 0.7272 - val_loss: 0.7004
Q: 4344-0 T: 4344  4343
Q: 9-485 T: -476  -476
Q: 24-2442 T: -2418  -2428
Q: 7-170 T: -163  -163
Q: 27-919 T: -892  -900
Q: 78-1389 T: -1311  -1300
Q: 8830-737 T: 8093  8146
Q: 2-4387 T: -4385  -4385
Q: 25-856 T: -831  -832
Q: 8759-3643 T: 5116  6020

--------------------------------------------------
Iteration 46
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.7834 - loss: 0.5855 - val_accuracy: 0.7359 - val_loss: 0.6874
Q: 8082-7 T: 8075  8074
Q: 9084-792 T: 8292  8260
Q: 406-9514 T: -9108  -9211
Q: 1334-2772 T: -1438  -1531
Q: 6-9733 T: -9727  -9728
Q: 694-7 T: 687  686
Q: 4-238 T: -234  -233
Q: 617-65 T: 552  555
Q: 3349-3229 T: 120  109
Q: 8-95 T: -87  -87

--------------------------------------------------
Iteration 47
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.7887 - loss: 0.5710 - val_accuracy: 0.7358 - val_loss: 0.6808
Q: 918-113 T: 805  706
Q: 552-8841 T: -8289  -8311
Q: 7377-4 T: 7373  7374
Q: 788-342 T: 446  443
Q: 922-17 T: 905  807
Q: 37-109 T: -72  -63
Q: 31-743 T: -712  -711
Q: 6-699 T: -693  -692
Q: 4457-611 T: 3846  3853
Q: 4670-1 T: 4669  4667

--------------------------------------------------
Iteration 48
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.7927 - loss: 0.5584 - val_accuracy: 0.7400 - val_loss: 0.6753
Q: 524-213 T: 311  323
Q: 60-224 T: -164  -163
Q: 9775-3 T: 9772  9773
Q: 98-136 T: -38  -37
Q: 379-30 T: 349  347
Q: 85-178 T: -93  -10
Q: 2104-8663 T: -6559  -6543
Q: 661-91 T: 570  583
Q: 90-216 T: -126  -134
Q: 8759-3643 T: 5116  4144

--------------------------------------------------
Iteration 49
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 10ms/step - accuracy: 0.7972 - loss: 0.5478 - val_accuracy: 0.7357 - val_loss: 0.6705
Q: 916-4931 T: -4015  -394
Q: 506-6024 T: -5518  -5544
Q: 0-5007 T: -5007  -5005
Q: 2609-877 T: 1732  1722
Q: 522-1837 T: -1315  -2333
Q: 167-841 T: -674  -675
Q: 740-78 T: 662  653
Q: 0-104 T: -104  -104
Q: 41-305 T: -264  -273
Q: 2299-147 T: 2152  2242

--------------------------------------------------
Iteration 50
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.8006 - loss: 0.5382 - val_accuracy: 0.7447 - val_loss: 0.6629
Q: 0-832 T: -832  -832
Q: 7213-20 T: 7193  7193
Q: 1388-3 T: 1385  1384
Q: 7829-490 T: 7339  7292
Q: 8169-105 T: 8064  8099
Q: 4-7409 T: -7405  -7405
Q: 269-8 T: 261  260
Q: 5175-96 T: 5079  5086
Q: 3050-2 T: 3048  3044
Q: 795-11 T: 784  774
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>1.4)</p>
<p>Using any neural network architecture of your liking, build  a model with the aim to beat the best performing model in 1.1 or 1.3. Compare your results in a meaningful way, and add a short explanation to why you think/thought your suggested network is better.</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">config</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">config</span><span class="p">[</span><span class="s2">"training_size"</span><span class="p">]</span> <span class="o">=</span> <span class="mi">40000</span>
<span class="n">config</span><span class="p">[</span><span class="s2">"digits"</span><span class="p">]</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">config</span><span class="p">[</span><span class="s2">"hidden_size"</span><span class="p">]</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">config</span><span class="p">[</span><span class="s2">"batch_size"</span><span class="p">]</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">config</span><span class="p">[</span><span class="s2">"iterations"</span><span class="p">]</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">chars</span> <span class="o">=</span> <span class="s1">'0123456789-+ '</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>SOLUTION:</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span><span class="p">,</span> <span class="n">DataLoader</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Dict</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">from</span> <span class="nn">math</span> <span class="kn">import</span> <span class="n">sqrt</span>

<span class="c1"># Configuration</span>
<span class="n">config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">"training_size"</span><span class="p">:</span> <span class="mi">40000</span><span class="p">,</span>
    <span class="s2">"digits"</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
    <span class="s2">"embedding_size"</span><span class="p">:</span> <span class="mi">128</span><span class="p">,</span>
    <span class="s2">"num_heads"</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
    <span class="s2">"num_layers"</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
    <span class="s2">"ffn_hidden_size"</span><span class="p">:</span> <span class="mi">512</span><span class="p">,</span>
    <span class="s2">"dropout"</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="s2">"batch_size"</span><span class="p">:</span> <span class="mi">128</span><span class="p">,</span>
    <span class="s2">"iterations"</span><span class="p">:</span> <span class="mi">250</span><span class="p">,</span>
    <span class="s2">"learning_rate"</span><span class="p">:</span> <span class="mf">1e-4</span><span class="p">,</span>
<span class="p">}</span>
<span class="n">chars</span> <span class="o">=</span> <span class="s1">'0123456789-+ '</span>

<span class="k">class</span> <span class="nc">CharacterTable</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""Handles encoding/decoding of characters to/from one-hot vectors"""</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">chars</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">chars</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">chars</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">char_indices</span> <span class="o">=</span> <span class="p">{</span><span class="n">c</span><span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">chars</span><span class="p">)}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">indices_char</span> <span class="o">=</span> <span class="p">{</span><span class="n">i</span><span class="p">:</span> <span class="n">c</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">chars</span><span class="p">)}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_chars</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">chars</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">encode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">C</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">num_rows</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""One hot encode given string C."""</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_rows</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">chars</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">C</span><span class="p">):</span>
            <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">char_indices</span><span class="p">[</span><span class="n">c</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">calc_argmax</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">calc_argmax</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="s1">''</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">indices_char</span><span class="p">[</span><span class="n">i</span><span class="o">.</span><span class="n">item</span><span class="p">()]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">x</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">ArithmeticDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">x</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y</span> <span class="o">=</span> <span class="n">y</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>

<span class="k">class</span> <span class="nc">PositionalEncoding</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">max_len</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">position</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_len</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
        <span class="n">div_term</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="o">*</span> <span class="o">-</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">))</span> <span class="o">/</span> <span class="n">embedding_size</span><span class="p">))</span>
        <span class="n">pe</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">max_len</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">)</span>
        <span class="n">pe</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
        <span class="n">pe</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">'pe'</span><span class="p">,</span> <span class="n">pe</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="p">[:</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)]</span>

<span class="k">class</span> <span class="nc">TransformerModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">output_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">output_length</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">ffn_hidden_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding_size</span> <span class="o">=</span> <span class="n">embedding_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_length</span> <span class="o">=</span> <span class="n">output_length</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_length</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">config</span><span class="p">[</span><span class="s1">'digits'</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span>

        <span class="c1"># Input embedding and positional encoding</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pos_encoder</span> <span class="o">=</span> <span class="n">PositionalEncoding</span><span class="p">(</span><span class="n">embedding_size</span><span class="p">,</span> <span class="n">max_len</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_length</span><span class="p">)</span>

        <span class="c1"># Transformer Encoder</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">TransformerEncoder</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">TransformerEncoderLayer</span><span class="p">(</span><span class="n">embedding_size</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">ffn_hidden_size</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
            <span class="n">num_layers</span>
        <span class="p">)</span>

        <span class="c1"># Output layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embedding_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>

        <span class="c1"># Embed and add positional encoding</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embedding_size</span><span class="p">)</span> <span class="c1">#scaling for stability</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># Transformer Encoder</span>
        <span class="n">encoder_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder_layers</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># Project the output of each token to desired output length</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_layer</span><span class="p">(</span><span class="n">encoder_outputs</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">generate_arithmetic_data</span><span class="p">(</span><span class="n">config</span><span class="p">:</span> <span class="n">Dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]:</span>
<span class="w">    </span><span class="sd">"""Generate data for both addition and subtraction"""</span>
    <span class="n">questions</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">expected</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">seen</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
    <span class="n">maxlen</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s2">"digits"</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">config</span><span class="p">[</span><span class="s2">"digits"</span><span class="p">]</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">'Generating data...'</span><span class="p">)</span>
    <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">questions</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">config</span><span class="p">[</span><span class="s2">"training_size"</span><span class="p">]:</span>
        <span class="c1"># Generate random numbers with max length of digits</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span> <span class="o">**</span> <span class="n">config</span><span class="p">[</span><span class="s2">"digits"</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span> <span class="o">**</span> <span class="n">config</span><span class="p">[</span><span class="s2">"digits"</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Randomly choose operation</span>
        <span class="n">op</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="s1">'+'</span><span class="p">,</span> <span class="s1">'-'</span><span class="p">])</span>

        <span class="c1"># Calculate result</span>
        <span class="k">if</span> <span class="n">op</span> <span class="o">==</span> <span class="s1">'+'</span><span class="p">:</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">a</span> <span class="o">-</span> <span class="n">b</span>

        <span class="c1"># Create question string</span>
        <span class="n">q</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">'</span><span class="si">{</span><span class="n">a</span><span class="si">}{</span><span class="n">op</span><span class="si">}{</span><span class="n">b</span><span class="si">}</span><span class="s1">'</span>

        <span class="c1"># Skip if we've seen this before</span>
        <span class="n">key</span> <span class="o">=</span> <span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">op</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">seen</span><span class="p">:</span>
            <span class="k">continue</span>
        <span class="n">seen</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>

        <span class="c1"># Pad the data</span>
        <span class="n">query</span> <span class="o">=</span> <span class="n">q</span> <span class="o">+</span> <span class="s1">' '</span> <span class="o">*</span> <span class="p">(</span><span class="n">maxlen</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">q</span><span class="p">))</span>
        <span class="n">ans</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
        <span class="n">ans</span> <span class="o">+=</span> <span class="s1">' '</span> <span class="o">*</span> <span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="s2">"digits"</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">ans</span><span class="p">))</span>

        <span class="n">questions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
        <span class="n">expected</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ans</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">questions</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">config</span><span class="p">[</span><span class="s2">"training_size"</span><span class="p">]:</span>
            <span class="k">break</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Total arithmetic questions: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">questions</span><span class="p">)</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">questions</span><span class="p">,</span> <span class="n">expected</span>

<span class="k">def</span> <span class="nf">prepare_data</span><span class="p">(</span><span class="n">questions</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">expected</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">ctable</span><span class="p">:</span> <span class="n">CharacterTable</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">"""Prepare data by converting to one-hot encoded tensors"""</span>
    <span class="n">maxlen</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s2">"digits"</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">config</span><span class="p">[</span><span class="s2">"digits"</span><span class="p">]</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">questions</span><span class="p">),</span> <span class="n">maxlen</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">chars</span><span class="p">)))</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">questions</span><span class="p">),</span> <span class="n">config</span><span class="p">[</span><span class="s2">"digits"</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">chars</span><span class="p">)))</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">questions</span><span class="p">):</span>
        <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">ctable</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">sentence</span><span class="p">,</span> <span class="n">maxlen</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">expected</span><span class="p">):</span>
        <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">ctable</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">sentence</span><span class="p">,</span> <span class="n">config</span><span class="p">[</span><span class="s2">"digits"</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span>

<span class="k">def</span> <span class="nf">train_epoch</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">:</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">criterion</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
                <span class="n">optimizer</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""Train for one epoch"""</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">total_correct</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">total_samples</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">for</span> <span class="n">batch_x</span><span class="p">,</span> <span class="n">batch_y</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
        <span class="n">batch_x</span><span class="p">,</span> <span class="n">batch_y</span> <span class="o">=</span> <span class="n">batch_x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">batch_y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch_x</span><span class="p">)</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">[:,</span> <span class="p">:</span><span class="n">config</span><span class="p">[</span><span class="s1">'digits'</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">chars</span><span class="p">)),</span> <span class="n">batch_y</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">chars</span><span class="p">)))</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

        <span class="c1"># Calculate accuracy</span>
        <span class="n">predicted_chars</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">output</span><span class="p">[:,</span> <span class="p">:</span><span class="n">config</span><span class="p">[</span><span class="s1">'digits'</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">true_chars</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">batch_y</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">correct_predictions</span> <span class="o">=</span> <span class="p">(</span><span class="n">predicted_chars</span> <span class="o">==</span> <span class="n">true_chars</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">total_correct</span> <span class="o">+=</span> <span class="n">correct_predictions</span>
        <span class="n">total_samples</span> <span class="o">+=</span> <span class="n">batch_x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

    <span class="n">accuracy</span> <span class="o">=</span> <span class="n">total_correct</span> <span class="o">/</span> <span class="n">total_samples</span> <span class="k">if</span> <span class="n">total_samples</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">0</span>
    <span class="k">return</span> <span class="n">total_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">),</span> <span class="n">accuracy</span>

<span class="k">def</span> <span class="nf">validate</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">:</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">criterion</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
            <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""Validate the model"""</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">total_correct</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">total_samples</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">batch_x</span><span class="p">,</span> <span class="n">batch_y</span> <span class="ow">in</span> <span class="n">val_loader</span><span class="p">:</span>
            <span class="n">batch_x</span><span class="p">,</span> <span class="n">batch_y</span> <span class="o">=</span> <span class="n">batch_x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">batch_y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch_x</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">[:,</span> <span class="p">:</span><span class="n">config</span><span class="p">[</span><span class="s1">'digits'</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">chars</span><span class="p">)),</span> <span class="n">batch_y</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">chars</span><span class="p">)))</span>
            <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

            <span class="c1"># Calculate accuracy</span>
            <span class="n">predicted_chars</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">output</span><span class="p">[:,</span> <span class="p">:</span><span class="n">config</span><span class="p">[</span><span class="s1">'digits'</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">true_chars</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">batch_y</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

            <span class="n">correct_predictions</span> <span class="o">=</span> <span class="p">(</span><span class="n">predicted_chars</span> <span class="o">==</span> <span class="n">true_chars</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="n">total_correct</span> <span class="o">+=</span> <span class="n">correct_predictions</span>
            <span class="n">total_samples</span> <span class="o">+=</span> <span class="n">batch_x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

    <span class="n">accuracy</span> <span class="o">=</span> <span class="n">total_correct</span> <span class="o">/</span> <span class="n">total_samples</span> <span class="k">if</span> <span class="n">total_samples</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">0</span>
    <span class="k">return</span> <span class="n">total_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">val_loader</span><span class="p">),</span> <span class="n">accuracy</span>


<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="c1"># Set device</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">'cuda'</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s1">'cpu'</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Using device: </span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

    <span class="c1"># Initialize character table</span>
    <span class="n">ctable</span> <span class="o">=</span> <span class="n">CharacterTable</span><span class="p">(</span><span class="n">chars</span><span class="p">)</span>

    <span class="c1"># Generate and prepare data</span>
    <span class="n">questions</span><span class="p">,</span> <span class="n">expected</span> <span class="o">=</span> <span class="n">generate_arithmetic_data</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">prepare_data</span><span class="p">(</span><span class="n">questions</span><span class="p">,</span> <span class="n">expected</span><span class="p">,</span> <span class="n">ctable</span><span class="p">)</span>

    <span class="c1"># Shuffle data</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randperm</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>

    <span class="c1"># Split data</span>
    <span class="n">split_at</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">//</span> <span class="mi">10</span>
    <span class="n">x_train</span><span class="p">,</span> <span class="n">x_val</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:</span><span class="n">split_at</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="n">split_at</span><span class="p">:]</span>
    <span class="n">y_train</span><span class="p">,</span> <span class="n">y_val</span> <span class="o">=</span> <span class="n">y</span><span class="p">[:</span><span class="n">split_at</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">split_at</span><span class="p">:]</span>

    <span class="c1"># Create datasets and dataloaders</span>
    <span class="n">train_dataset</span> <span class="o">=</span> <span class="n">ArithmeticDataset</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">val_dataset</span> <span class="o">=</span> <span class="n">ArithmeticDataset</span><span class="p">(</span><span class="n">x_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">)</span>

    <span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s2">"batch_size"</span><span class="p">],</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">val_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">val_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s2">"batch_size"</span><span class="p">])</span>

    <span class="c1"># Initialize model</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">TransformerModel</span><span class="p">(</span>
        <span class="n">input_size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">chars</span><span class="p">),</span>
        <span class="n">embedding_size</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s2">"embedding_size"</span><span class="p">],</span>
        <span class="n">num_heads</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s2">"num_heads"</span><span class="p">],</span>
        <span class="n">num_layers</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s2">"num_layers"</span><span class="p">],</span>
        <span class="n">output_size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">chars</span><span class="p">),</span>
        <span class="n">output_length</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s2">"digits"</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">ffn_hidden_size</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s2">"ffn_hidden_size"</span><span class="p">],</span>
        <span class="n">dropout</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s2">"dropout"</span><span class="p">]</span>
    <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s2">"learning_rate"</span><span class="p">])</span>
    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>

    <span class="c1"># Training loop</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Starting training..."</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="s2">"iterations"</span><span class="p">]):</span>
        <span class="n">train_loss</span><span class="p">,</span> <span class="n">train_accuracy</span> <span class="o">=</span> <span class="n">train_epoch</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
        <span class="n">val_loss</span><span class="p">,</span> <span class="n">val_accuracy</span> <span class="o">=</span> <span class="n">validate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>

        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s1">/</span><span class="si">{</span><span class="n">config</span><span class="p">[</span><span class="s2">"iterations"</span><span class="p">]</span><span class="si">}</span><span class="s1">:'</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Train Loss: </span><span class="si">{</span><span class="n">train_loss</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">, Train Accuracy: </span><span class="si">{</span><span class="n">train_accuracy</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">, Val Loss: </span><span class="si">{</span><span class="n">val_loss</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">, Val Accuracy: </span><span class="si">{</span><span class="n">val_accuracy</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>

        <span class="c1"># Validation samples</span>
        <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">5</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Validation Examples:"</span><span class="p">)</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
                    <span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_val</span><span class="p">))</span>
                    <span class="n">x_sample</span> <span class="o">=</span> <span class="n">x_val</span><span class="p">[</span><span class="n">idx</span><span class="p">:</span><span class="n">idx</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
                    <span class="n">y_true</span> <span class="o">=</span> <span class="n">y_val</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>

                    <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x_sample</span><span class="p">)</span>
                    <span class="n">pred</span> <span class="o">=</span> <span class="n">pred</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>

                    <span class="n">q</span> <span class="o">=</span> <span class="n">ctable</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">x_sample</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">cpu</span><span class="p">())</span>
                    <span class="n">correct</span> <span class="o">=</span> <span class="n">ctable</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">y_true</span><span class="p">)</span>
                    <span class="n">guess</span> <span class="o">=</span> <span class="n">ctable</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">pred</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:</span><span class="n">config</span><span class="p">[</span><span class="s1">'digits'</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span>

                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Question: </span><span class="si">{</span><span class="n">q</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="si">}</span><span class="s1"> | True: </span><span class="si">{</span><span class="n">correct</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="si">}</span><span class="s1"> | '</span>
                          <span class="sa">f</span><span class="s1">'Predicted: </span><span class="si">{</span><span class="n">guess</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="si">}</span><span class="s1"> | '</span>
                          <span class="sa">f</span><span class="s1">'</span><span class="si">{</span><span class="s2">""</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">correct</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">guess</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s2">""</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">()</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">"__main__"</span><span class="p">:</span>
    <span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Using device: cuda
Generating data...
Total arithmetic questions: 40000
Starting training...
Epoch 1/250:
Train Loss: 2.4428, Train Accuracy: 0.0001, Val Loss: 2.4005, Val Accuracy: 0.0000

Validation Examples:
Question: 4576-3464 | True: 1112 | Predicted: -113 | 
Question: 3250-4777 | True: -1527 | Predicted: --13 | 
Question: 9670+3822 | True: 13492 | Predicted: 1111 | 
Question: 6353-6167 | True: 186 | Predicted: 1-13 | 
Question: 3854-5930 | True: -2076 | Predicted: -113 | 

Epoch 2/250:
Train Loss: 2.3925, Train Accuracy: 0.0001, Val Loss: 2.3864, Val Accuracy: 0.0003
Epoch 3/250:
Train Loss: 2.3869, Train Accuracy: 0.0001, Val Loss: 2.3832, Val Accuracy: 0.0003
Epoch 4/250:
Train Loss: 2.3839, Train Accuracy: 0.0001, Val Loss: 2.3805, Val Accuracy: 0.0003
Epoch 5/250:
Train Loss: 2.3812, Train Accuracy: 0.0001, Val Loss: 2.3787, Val Accuracy: 0.0003
Epoch 6/250:
Train Loss: 2.3789, Train Accuracy: 0.0001, Val Loss: 2.3755, Val Accuracy: 0.0000

Validation Examples:
Question: 9781+6054 | True: 15835 | Predicted: 1730 | 
Question: 9773+2037 | True: 11810 | Predicted: 1774 | 
Question: 4368+631 | True: 4999 | Predicted: 5344 | 
Question: 1588-3161 | True: -1573 | Predicted: -113 | 
Question: 9755-8441 | True: 1314 | Predicted: 3211 | 

Epoch 7/250:
Train Loss: 2.3760, Train Accuracy: 0.0002, Val Loss: 2.3719, Val Accuracy: 0.0005
Epoch 8/250:
Train Loss: 2.3729, Train Accuracy: 0.0003, Val Loss: 2.3698, Val Accuracy: 0.0005
Epoch 9/250:
Train Loss: 2.3694, Train Accuracy: 0.0002, Val Loss: 2.3655, Val Accuracy: 0.0005
Epoch 10/250:
Train Loss: 2.3646, Train Accuracy: 0.0003, Val Loss: 2.3602, Val Accuracy: 0.0003
Epoch 11/250:
Train Loss: 2.3594, Train Accuracy: 0.0004, Val Loss: 2.3546, Val Accuracy: 0.0010

Validation Examples:
Question: 5412-9155 | True: -3743 | Predicted: -433 | 
Question: 4368+631 | True: 4999 | Predicted: 5445 | 
Question: 2322-823 | True: 1499 | Predicted: -111 | 
Question: 731+6313 | True: 7044 | Predicted: 744 | 
Question: 9598+499 | True: 10097 | Predicted: 1003 | 

Epoch 12/250:
Train Loss: 2.3534, Train Accuracy: 0.0004, Val Loss: 2.3482, Val Accuracy: 0.0018
Epoch 13/250:
Train Loss: 2.3464, Train Accuracy: 0.0008, Val Loss: 2.3370, Val Accuracy: 0.0015
Epoch 14/250:
Train Loss: 2.3358, Train Accuracy: 0.0016, Val Loss: 2.3216, Val Accuracy: 0.0040
Epoch 15/250:
Train Loss: 2.3231, Train Accuracy: 0.0029, Val Loss: 2.3072, Val Accuracy: 0.0030
Epoch 16/250:
Train Loss: 2.3095, Train Accuracy: 0.0037, Val Loss: 2.2913, Val Accuracy: 0.0067

Validation Examples:
Question: 7662+9916 | True: 17578 | Predicted: 16581 | 
Question: 1065+6292 | True: 7357 | Predicted: 8053 | 
Question: 9926+9154 | True: 19080 | Predicted: 11310 | 
Question: 7654-8257 | True: -603 | Predicted: -111 | 
Question: 535-9502 | True: -8967 | Predicted: -35 | 

Epoch 17/250:
Train Loss: 2.2966, Train Accuracy: 0.0064, Val Loss: 2.2755, Val Accuracy: 0.0092
Epoch 18/250:
Train Loss: 2.2819, Train Accuracy: 0.0093, Val Loss: 2.2569, Val Accuracy: 0.0123
Epoch 19/250:
Train Loss: 2.2646, Train Accuracy: 0.0126, Val Loss: 2.2309, Val Accuracy: 0.0260
Epoch 20/250:
Train Loss: 2.2447, Train Accuracy: 0.0184, Val Loss: 2.2007, Val Accuracy: 0.0365
Epoch 21/250:
Train Loss: 2.2207, Train Accuracy: 0.0267, Val Loss: 2.1683, Val Accuracy: 0.0545

Validation Examples:
Question: 8276+7719 | True: 15995 | Predicted: 15655 | 
Question: 4591-8521 | True: -3930 | Predicted: -332 | 
Question: 1820+8376 | True: 10196 | Predicted: 1096 | 
Question: 3806+2871 | True: 6677 | Predicted: 5697 | 
Question: 1305-1007 | True: 298 | Predicted: -302 | 

Epoch 22/250:
Train Loss: 2.1939, Train Accuracy: 0.0375, Val Loss: 2.1422, Val Accuracy: 0.0725
Epoch 23/250:
Train Loss: 2.1728, Train Accuracy: 0.0482, Val Loss: 2.1246, Val Accuracy: 0.0833
Epoch 24/250:
Train Loss: 2.1570, Train Accuracy: 0.0589, Val Loss: 2.1162, Val Accuracy: 0.0998
Epoch 25/250:
Train Loss: 2.1464, Train Accuracy: 0.0661, Val Loss: 2.1074, Val Accuracy: 0.0998
Epoch 26/250:
Train Loss: 2.1383, Train Accuracy: 0.0734, Val Loss: 2.0995, Val Accuracy: 0.1062

Validation Examples:
Question: 795-590 | True: 205 | Predicted: -45 | 
Question: 7256-4636 | True: 2620 | Predicted: 2610 | 
Question: 5794+3258 | True: 9052 | Predicted: 8042 | 
Question: 8452-4275 | True: 4177 | Predicted: 4287 | 
Question: 2534+8695 | True: 11229 | Predicted: 11299 | 

Epoch 27/250:
Train Loss: 2.1295, Train Accuracy: 0.0786, Val Loss: 2.0945, Val Accuracy: 0.1100
Epoch 28/250:
Train Loss: 2.1238, Train Accuracy: 0.0841, Val Loss: 2.0889, Val Accuracy: 0.1175
Epoch 29/250:
Train Loss: 2.1165, Train Accuracy: 0.0930, Val Loss: 2.0810, Val Accuracy: 0.1275
Epoch 30/250:
Train Loss: 2.1109, Train Accuracy: 0.0984, Val Loss: 2.0761, Val Accuracy: 0.1353
Epoch 31/250:
Train Loss: 2.1057, Train Accuracy: 0.1034, Val Loss: 2.0713, Val Accuracy: 0.1403

Validation Examples:
Question: 3165+9934 | True: 13099 | Predicted: 13099 | 
Question: 9513+8096 | True: 17609 | Predicted: 17099 | 
Question: 5832+6287 | True: 12119 | Predicted: 12199 | 
Question: 8888+6989 | True: 15877 | Predicted: 16777 | 
Question: 3506+3678 | True: 7184 | Predicted: 7274 | 

Epoch 32/250:
Train Loss: 2.0985, Train Accuracy: 0.1147, Val Loss: 2.0620, Val Accuracy: 0.1600
Epoch 33/250:
Train Loss: 2.0920, Train Accuracy: 0.1244, Val Loss: 2.0538, Val Accuracy: 0.1787
Epoch 34/250:
Train Loss: 2.0856, Train Accuracy: 0.1351, Val Loss: 2.0473, Val Accuracy: 0.1945
Epoch 35/250:
Train Loss: 2.0792, Train Accuracy: 0.1446, Val Loss: 2.0380, Val Accuracy: 0.2230
Epoch 36/250:
Train Loss: 2.0724, Train Accuracy: 0.1571, Val Loss: 2.0300, Val Accuracy: 0.2320

Validation Examples:
Question: 5843+2504 | True: 8347 | Predicted: 8347 | 
Question: 831+7006 | True: 7837 | Predicted: 7517 | 
Question: 8786-8985 | True: -199 | Predicted: -111 | 
Question: 1315+1202 | True: 2517 | Predicted: 2517 | 
Question: 9136-6772 | True: 2364 | Predicted: 2464 | 

Epoch 37/250:
Train Loss: 2.0655, Train Accuracy: 0.1659, Val Loss: 2.0203, Val Accuracy: 0.2567
Epoch 38/250:
Train Loss: 2.0587, Train Accuracy: 0.1821, Val Loss: 2.0143, Val Accuracy: 0.2660
Epoch 39/250:
Train Loss: 2.0517, Train Accuracy: 0.1953, Val Loss: 2.0040, Val Accuracy: 0.2945
Epoch 40/250:
Train Loss: 2.0446, Train Accuracy: 0.2090, Val Loss: 1.9945, Val Accuracy: 0.3137
Epoch 41/250:
Train Loss: 2.0354, Train Accuracy: 0.2266, Val Loss: 1.9879, Val Accuracy: 0.3365

Validation Examples:
Question: 7423-5607 | True: 1816 | Predicted: 1816 | 
Question: 1972-1330 | True: 642 | Predicted: -642 | 
Question: 7019-468 | True: 6551 | Predicted: 6551 | 
Question: 6699+516 | True: 7215 | Predicted: 7215 | 
Question: 3189-1765 | True: 1424 | Predicted: 1424 | 

Epoch 42/250:
Train Loss: 2.0265, Train Accuracy: 0.2461, Val Loss: 1.9713, Val Accuracy: 0.3772
Epoch 43/250:
Train Loss: 2.0158, Train Accuracy: 0.2672, Val Loss: 1.9616, Val Accuracy: 0.4030
Epoch 44/250:
Train Loss: 2.0051, Train Accuracy: 0.2896, Val Loss: 1.9469, Val Accuracy: 0.4235
Epoch 45/250:
Train Loss: 1.9959, Train Accuracy: 0.3061, Val Loss: 1.9337, Val Accuracy: 0.4612
Epoch 46/250:
Train Loss: 1.9826, Train Accuracy: 0.3281, Val Loss: 1.9196, Val Accuracy: 0.4963

Validation Examples:
Question: 9743-6443 | True: 3300 | Predicted: 3300 | 
Question: 3882-8747 | True: -4865 | Predicted: -405 | 
Question: 2057-9018 | True: -6961 | Predicted: -713 | 
Question: 8953-1330 | True: 7623 | Predicted: 7623 | 
Question: 6324+3916 | True: 10240 | Predicted: 10480 | 

Epoch 47/250:
Train Loss: 1.9723, Train Accuracy: 0.3459, Val Loss: 1.9055, Val Accuracy: 0.5245
Epoch 48/250:
Train Loss: 1.9607, Train Accuracy: 0.3748, Val Loss: 1.8985, Val Accuracy: 0.5357
Epoch 49/250:
Train Loss: 1.9500, Train Accuracy: 0.3922, Val Loss: 1.8860, Val Accuracy: 0.5567
Epoch 50/250:
Train Loss: 1.9344, Train Accuracy: 0.4153, Val Loss: 1.8524, Val Accuracy: 0.6342
Epoch 51/250:
Train Loss: 1.9139, Train Accuracy: 0.4482, Val Loss: 1.8390, Val Accuracy: 0.6657

Validation Examples:
Question: 2441+1237 | True: 3678 | Predicted: 3678 | 
Question: 4647+7207 | True: 11854 | Predicted: 11854 | 
Question: 8380+4235 | True: 12615 | Predicted: 12615 | 
Question: 2205+9242 | True: 11447 | Predicted: 11447 | 
Question: 9235+1353 | True: 10588 | Predicted: 10588 | 

Epoch 52/250:
Train Loss: 1.9016, Train Accuracy: 0.4724, Val Loss: 1.8347, Val Accuracy: 0.6705
Epoch 53/250:
Train Loss: 1.8914, Train Accuracy: 0.4954, Val Loss: 1.8284, Val Accuracy: 0.6907
Epoch 54/250:
Train Loss: 1.8825, Train Accuracy: 0.5182, Val Loss: 1.8253, Val Accuracy: 0.6975
Epoch 55/250:
Train Loss: 1.8766, Train Accuracy: 0.5358, Val Loss: 1.8224, Val Accuracy: 0.7037
Epoch 56/250:
Train Loss: 1.8714, Train Accuracy: 0.5443, Val Loss: 1.8176, Val Accuracy: 0.7285

Validation Examples:
Question: 3177+5501 | True: 8678 | Predicted: 8678 | 
Question: 9361-8041 | True: 1320 | Predicted: 1320 | 
Question: 6632+3723 | True: 10355 | Predicted: 10355 | 
Question: 2123-8523 | True: -6400 | Predicted: -6400 | 
Question: 5015-7480 | True: -2465 | Predicted: -2465 | 

Epoch 57/250:
Train Loss: 1.8670, Train Accuracy: 0.5609, Val Loss: 1.8154, Val Accuracy: 0.7292
Epoch 58/250:
Train Loss: 1.8622, Train Accuracy: 0.5709, Val Loss: 1.8138, Val Accuracy: 0.7355
Epoch 59/250:
Train Loss: 1.8570, Train Accuracy: 0.5842, Val Loss: 1.8112, Val Accuracy: 0.7372
Epoch 60/250:
Train Loss: 1.8539, Train Accuracy: 0.5937, Val Loss: 1.8126, Val Accuracy: 0.7400
Epoch 61/250:
Train Loss: 1.8516, Train Accuracy: 0.6003, Val Loss: 1.8090, Val Accuracy: 0.7520

Validation Examples:
Question: 2112+3381 | True: 5493 | Predicted: 5493 | 
Question: 8828+3584 | True: 12412 | Predicted: 12412 | 
Question: 9665-1738 | True: 7927 | Predicted: 7927 | 
Question: 2527-2207 | True: 320 | Predicted: -320 | 
Question: 468-4045 | True: -3577 | Predicted: -2877 | 

Epoch 62/250:
Train Loss: 1.8481, Train Accuracy: 0.6081, Val Loss: 1.8066, Val Accuracy: 0.7468
Epoch 63/250:
Train Loss: 1.8469, Train Accuracy: 0.6150, Val Loss: 1.8083, Val Accuracy: 0.7455
Epoch 64/250:
Train Loss: 1.8442, Train Accuracy: 0.6225, Val Loss: 1.8077, Val Accuracy: 0.7475
Epoch 65/250:
Train Loss: 1.8426, Train Accuracy: 0.6228, Val Loss: 1.8110, Val Accuracy: 0.7388
Epoch 66/250:
Train Loss: 1.8396, Train Accuracy: 0.6337, Val Loss: 1.8045, Val Accuracy: 0.7505

Validation Examples:
Question: 8476-6124 | True: 2352 | Predicted: 2352 | 
Question: 5386-7287 | True: -1901 | Predicted: -1901 | 
Question: 5851+6943 | True: 12794 | Predicted: 12794 | 
Question: 7223+846 | True: 8069 | Predicted: 8069 | 
Question: 6439+1478 | True: 7917 | Predicted: 7917 | 

Epoch 67/250:
Train Loss: 1.8389, Train Accuracy: 0.6351, Val Loss: 1.8078, Val Accuracy: 0.7488
Epoch 68/250:
Train Loss: 1.8383, Train Accuracy: 0.6388, Val Loss: 1.8059, Val Accuracy: 0.7512
Epoch 69/250:
Train Loss: 1.8363, Train Accuracy: 0.6434, Val Loss: 1.8075, Val Accuracy: 0.7495
Epoch 70/250:
Train Loss: 1.8349, Train Accuracy: 0.6486, Val Loss: 1.8038, Val Accuracy: 0.7538
Epoch 71/250:
Train Loss: 1.8352, Train Accuracy: 0.6481, Val Loss: 1.8026, Val Accuracy: 0.7615

Validation Examples:
Question: 3699+2800 | True: 6499 | Predicted: 6499 | 
Question: 8021-8099 | True: -78 | Predicted: -118 | 
Question: 3024-3176 | True: -152 | Predicted: -122 | 
Question: 3474-5435 | True: -1961 | Predicted: -1061 | 
Question: 9892+6970 | True: 16862 | Predicted: 16862 | 

Epoch 72/250:
Train Loss: 1.8315, Train Accuracy: 0.6611, Val Loss: 1.8034, Val Accuracy: 0.7550
Epoch 73/250:
Train Loss: 1.8307, Train Accuracy: 0.6610, Val Loss: 1.8044, Val Accuracy: 0.7570
Epoch 74/250:
Train Loss: 1.8288, Train Accuracy: 0.6644, Val Loss: 1.7985, Val Accuracy: 0.7652
Epoch 75/250:
Train Loss: 1.8274, Train Accuracy: 0.6687, Val Loss: 1.8032, Val Accuracy: 0.7542
Epoch 76/250:
Train Loss: 1.8284, Train Accuracy: 0.6677, Val Loss: 1.7970, Val Accuracy: 0.7730

Validation Examples:
Question: 9597-6593 | True: 3004 | Predicted: 2004 | 
Question: 2512+6751 | True: 9263 | Predicted: 9263 | 
Question: 1414+2856 | True: 4270 | Predicted: 4270 | 
Question: 4592-8376 | True: -3784 | Predicted: -3784 | 
Question: 3528-5605 | True: -2077 | Predicted: -2087 | 

Epoch 77/250:
Train Loss: 1.8271, Train Accuracy: 0.6734, Val Loss: 1.7976, Val Accuracy: 0.7742
Epoch 78/250:
Train Loss: 1.8253, Train Accuracy: 0.6758, Val Loss: 1.8049, Val Accuracy: 0.7568
Epoch 79/250:
Train Loss: 1.8257, Train Accuracy: 0.6777, Val Loss: 1.7981, Val Accuracy: 0.7668
Epoch 80/250:
Train Loss: 1.8232, Train Accuracy: 0.6815, Val Loss: 1.8012, Val Accuracy: 0.7592
Epoch 81/250:
Train Loss: 1.8211, Train Accuracy: 0.6861, Val Loss: 1.7966, Val Accuracy: 0.7728

Validation Examples:
Question: 9770+5738 | True: 15508 | Predicted: 15508 | 
Question: 8688-1442 | True: 7246 | Predicted: 7246 | 
Question: 102-1002 | True: -900 | Predicted: -8000 | 
Question: 6551-4981 | True: 1570 | Predicted: 1570 | 
Question: 9862-8659 | True: 1203 | Predicted: 1203 | 

Epoch 82/250:
Train Loss: 1.8217, Train Accuracy: 0.6860, Val Loss: 1.7974, Val Accuracy: 0.7742
Epoch 83/250:
Train Loss: 1.8203, Train Accuracy: 0.6869, Val Loss: 1.7964, Val Accuracy: 0.7695
Epoch 84/250:
Train Loss: 1.8195, Train Accuracy: 0.6897, Val Loss: 1.7955, Val Accuracy: 0.7758
Epoch 85/250:
Train Loss: 1.8189, Train Accuracy: 0.6894, Val Loss: 1.7928, Val Accuracy: 0.7760
Epoch 86/250:
Train Loss: 1.8179, Train Accuracy: 0.6933, Val Loss: 1.7917, Val Accuracy: 0.7802

Validation Examples:
Question: 4757-7396 | True: -2639 | Predicted: -2639 | 
Question: 9120-9010 | True: 110 | Predicted: 2988 | 
Question: 3914-3206 | True: 708 | Predicted: 2708 | 
Question: 7564+4824 | True: 12388 | Predicted: 12388 | 
Question: 7956-7794 | True: 162 | Predicted: -162 | 

Epoch 87/250:
Train Loss: 1.8158, Train Accuracy: 0.6987, Val Loss: 1.7944, Val Accuracy: 0.7725
Epoch 88/250:
Train Loss: 1.8151, Train Accuracy: 0.6983, Val Loss: 1.7912, Val Accuracy: 0.7850
Epoch 89/250:
Train Loss: 1.8153, Train Accuracy: 0.6960, Val Loss: 1.7932, Val Accuracy: 0.7778
Epoch 90/250:
Train Loss: 1.8133, Train Accuracy: 0.7021, Val Loss: 1.7900, Val Accuracy: 0.7785
Epoch 91/250:
Train Loss: 1.8145, Train Accuracy: 0.6982, Val Loss: 1.7918, Val Accuracy: 0.7800

Validation Examples:
Question: 1412-1150 | True: 262 | Predicted: 2232 | 
Question: 2407-3824 | True: -1417 | Predicted: -1417 | 
Question: 3714-7975 | True: -4261 | Predicted: -4261 | 
Question: 5789+678 | True: 6467 | Predicted: 6467 | 
Question: 3326-1068 | True: 2258 | Predicted: 2258 | 

Epoch 92/250:
Train Loss: 1.8128, Train Accuracy: 0.7025, Val Loss: 1.7918, Val Accuracy: 0.7808
Epoch 93/250:
Train Loss: 1.8121, Train Accuracy: 0.7036, Val Loss: 1.7869, Val Accuracy: 0.7877
Epoch 94/250:
Train Loss: 1.8107, Train Accuracy: 0.7091, Val Loss: 1.7893, Val Accuracy: 0.7875
Epoch 95/250:
Train Loss: 1.8099, Train Accuracy: 0.7097, Val Loss: 1.7877, Val Accuracy: 0.7845
Epoch 96/250:
Train Loss: 1.8104, Train Accuracy: 0.7079, Val Loss: 1.7898, Val Accuracy: 0.7860

Validation Examples:
Question: 3522-1400 | True: 2122 | Predicted: 2122 | 
Question: 39-6487 | True: -6448 | Predicted: -4437 | 
Question: 6034-8796 | True: -2762 | Predicted: -2762 | 
Question: 6758-8447 | True: -1689 | Predicted: -1689 | 
Question: 8455+1023 | True: 9478 | Predicted: 9478 | 

Epoch 97/250:
Train Loss: 1.8095, Train Accuracy: 0.7100, Val Loss: 1.7892, Val Accuracy: 0.7827
Epoch 98/250:
Train Loss: 1.8094, Train Accuracy: 0.7117, Val Loss: 1.7906, Val Accuracy: 0.7835
Epoch 99/250:
Train Loss: 1.8079, Train Accuracy: 0.7119, Val Loss: 1.7872, Val Accuracy: 0.7917
Epoch 100/250:
Train Loss: 1.8070, Train Accuracy: 0.7202, Val Loss: 1.7837, Val Accuracy: 0.7957
Epoch 101/250:
Train Loss: 1.8050, Train Accuracy: 0.7202, Val Loss: 1.7860, Val Accuracy: 0.7915

Validation Examples:
Question: 6808+1114 | True: 7922 | Predicted: 7922 | 
Question: 1111-9698 | True: -8587 | Predicted: -8587 | 
Question: 4560-5119 | True: -559 | Predicted: -453 | 
Question: 180-5103 | True: -4923 | Predicted: -4233 | 
Question: 8363-7320 | True: 1043 | Predicted: 1043 | 

Epoch 102/250:
Train Loss: 1.8048, Train Accuracy: 0.7223, Val Loss: 1.7841, Val Accuracy: 0.7913
Epoch 103/250:
Train Loss: 1.8034, Train Accuracy: 0.7215, Val Loss: 1.7828, Val Accuracy: 0.7955
Epoch 104/250:
Train Loss: 1.8034, Train Accuracy: 0.7203, Val Loss: 1.7822, Val Accuracy: 0.7917
Epoch 105/250:
Train Loss: 1.8013, Train Accuracy: 0.7294, Val Loss: 1.7802, Val Accuracy: 0.8000
Epoch 106/250:
Train Loss: 1.8016, Train Accuracy: 0.7268, Val Loss: 1.7801, Val Accuracy: 0.8013

Validation Examples:
Question: 4354+2132 | True: 6486 | Predicted: 6486 | 
Question: 9174+1205 | True: 10379 | Predicted: 10379 | 
Question: 6625-5878 | True: 747 | Predicted: 8747 | 
Question: 8959-3202 | True: 5757 | Predicted: 5757 | 
Question: 617-845 | True: -228 | Predicted: -828 | 

Epoch 107/250:
Train Loss: 1.8019, Train Accuracy: 0.7239, Val Loss: 1.7798, Val Accuracy: 0.7995
Epoch 108/250:
Train Loss: 1.7992, Train Accuracy: 0.7328, Val Loss: 1.7784, Val Accuracy: 0.8057
Epoch 109/250:
Train Loss: 1.7996, Train Accuracy: 0.7286, Val Loss: 1.7789, Val Accuracy: 0.8010
Epoch 110/250:
Train Loss: 1.7996, Train Accuracy: 0.7319, Val Loss: 1.7796, Val Accuracy: 0.8040
Epoch 111/250:
Train Loss: 1.7987, Train Accuracy: 0.7302, Val Loss: 1.7783, Val Accuracy: 0.8005

Validation Examples:
Question: 3064-77 | True: 2987 | Predicted: 2987 | 
Question: 8131+7320 | True: 15451 | Predicted: 15451 | 
Question: 6518+1556 | True: 8074 | Predicted: 8074 | 
Question: 1416-4596 | True: -3180 | Predicted: -3180 | 
Question: 6518+642 | True: 7160 | Predicted: 7160 | 

Epoch 112/250:
Train Loss: 1.7975, Train Accuracy: 0.7326, Val Loss: 1.7802, Val Accuracy: 0.8047
Epoch 113/250:
Train Loss: 1.7959, Train Accuracy: 0.7384, Val Loss: 1.7757, Val Accuracy: 0.8075
Epoch 114/250:
Train Loss: 1.7960, Train Accuracy: 0.7389, Val Loss: 1.7774, Val Accuracy: 0.8067
Epoch 115/250:
Train Loss: 1.7943, Train Accuracy: 0.7412, Val Loss: 1.7763, Val Accuracy: 0.8197
Epoch 116/250:
Train Loss: 1.7945, Train Accuracy: 0.7425, Val Loss: 1.7734, Val Accuracy: 0.8193

Validation Examples:
Question: 6240+5980 | True: 12220 | Predicted: 12220 | 
Question: 3116-2493 | True: 623 | Predicted: 8623 | 
Question: 1658-2350 | True: -692 | Predicted: -699 | 
Question: 1987-7899 | True: -5912 | Predicted: -5912 | 
Question: 8117-620 | True: 7497 | Predicted: 7497 | 

Epoch 117/250:
Train Loss: 1.7918, Train Accuracy: 0.7505, Val Loss: 1.7726, Val Accuracy: 0.8243
Epoch 118/250:
Train Loss: 1.7922, Train Accuracy: 0.7498, Val Loss: 1.7737, Val Accuracy: 0.8180
Epoch 119/250:
Train Loss: 1.7918, Train Accuracy: 0.7475, Val Loss: 1.7766, Val Accuracy: 0.8153
Epoch 120/250:
Train Loss: 1.7917, Train Accuracy: 0.7493, Val Loss: 1.7710, Val Accuracy: 0.8220
Epoch 121/250:
Train Loss: 1.7921, Train Accuracy: 0.7455, Val Loss: 1.7740, Val Accuracy: 0.8165

Validation Examples:
Question: 8482+8270 | True: 16752 | Predicted: 16752 | 
Question: 377-2588 | True: -2211 | Predicted: -2811 | 
Question: 2458+4193 | True: 6651 | Predicted: 6651 | 
Question: 7726-8708 | True: -982 | Predicted: -0982 | 
Question: 6387-9672 | True: -3285 | Predicted: -3285 | 

Epoch 122/250:
Train Loss: 1.7918, Train Accuracy: 0.7487, Val Loss: 1.7706, Val Accuracy: 0.8270
Epoch 123/250:
Train Loss: 1.7904, Train Accuracy: 0.7512, Val Loss: 1.7691, Val Accuracy: 0.8263
Epoch 124/250:
Train Loss: 1.7890, Train Accuracy: 0.7552, Val Loss: 1.7717, Val Accuracy: 0.8263
Epoch 125/250:
Train Loss: 1.7882, Train Accuracy: 0.7595, Val Loss: 1.7693, Val Accuracy: 0.8325
Epoch 126/250:
Train Loss: 1.7876, Train Accuracy: 0.7585, Val Loss: 1.7701, Val Accuracy: 0.8225

Validation Examples:
Question: 9109+8187 | True: 17296 | Predicted: 17296 | 
Question: 7757+6935 | True: 14692 | Predicted: 14692 | 
Question: 6163+2951 | True: 9114 | Predicted: 9114 | 
Question: 8094+1985 | True: 10079 | Predicted: 1079 | 
Question: 4648+4889 | True: 9537 | Predicted: 9537 | 

Epoch 127/250:
Train Loss: 1.7877, Train Accuracy: 0.7595, Val Loss: 1.7692, Val Accuracy: 0.8315
Epoch 128/250:
Train Loss: 1.7867, Train Accuracy: 0.7615, Val Loss: 1.7740, Val Accuracy: 0.8163
Epoch 129/250:
Train Loss: 1.7867, Train Accuracy: 0.7607, Val Loss: 1.7705, Val Accuracy: 0.8240
Epoch 130/250:
Train Loss: 1.7872, Train Accuracy: 0.7563, Val Loss: 1.7691, Val Accuracy: 0.8303
Epoch 131/250:
Train Loss: 1.7869, Train Accuracy: 0.7604, Val Loss: 1.7684, Val Accuracy: 0.8297

Validation Examples:
Question: 1885-4702 | True: -2817 | Predicted: -2817 | 
Question: 8253+7305 | True: 15558 | Predicted: 15558 | 
Question: 895-2592 | True: -1697 | Predicted: -1377 | 
Question: 7016+9520 | True: 16536 | Predicted: 16536 | 
Question: 7243-7060 | True: 183 | Predicted: 1727 | 

Epoch 132/250:
Train Loss: 1.7869, Train Accuracy: 0.7584, Val Loss: 1.7684, Val Accuracy: 0.8303
Epoch 133/250:
Train Loss: 1.7859, Train Accuracy: 0.7644, Val Loss: 1.7674, Val Accuracy: 0.8305
Epoch 134/250:
Train Loss: 1.7852, Train Accuracy: 0.7622, Val Loss: 1.7673, Val Accuracy: 0.8330
Epoch 135/250:
Train Loss: 1.7832, Train Accuracy: 0.7679, Val Loss: 1.7656, Val Accuracy: 0.8347
Epoch 136/250:
Train Loss: 1.7837, Train Accuracy: 0.7652, Val Loss: 1.7687, Val Accuracy: 0.8310

Validation Examples:
Question: 1949+6985 | True: 8934 | Predicted: 8934 | 
Question: 1984-2056 | True: -72 | Predicted: -077 | 
Question: 3403-6269 | True: -2866 | Predicted: -2866 | 
Question: 3921+1200 | True: 5121 | Predicted: 5121 | 
Question: 9118-8411 | True: 707 | Predicted: 8707 | 

Epoch 137/250:
Train Loss: 1.7834, Train Accuracy: 0.7674, Val Loss: 1.7681, Val Accuracy: 0.8317
Epoch 138/250:
Train Loss: 1.7827, Train Accuracy: 0.7690, Val Loss: 1.7707, Val Accuracy: 0.8285
Epoch 139/250:
Train Loss: 1.7831, Train Accuracy: 0.7706, Val Loss: 1.7649, Val Accuracy: 0.8345
Epoch 140/250:
Train Loss: 1.7820, Train Accuracy: 0.7713, Val Loss: 1.7678, Val Accuracy: 0.8253
Epoch 141/250:
Train Loss: 1.7838, Train Accuracy: 0.7652, Val Loss: 1.7718, Val Accuracy: 0.8200

Validation Examples:
Question: 7605-6572 | True: 1033 | Predicted: 1033 | 
Question: 1922+1840 | True: 3762 | Predicted: 3762 | 
Question: 5686-1221 | True: 4465 | Predicted: 4465 | 
Question: 5443+9179 | True: 14622 | Predicted: 14622 | 
Question: 806+7894 | True: 8700 | Predicted: 8700 | 

Epoch 142/250:
Train Loss: 1.7842, Train Accuracy: 0.7658, Val Loss: 1.7663, Val Accuracy: 0.8355
Epoch 143/250:
Train Loss: 1.7813, Train Accuracy: 0.7724, Val Loss: 1.7639, Val Accuracy: 0.8347
Epoch 144/250:
Train Loss: 1.7812, Train Accuracy: 0.7724, Val Loss: 1.7665, Val Accuracy: 0.8335
Epoch 145/250:
Train Loss: 1.7804, Train Accuracy: 0.7740, Val Loss: 1.7658, Val Accuracy: 0.8305
Epoch 146/250:
Train Loss: 1.7797, Train Accuracy: 0.7753, Val Loss: 1.7640, Val Accuracy: 0.8323

Validation Examples:
Question: 3653-4168 | True: -515 | Predicted: -515 | 
Question: 4998+7793 | True: 12791 | Predicted: 12791 | 
Question: 6333-7326 | True: -993 | Predicted: -193 | 
Question: 8027-6731 | True: 1296 | Predicted: 1296 | 
Question: 7408-7541 | True: -133 | Predicted: -143 | 

Epoch 147/250:
Train Loss: 1.7781, Train Accuracy: 0.7787, Val Loss: 1.7654, Val Accuracy: 0.8317
Epoch 148/250:
Train Loss: 1.7765, Train Accuracy: 0.7817, Val Loss: 1.7631, Val Accuracy: 0.8345
Epoch 149/250:
Train Loss: 1.7773, Train Accuracy: 0.7770, Val Loss: 1.7620, Val Accuracy: 0.8333
Epoch 150/250:
Train Loss: 1.7781, Train Accuracy: 0.7742, Val Loss: 1.7647, Val Accuracy: 0.8297
Epoch 151/250:
Train Loss: 1.7778, Train Accuracy: 0.7751, Val Loss: 1.7623, Val Accuracy: 0.8350

Validation Examples:
Question: 5002+2275 | True: 7277 | Predicted: 7277 | 
Question: 7012+9313 | True: 16325 | Predicted: 16325 | 
Question: 7440+6452 | True: 13892 | Predicted: 13892 | 
Question: 8449-7304 | True: 1145 | Predicted: 1145 | 
Question: 2018-3544 | True: -1526 | Predicted: -1526 | 

Epoch 152/250:
Train Loss: 1.7760, Train Accuracy: 0.7783, Val Loss: 1.7608, Val Accuracy: 0.8405
Epoch 153/250:
Train Loss: 1.7755, Train Accuracy: 0.7823, Val Loss: 1.7602, Val Accuracy: 0.8390
Epoch 154/250:
Train Loss: 1.7763, Train Accuracy: 0.7774, Val Loss: 1.7625, Val Accuracy: 0.8343
Epoch 155/250:
Train Loss: 1.7756, Train Accuracy: 0.7817, Val Loss: 1.7622, Val Accuracy: 0.8367
Epoch 156/250:
Train Loss: 1.7756, Train Accuracy: 0.7819, Val Loss: 1.7620, Val Accuracy: 0.8390

Validation Examples:
Question: 5487+4047 | True: 9534 | Predicted: 9534 | 
Question: 7976+1545 | True: 9521 | Predicted: 9521 | 
Question: 2306+2612 | True: 4918 | Predicted: 4918 | 
Question: 390-7305 | True: -6915 | Predicted: -6115 | 
Question: 9145+1800 | True: 10945 | Predicted: 10945 | 

Epoch 157/250:
Train Loss: 1.7740, Train Accuracy: 0.7842, Val Loss: 1.7621, Val Accuracy: 0.8335
Epoch 158/250:
Train Loss: 1.7747, Train Accuracy: 0.7830, Val Loss: 1.7594, Val Accuracy: 0.8405
Epoch 159/250:
Train Loss: 1.7751, Train Accuracy: 0.7796, Val Loss: 1.7627, Val Accuracy: 0.8290
Epoch 160/250:
Train Loss: 1.7743, Train Accuracy: 0.7825, Val Loss: 1.7594, Val Accuracy: 0.8387
Epoch 161/250:
Train Loss: 1.7736, Train Accuracy: 0.7833, Val Loss: 1.7608, Val Accuracy: 0.8355

Validation Examples:
Question: 8103+6493 | True: 14596 | Predicted: 14596 | 
Question: 9585+3149 | True: 12734 | Predicted: 12734 | 
Question: 3811-6103 | True: -2292 | Predicted: -2292 | 
Question: 4576-4146 | True: 430 | Predicted: 2560 | 
Question: 6200-3362 | True: 2838 | Predicted: 2838 | 

Epoch 162/250:
Train Loss: 1.7731, Train Accuracy: 0.7879, Val Loss: 1.7611, Val Accuracy: 0.8403
Epoch 163/250:
Train Loss: 1.7759, Train Accuracy: 0.7809, Val Loss: 1.7600, Val Accuracy: 0.8385
Epoch 164/250:
Train Loss: 1.7727, Train Accuracy: 0.7866, Val Loss: 1.7600, Val Accuracy: 0.8407
Epoch 165/250:
Train Loss: 1.7722, Train Accuracy: 0.7887, Val Loss: 1.7592, Val Accuracy: 0.8393
Epoch 166/250:
Train Loss: 1.7712, Train Accuracy: 0.7901, Val Loss: 1.7609, Val Accuracy: 0.8333

Validation Examples:
Question: 2777-4695 | True: -1918 | Predicted: -1918 | 
Question: 8455+1023 | True: 9478 | Predicted: 9478 | 
Question: 7105-5738 | True: 1367 | Predicted: 1367 | 
Question: 6456+4340 | True: 10796 | Predicted: 10796 | 
Question: 2654+8375 | True: 11029 | Predicted: 11029 | 

Epoch 167/250:
Train Loss: 1.7709, Train Accuracy: 0.7882, Val Loss: 1.7574, Val Accuracy: 0.8413
Epoch 168/250:
Train Loss: 1.7705, Train Accuracy: 0.7914, Val Loss: 1.7592, Val Accuracy: 0.8365
Epoch 169/250:
Train Loss: 1.7698, Train Accuracy: 0.7923, Val Loss: 1.7595, Val Accuracy: 0.8345
Epoch 170/250:
Train Loss: 1.7706, Train Accuracy: 0.7874, Val Loss: 1.7584, Val Accuracy: 0.8360
Epoch 171/250:
Train Loss: 1.7708, Train Accuracy: 0.7857, Val Loss: 1.7570, Val Accuracy: 0.8410

Validation Examples:
Question: 7594+452 | True: 8046 | Predicted: 7046 | 
Question: 2504-2169 | True: 335 | Predicted: 1365 | 
Question: 6361-7259 | True: -898 | Predicted: -894 | 
Question: 4162+1654 | True: 5816 | Predicted: 5816 | 
Question: 269+9484 | True: 9753 | Predicted: 9753 | 

Epoch 172/250:
Train Loss: 1.7704, Train Accuracy: 0.7894, Val Loss: 1.7558, Val Accuracy: 0.8468
Epoch 173/250:
Train Loss: 1.7701, Train Accuracy: 0.7917, Val Loss: 1.7573, Val Accuracy: 0.8420
Epoch 174/250:
Train Loss: 1.7693, Train Accuracy: 0.7921, Val Loss: 1.7566, Val Accuracy: 0.8440
Epoch 175/250:
Train Loss: 1.7687, Train Accuracy: 0.7935, Val Loss: 1.7580, Val Accuracy: 0.8397
Epoch 176/250:
Train Loss: 1.7693, Train Accuracy: 0.7912, Val Loss: 1.7588, Val Accuracy: 0.8350

Validation Examples:
Question: 1957-2716 | True: -759 | Predicted: -759 | 
Question: 2572-9361 | True: -6789 | Predicted: -6789 | 
Question: 2337-5182 | True: -2845 | Predicted: -2845 | 
Question: 8491-8539 | True: -48 | Predicted: -048 | 
Question: 8548-8329 | True: 219 | Predicted: 1789 | 

Epoch 177/250:
Train Loss: 1.7687, Train Accuracy: 0.7934, Val Loss: 1.7556, Val Accuracy: 0.8455
Epoch 178/250:
Train Loss: 1.7684, Train Accuracy: 0.7940, Val Loss: 1.7572, Val Accuracy: 0.8417
Epoch 179/250:
Train Loss: 1.7683, Train Accuracy: 0.7939, Val Loss: 1.7558, Val Accuracy: 0.8427
Epoch 180/250:
Train Loss: 1.7687, Train Accuracy: 0.7942, Val Loss: 1.7584, Val Accuracy: 0.8350
Epoch 181/250:
Train Loss: 1.7682, Train Accuracy: 0.7959, Val Loss: 1.7588, Val Accuracy: 0.8375

Validation Examples:
Question: 282+4661 | True: 4943 | Predicted: 4943 | 
Question: 2014-736 | True: 1278 | Predicted: 1278 | 
Question: 298-9152 | True: -8854 | Predicted: -8554 | 
Question: 5176-2310 | True: 2866 | Predicted: 2866 | 
Question: 6954-5646 | True: 1308 | Predicted: 1308 | 

Epoch 182/250:
Train Loss: 1.7671, Train Accuracy: 0.7963, Val Loss: 1.7543, Val Accuracy: 0.8482
Epoch 183/250:
Train Loss: 1.7683, Train Accuracy: 0.7961, Val Loss: 1.7597, Val Accuracy: 0.8403
Epoch 184/250:
Train Loss: 1.7669, Train Accuracy: 0.7989, Val Loss: 1.7533, Val Accuracy: 0.8502
Epoch 185/250:
Train Loss: 1.7662, Train Accuracy: 0.7992, Val Loss: 1.7567, Val Accuracy: 0.8387
Epoch 186/250:
Train Loss: 1.7659, Train Accuracy: 0.8026, Val Loss: 1.7538, Val Accuracy: 0.8485

Validation Examples:
Question: 3218+1332 | True: 4550 | Predicted: 4550 | 
Question: 8477+7694 | True: 16171 | Predicted: 16171 | 
Question: 9890+4036 | True: 13926 | Predicted: 13926 | 
Question: 4513-6837 | True: -2324 | Predicted: -2324 | 
Question: 367-8504 | True: -8137 | Predicted: -8337 | 

Epoch 187/250:
Train Loss: 1.7649, Train Accuracy: 0.8036, Val Loss: 1.7555, Val Accuracy: 0.8468
Epoch 188/250:
Train Loss: 1.7654, Train Accuracy: 0.8011, Val Loss: 1.7571, Val Accuracy: 0.8395
Epoch 189/250:
Train Loss: 1.7657, Train Accuracy: 0.8004, Val Loss: 1.7545, Val Accuracy: 0.8438
Epoch 190/250:
Train Loss: 1.7640, Train Accuracy: 0.8033, Val Loss: 1.7531, Val Accuracy: 0.8528
Epoch 191/250:
Train Loss: 1.7634, Train Accuracy: 0.8063, Val Loss: 1.7524, Val Accuracy: 0.8538

Validation Examples:
Question: 7605-6572 | True: 1033 | Predicted: 1033 | 
Question: 711+8986 | True: 9697 | Predicted: 9697 | 
Question: 2340+8550 | True: 10890 | Predicted: 10890 | 
Question: 8306+6900 | True: 15206 | Predicted: 15206 | 
Question: 2436-8478 | True: -6042 | Predicted: -6042 | 

Epoch 192/250:
Train Loss: 1.7658, Train Accuracy: 0.7997, Val Loss: 1.7543, Val Accuracy: 0.8468
Epoch 193/250:
Train Loss: 1.7638, Train Accuracy: 0.8069, Val Loss: 1.7534, Val Accuracy: 0.8485
Epoch 194/250:
Train Loss: 1.7636, Train Accuracy: 0.8050, Val Loss: 1.7527, Val Accuracy: 0.8542
Epoch 195/250:
Train Loss: 1.7632, Train Accuracy: 0.8061, Val Loss: 1.7526, Val Accuracy: 0.8525
Epoch 196/250:
Train Loss: 1.7618, Train Accuracy: 0.8110, Val Loss: 1.7519, Val Accuracy: 0.8578

Validation Examples:
Question: 4504-555 | True: 3949 | Predicted: 3949 | 
Question: 959-9611 | True: -8652 | Predicted: -8552 | 
Question: 6682+7948 | True: 14630 | Predicted: 14630 | 
Question: 3049-9647 | True: -6598 | Predicted: -6698 | 
Question: 4446-3053 | True: 1393 | Predicted: 1393 | 

Epoch 197/250:
Train Loss: 1.7642, Train Accuracy: 0.8026, Val Loss: 1.7519, Val Accuracy: 0.8555
Epoch 198/250:
Train Loss: 1.7650, Train Accuracy: 0.8030, Val Loss: 1.7560, Val Accuracy: 0.8450
Epoch 199/250:
Train Loss: 1.7630, Train Accuracy: 0.8070, Val Loss: 1.7531, Val Accuracy: 0.8515
Epoch 200/250:
Train Loss: 1.7614, Train Accuracy: 0.8113, Val Loss: 1.7511, Val Accuracy: 0.8552
Epoch 201/250:
Train Loss: 1.7630, Train Accuracy: 0.8079, Val Loss: 1.7523, Val Accuracy: 0.8515

Validation Examples:
Question: 3666+9401 | True: 13067 | Predicted: 13067 | 
Question: 3290+7122 | True: 10412 | Predicted: 10412 | 
Question: 8828+3584 | True: 12412 | Predicted: 12412 | 
Question: 3545-593 | True: 2952 | Predicted: 2952 | 
Question: 9358+2583 | True: 11941 | Predicted: 11941 | 

Epoch 202/250:
Train Loss: 1.7631, Train Accuracy: 0.8061, Val Loss: 1.7521, Val Accuracy: 0.8518
Epoch 203/250:
Train Loss: 1.7621, Train Accuracy: 0.8100, Val Loss: 1.7506, Val Accuracy: 0.8575
Epoch 204/250:
Train Loss: 1.7616, Train Accuracy: 0.8120, Val Loss: 1.7518, Val Accuracy: 0.8538
Epoch 205/250:
Train Loss: 1.7621, Train Accuracy: 0.8104, Val Loss: 1.7527, Val Accuracy: 0.8530
Epoch 206/250:
Train Loss: 1.7613, Train Accuracy: 0.8138, Val Loss: 1.7507, Val Accuracy: 0.8568

Validation Examples:
Question: 420+3095 | True: 3515 | Predicted: 3515 | 
Question: 249+4477 | True: 4726 | Predicted: 4726 | 
Question: 9478-9435 | True: 43 | Predicted: -957 | 
Question: 791+3661 | True: 4452 | Predicted: 4452 | 
Question: 7601+5765 | True: 13366 | Predicted: 13366 | 

Epoch 207/250:
Train Loss: 1.7628, Train Accuracy: 0.8080, Val Loss: 1.7537, Val Accuracy: 0.8475
Epoch 208/250:
Train Loss: 1.7627, Train Accuracy: 0.8096, Val Loss: 1.7522, Val Accuracy: 0.8560
Epoch 209/250:
Train Loss: 1.7614, Train Accuracy: 0.8119, Val Loss: 1.7528, Val Accuracy: 0.8515
Epoch 210/250:
Train Loss: 1.7606, Train Accuracy: 0.8140, Val Loss: 1.7537, Val Accuracy: 0.8482
Epoch 211/250:
Train Loss: 1.7615, Train Accuracy: 0.8114, Val Loss: 1.7512, Val Accuracy: 0.8550

Validation Examples:
Question: 2005-703 | True: 1302 | Predicted: 1302 | 
Question: 8662-5208 | True: 3454 | Predicted: 3454 | 
Question: 7808-7162 | True: 646 | Predicted: 2646 | 
Question: 138+133 | True: 271 | Predicted: 127 | 
Question: 9027-5509 | True: 3518 | Predicted: 3518 | 

Epoch 212/250:
Train Loss: 1.7599, Train Accuracy: 0.8149, Val Loss: 1.7502, Val Accuracy: 0.8598
Epoch 213/250:
Train Loss: 1.7623, Train Accuracy: 0.8089, Val Loss: 1.7518, Val Accuracy: 0.8528
Epoch 214/250:
Train Loss: 1.7620, Train Accuracy: 0.8101, Val Loss: 1.7543, Val Accuracy: 0.8495
Epoch 215/250:
Train Loss: 1.7602, Train Accuracy: 0.8141, Val Loss: 1.7511, Val Accuracy: 0.8585
Epoch 216/250:
Train Loss: 1.7594, Train Accuracy: 0.8175, Val Loss: 1.7507, Val Accuracy: 0.8565

Validation Examples:
Question: 273+8492 | True: 8765 | Predicted: 8765 | 
Question: 1738+7944 | True: 9682 | Predicted: 9682 | 
Question: 4012-8082 | True: -4070 | Predicted: -4070 | 
Question: 2524+730 | True: 3254 | Predicted: 3254 | 
Question: 5226-5907 | True: -681 | Predicted: -671 | 

Epoch 217/250:
Train Loss: 1.7605, Train Accuracy: 0.8145, Val Loss: 1.7509, Val Accuracy: 0.8548
Epoch 218/250:
Train Loss: 1.7617, Train Accuracy: 0.8097, Val Loss: 1.7500, Val Accuracy: 0.8568
Epoch 219/250:
Train Loss: 1.7588, Train Accuracy: 0.8186, Val Loss: 1.7505, Val Accuracy: 0.8565
Epoch 220/250:
Train Loss: 1.7604, Train Accuracy: 0.8141, Val Loss: 1.7505, Val Accuracy: 0.8595
Epoch 221/250:
Train Loss: 1.7599, Train Accuracy: 0.8148, Val Loss: 1.7502, Val Accuracy: 0.8562

Validation Examples:
Question: 2106+6613 | True: 8719 | Predicted: 8719 | 
Question: 8483-1503 | True: 6980 | Predicted: 6980 | 
Question: 5385+3743 | True: 9128 | Predicted: 9128 | 
Question: 8786-8985 | True: -199 | Predicted: -299 | 
Question: 485+6134 | True: 6619 | Predicted: 6619 | 

Epoch 222/250:
Train Loss: 1.7595, Train Accuracy: 0.8162, Val Loss: 1.7505, Val Accuracy: 0.8585
Epoch 223/250:
Train Loss: 1.7606, Train Accuracy: 0.8154, Val Loss: 1.7538, Val Accuracy: 0.8535
Epoch 224/250:
Train Loss: 1.7612, Train Accuracy: 0.8106, Val Loss: 1.7493, Val Accuracy: 0.8595
Epoch 225/250:
Train Loss: 1.7586, Train Accuracy: 0.8181, Val Loss: 1.7506, Val Accuracy: 0.8568
Epoch 226/250:
Train Loss: 1.7591, Train Accuracy: 0.8174, Val Loss: 1.7509, Val Accuracy: 0.8555

Validation Examples:
Question: 2330-9136 | True: -6806 | Predicted: -6806 | 
Question: 17+2967 | True: 2984 | Predicted: 2984 | 
Question: 9888-1351 | True: 8537 | Predicted: 8537 | 
Question: 9100-5838 | True: 3262 | Predicted: 3262 | 
Question: 1966-5876 | True: -3910 | Predicted: -3910 | 

Epoch 227/250:
Train Loss: 1.7582, Train Accuracy: 0.8202, Val Loss: 1.7496, Val Accuracy: 0.8568
Epoch 228/250:
Train Loss: 1.7594, Train Accuracy: 0.8142, Val Loss: 1.7502, Val Accuracy: 0.8545
Epoch 229/250:
Train Loss: 1.7589, Train Accuracy: 0.8172, Val Loss: 1.7487, Val Accuracy: 0.8600
Epoch 230/250:
Train Loss: 1.7586, Train Accuracy: 0.8188, Val Loss: 1.7518, Val Accuracy: 0.8542
Epoch 231/250:
Train Loss: 1.7603, Train Accuracy: 0.8136, Val Loss: 1.7508, Val Accuracy: 0.8520

Validation Examples:
Question: 782+3272 | True: 4054 | Predicted: 4054 | 
Question: 8611-7782 | True: 829 | Predicted: 8829 | 
Question: 273+8492 | True: 8765 | Predicted: 8765 | 
Question: 2524+730 | True: 3254 | Predicted: 3254 | 
Question: 4449-9204 | True: -4755 | Predicted: -4755 | 

Epoch 232/250:
Train Loss: 1.7590, Train Accuracy: 0.8180, Val Loss: 1.7498, Val Accuracy: 0.8610
Epoch 233/250:
Train Loss: 1.7604, Train Accuracy: 0.8143, Val Loss: 1.7547, Val Accuracy: 0.8460
Epoch 234/250:
Train Loss: 1.7594, Train Accuracy: 0.8169, Val Loss: 1.7502, Val Accuracy: 0.8572
Epoch 235/250:
Train Loss: 1.7598, Train Accuracy: 0.8161, Val Loss: 1.7490, Val Accuracy: 0.8585
Epoch 236/250:
Train Loss: 1.7582, Train Accuracy: 0.8201, Val Loss: 1.7494, Val Accuracy: 0.8605

Validation Examples:
Question: 7315+861 | True: 8176 | Predicted: 8176 | 
Question: 8041-2394 | True: 5647 | Predicted: 5647 | 
Question: 5335+9732 | True: 15067 | Predicted: 15067 | 
Question: 4774-2030 | True: 2744 | Predicted: 2744 | 
Question: 3074-4680 | True: -1606 | Predicted: -1606 | 

Epoch 237/250:
Train Loss: 1.7579, Train Accuracy: 0.8197, Val Loss: 1.7501, Val Accuracy: 0.8588
Epoch 238/250:
Train Loss: 1.7564, Train Accuracy: 0.8258, Val Loss: 1.7493, Val Accuracy: 0.8602
Epoch 239/250:
Train Loss: 1.7571, Train Accuracy: 0.8225, Val Loss: 1.7518, Val Accuracy: 0.8558
Epoch 240/250:
Train Loss: 1.7577, Train Accuracy: 0.8224, Val Loss: 1.7503, Val Accuracy: 0.8558
Epoch 241/250:
Train Loss: 1.7596, Train Accuracy: 0.8171, Val Loss: 1.7520, Val Accuracy: 0.8518

Validation Examples:
Question: 1510+2015 | True: 3525 | Predicted: 3525 | 
Question: 7642-4903 | True: 2739 | Predicted: 2739 | 
Question: 9504+4764 | True: 14268 | Predicted: 14268 | 
Question: 5112+8033 | True: 13145 | Predicted: 13145 | 
Question: 3916-6927 | True: -3011 | Predicted: -3011 | 

Epoch 242/250:
Train Loss: 1.7574, Train Accuracy: 0.8229, Val Loss: 1.7495, Val Accuracy: 0.8578
Epoch 243/250:
Train Loss: 1.7569, Train Accuracy: 0.8223, Val Loss: 1.7494, Val Accuracy: 0.8568
Epoch 244/250:
Train Loss: 1.7577, Train Accuracy: 0.8209, Val Loss: 1.7500, Val Accuracy: 0.8588
Epoch 245/250:
Train Loss: 1.7592, Train Accuracy: 0.8171, Val Loss: 1.7497, Val Accuracy: 0.8560
Epoch 246/250:
Train Loss: 1.7578, Train Accuracy: 0.8207, Val Loss: 1.7507, Val Accuracy: 0.8548

Validation Examples:
Question: 3002+4666 | True: 7668 | Predicted: 7668 | 
Question: 5911-8098 | True: -2187 | Predicted: -2187 | 
Question: 1476+587 | True: 2063 | Predicted: 2063 | 
Question: 4460-6176 | True: -1716 | Predicted: -1716 | 
Question: 3360+1906 | True: 5266 | Predicted: 5266 | 

Epoch 247/250:
Train Loss: 1.7599, Train Accuracy: 0.8159, Val Loss: 1.7492, Val Accuracy: 0.8575
Epoch 248/250:
Train Loss: 1.7570, Train Accuracy: 0.8226, Val Loss: 1.7485, Val Accuracy: 0.8612
Epoch 249/250:
Train Loss: 1.7565, Train Accuracy: 0.8241, Val Loss: 1.7481, Val Accuracy: 0.8608
Epoch 250/250:
Train Loss: 1.7573, Train Accuracy: 0.8224, Val Loss: 1.7495, Val Accuracy: 0.8585
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>Key Changes and Explanation:</p>
<p>Transformer Model:</p>
<p>Embedding Layer: Converts one-hot vectors to dense embeddings.</p>
<p>Positional Encoding: Adds positional information to the embeddings.positional encodings are crucial to incorporate sequence order which is not handled in the transformer's self attention.</p>
<p>Transformer Encoder: The core of the model, using multi-head self-attention to capture relationships between all positions at once, unlike the LSTM in your original model which was working sequentially. This makes the model way more powerful.</p>
<p>Output Layer: Projects the final transformer outputs back to the size of your character set.</p>
<p>Data generation and processing: remains largely the same.</p>
<p>Hyperparameter Tuning</p>
<p>Improved Attention: The multi-head self-attention allows for more nuanced understanding of the input tokens compared to the naive attention in the original code.</p>
<p>Parallel Processing: The transformer can be highly parallelized on GPU, which greatly speeds up training.</p>
<p>Better Representation: The Transformer excels at learning relationships between words/tokens (in this case characters), allowing it to better learn how to perform arithmetic.</p>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<hr/>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="Part-2:-A-language-translation-model-with-attention">Part 2: A language translation model with attention<a class="anchor-link" href="#Part-2:-A-language-translation-model-with-attention"></a></h2>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>In this part of the problem set we are going to implement a translation with a Sequence to Sequence Network and Attention model.</p>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<ol start="0">
<li>Please go over the NLP From Scratch: Translation with a Sequence to Sequence Network and Attention <a href="https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html">tutorial</a>. This attention model is very similar to what was learned in class (Luong), but a bit different. What are the main differences between  Badahnau and Luong attention mechanisms?</li>
</ol>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>Similarities to Luong Attention (Discussed in Class)</p>
<p>Goal: Both attention mechanisms share the primary goal of allowing the decoder to focus on the relevant parts of the input sequence.</p>
<p>Context Vector: Both calculate a context vector by taking a weighted sum of the encoder's outputs, where the weights reflect the alignment between the decoder's current state and the encoder's states.</p>
<p>Softmax Normalization: They use softmax to normalize the attention scores into probability distribution, where the attention weights sum to one.</p>
<p>How This Code Differs from What You Might've Seen in Class</p>
<p>Bahdanau: The code explicitly uses the Bahdanau attention mechanism which uses a single layer MLP to calculate the alignment scores. Whereas Luong attention may involve a dot product, general, or concat product to calculate the alignment scores.</p>
<p>GRU Instead of LSTM: The code uses GRUs instead of LSTMs for the RNNs, but both are valid in seq2seq models. The math is also similar, therefore not causing a fundamental difference in results.</p>
<p>Weight Calculation: Bahdanau attention combines the decoder and encoder hidden states to produce the attention weights before passing into the GRU cell in the decoder, while some forms of Luong attention (i.e. dot product form) may calculate weights after the GRU cell.</p>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>1.a) Using <code>!wget</code>, <code>!unzip</code> , download and extract the <a href="https://www.manythings.org/anki/">hebrew-english</a> sentence pairs text file to the Colab <code>content/</code>  folder (or local folder if not using Colab).
1.b) The <code>heb.txt</code> must be parsed and cleaned (see tutorial for requirements or change the code as you see fit).</p>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>2.a) Use the tutorial example to build  and train a Hebrew to English translation model with attention (using the parameters in the code cell below). Apply the same <code>eng_prefixes</code> filter to limit the train/test data.<br/>
2.b) Evaluate your trained model randomly on 20 sentences.<br/>
2.c) Show the attention plot for 5 random sentences.</p>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<ol start="3">
<li>Do you think this model performs well? Why or why not? What are its limitations/disadvantages? What would you do to improve it?</li>
</ol>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<ol start="4">
<li>Using any neural network architecture of your liking, build  a model with the aim to beat the model in 2.a. Compare your results in a meaningful way, and add a short explanation to why you think/thought your suggested network is better.</li>
</ol>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[3]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># use the following parameters:</span>
<span class="n">MAX_LENGTH</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">50</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>SOLUTION:</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[4]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1">### MISSING</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[5]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># In Colab:</span>
<span class="o">!</span>wget<span class="w"> </span>https://www.manythings.org/anki/heb-eng.zip
<span class="o">!</span>unzip<span class="w"> </span>heb-eng.zip<span class="w"> </span>-d<span class="w"> </span>data
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>--2025-02-02 13:54:27--  https://www.manythings.org/anki/heb-eng.zip
Resolving www.manythings.org (www.manythings.org)... 173.254.30.110
Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 4466359 (4.3M) [application/zip]
Saving to: heb-eng.zip

heb-eng.zip         100%[===================&gt;]   4.26M  2.29MB/s    in 1.9s    

2025-02-02 13:54:31 (2.29 MB/s) - heb-eng.zip saved [4466359/4466359]

Archive:  heb-eng.zip
  inflating: data/_about.txt         
  inflating: data/heb.txt            
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[6]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">unicode_literals</span><span class="p">,</span> <span class="n">print_function</span><span class="p">,</span> <span class="n">division</span>
<span class="kn">from</span> <span class="nn">io</span> <span class="kn">import</span> <span class="nb">open</span>
<span class="kn">import</span> <span class="nn">unicodedata</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">random</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">optim</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">TensorDataset</span><span class="p">,</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">RandomSampler</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[7]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">SOS_token</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">EOS_token</span> <span class="o">=</span> <span class="mi">1</span>

<span class="k">class</span> <span class="nc">Lang</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">name</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">word2index</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">word2count</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">index2word</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="s2">"SOS"</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="s2">"EOS"</span><span class="p">}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_words</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c1"># Count SOS and EOS</span>

    <span class="k">def</span> <span class="nf">addSentence</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sentence</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">sentence</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">' '</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">addWord</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">addWord</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">word</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">word2index</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">word2index</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_words</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">word2count</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">index2word</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">n_words</span><span class="p">]</span> <span class="o">=</span> <span class="n">word</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">n_words</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">word2count</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[8]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Turn a Unicode string to plain ASCII, thanks to</span>
<span class="c1"># https://stackoverflow.com/a/518232/2809427</span>
<span class="k">def</span> <span class="nf">unicodeToAscii</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
    <span class="k">return</span> <span class="s1">''</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
        <span class="n">c</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">unicodedata</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="s1">'NFD'</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">unicodedata</span><span class="o">.</span><span class="n">category</span><span class="p">(</span><span class="n">c</span><span class="p">)</span> <span class="o">!=</span> <span class="s1">'Mn'</span>
    <span class="p">)</span>

<span class="c1"># Lowercase, trim, and remove non-letter characters</span>
<span class="k">def</span> <span class="nf">normalizeString</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">unicodeToAscii</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">strip</span><span class="p">())</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s2">"([.!?])"</span><span class="p">,</span> <span class="sa">r</span><span class="s2">" \1"</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s2">"[^a-zA-Z!?]+"</span><span class="p">,</span> <span class="sa">r</span><span class="s2">" "</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">s</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[9]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">readLangs</span><span class="p">(</span><span class="n">lang1</span><span class="p">,</span> <span class="n">lang2</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Reading lines..."</span><span class="p">)</span>

    <span class="c1"># Read the file and split into lines</span>
    <span class="n">lines</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s1">'data/</span><span class="si">%s</span><span class="s1">.txt'</span> <span class="o">%</span> <span class="p">(</span><span class="n">lang2</span><span class="p">),</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">'utf-8'</span><span class="p">)</span><span class="o">.</span>\
        <span class="n">read</span><span class="p">()</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">'</span><span class="se">\n</span><span class="s1">'</span><span class="p">)</span>
    <span class="c1"># Split every line into pairs and normalize</span>
    <span class="n">pairs</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">lines</span><span class="p">:</span>
        <span class="n">parts</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">'</span><span class="se">\t</span><span class="s1">'</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">parts</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">2</span><span class="p">:</span>  <span class="c1"># Ensure the line has at least two parts</span>
            <span class="n">eng</span> <span class="o">=</span> <span class="n">normalizeString</span><span class="p">(</span><span class="n">parts</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>  <span class="c1"># Normalize the English sentence</span>
            <span class="n">heb</span> <span class="o">=</span> <span class="n">parts</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>  <span class="c1"># Keep the Hebrew sentence as is</span>
            <span class="n">pairs</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">eng</span><span class="p">,</span> <span class="n">heb</span><span class="p">))</span>





    <span class="c1"># Reverse pairs, make Lang instances</span>
    <span class="k">if</span> <span class="n">reverse</span><span class="p">:</span>
        <span class="n">pairs</span> <span class="o">=</span> <span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">p</span><span class="p">))</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">pairs</span><span class="p">]</span>
        <span class="n">input_lang</span> <span class="o">=</span> <span class="n">Lang</span><span class="p">(</span><span class="n">lang2</span><span class="p">)</span>
        <span class="n">output_lang</span> <span class="o">=</span> <span class="n">Lang</span><span class="p">(</span><span class="n">lang1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">input_lang</span> <span class="o">=</span> <span class="n">Lang</span><span class="p">(</span><span class="n">lang1</span><span class="p">)</span>
        <span class="n">output_lang</span> <span class="o">=</span> <span class="n">Lang</span><span class="p">(</span><span class="n">lang2</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">input_lang</span><span class="p">,</span> <span class="n">output_lang</span><span class="p">,</span> <span class="n">pairs</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[10]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">MAX_LENGTH</span> <span class="o">=</span> <span class="mi">10</span>

<span class="n">eng_prefixes</span> <span class="o">=</span> <span class="p">(</span>
    <span class="s2">"i am "</span><span class="p">,</span> <span class="s2">"i m "</span><span class="p">,</span>
    <span class="s2">"he is"</span><span class="p">,</span> <span class="s2">"he s "</span><span class="p">,</span>
    <span class="s2">"she is"</span><span class="p">,</span> <span class="s2">"she s "</span><span class="p">,</span>
    <span class="s2">"you are"</span><span class="p">,</span> <span class="s2">"you re "</span><span class="p">,</span>
    <span class="s2">"we are"</span><span class="p">,</span> <span class="s2">"we re "</span><span class="p">,</span>
    <span class="s2">"they are"</span><span class="p">,</span> <span class="s2">"they re "</span>
<span class="p">)</span>

<span class="k">def</span> <span class="nf">filterPair</span><span class="p">(</span><span class="n">p</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">' '</span><span class="p">))</span> <span class="o">&lt;</span> <span class="n">MAX_LENGTH</span> <span class="ow">and</span> \
        <span class="nb">len</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">' '</span><span class="p">))</span> <span class="o">&lt;</span> <span class="n">MAX_LENGTH</span> <span class="ow">and</span> \
        <span class="n">p</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="n">eng_prefixes</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">filterPairs</span><span class="p">(</span><span class="n">pairs</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">pair</span> <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">pairs</span> <span class="k">if</span> <span class="n">filterPair</span><span class="p">(</span><span class="n">pair</span><span class="p">)]</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[11]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">prepareData</span><span class="p">(</span><span class="n">lang1</span><span class="p">,</span> <span class="n">lang2</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="n">input_lang</span><span class="p">,</span> <span class="n">output_lang</span><span class="p">,</span> <span class="n">pairs</span> <span class="o">=</span> <span class="n">readLangs</span><span class="p">(</span><span class="n">lang1</span><span class="p">,</span> <span class="n">lang2</span><span class="p">,</span> <span class="n">reverse</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Read </span><span class="si">%s</span><span class="s2"> sentence pairs"</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">pairs</span><span class="p">))</span>
    <span class="n">pairs</span> <span class="o">=</span> <span class="n">filterPairs</span><span class="p">(</span><span class="n">pairs</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Trimmed to </span><span class="si">%s</span><span class="s2"> sentence pairs"</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">pairs</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Counting words..."</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">pairs</span><span class="p">:</span>
        <span class="n">input_lang</span><span class="o">.</span><span class="n">addSentence</span><span class="p">(</span><span class="n">pair</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">output_lang</span><span class="o">.</span><span class="n">addSentence</span><span class="p">(</span><span class="n">pair</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Counted words:"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">input_lang</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">input_lang</span><span class="o">.</span><span class="n">n_words</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">output_lang</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">output_lang</span><span class="o">.</span><span class="n">n_words</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">input_lang</span><span class="p">,</span> <span class="n">output_lang</span><span class="p">,</span> <span class="n">pairs</span>

<span class="n">input_lang</span><span class="p">,</span> <span class="n">output_lang</span><span class="p">,</span> <span class="n">pairs</span> <span class="o">=</span> <span class="n">prepareData</span><span class="p">(</span><span class="s1">'eng'</span><span class="p">,</span> <span class="s1">'heb'</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">pairs</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Reading lines...
Read 128133 sentence pairs
Trimmed to 9320 sentence pairs
Counting words...
Counted words:
heb 7519
eng 3067
[' .', 'they re gorgeous']
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[11]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span> 
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[12]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">EncoderRNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">EncoderRNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gru</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_p</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="n">embedded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
        <span class="n">output</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gru</span><span class="p">(</span><span class="n">embedded</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">hidden</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[13]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">DecoderRNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DecoderRNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gru</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">,</span> <span class="n">encoder_hidden</span><span class="p">,</span> <span class="n">target_tensor</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">encoder_outputs</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">decoder_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="n">SOS_token</span><span class="p">)</span>
        <span class="n">decoder_hidden</span> <span class="o">=</span> <span class="n">encoder_hidden</span>
        <span class="n">decoder_outputs</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">MAX_LENGTH</span><span class="p">):</span>
            <span class="n">decoder_output</span><span class="p">,</span> <span class="n">decoder_hidden</span>  <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward_step</span><span class="p">(</span><span class="n">decoder_input</span><span class="p">,</span> <span class="n">decoder_hidden</span><span class="p">)</span>
            <span class="n">decoder_outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">decoder_output</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">target_tensor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># Teacher forcing: Feed the target as the next input</span>
                <span class="n">decoder_input</span> <span class="o">=</span> <span class="n">target_tensor</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># Teacher forcing</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Without teacher forcing: use its own predictions as the next input</span>
                <span class="n">_</span><span class="p">,</span> <span class="n">topi</span> <span class="o">=</span> <span class="n">decoder_output</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">decoder_input</span> <span class="o">=</span> <span class="n">topi</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>  <span class="c1"># detach from history as input</span>

        <span class="n">decoder_outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">decoder_outputs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">decoder_outputs</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">decoder_outputs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">decoder_outputs</span><span class="p">,</span> <span class="n">decoder_hidden</span><span class="p">,</span> <span class="kc">None</span> <span class="c1"># We return `None` for consistency in the training loop</span>

    <span class="k">def</span> <span class="nf">forward_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">hidden</span><span class="p">):</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
        <span class="n">output</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gru</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">hidden</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">hidden</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[14]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">BahdanauAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">BahdanauAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Wa</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Ua</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Va</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">keys</span><span class="p">):</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Va</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Wa</span><span class="p">(</span><span class="n">query</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">Ua</span><span class="p">(</span><span class="n">keys</span><span class="p">)))</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">weights</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">context</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">keys</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">context</span><span class="p">,</span> <span class="n">weights</span>

<span class="k">class</span> <span class="nc">AttnDecoderRNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">AttnDecoderRNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">BahdanauAttention</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gru</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_p</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">,</span> <span class="n">encoder_hidden</span><span class="p">,</span> <span class="n">target_tensor</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">encoder_outputs</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">decoder_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="n">SOS_token</span><span class="p">)</span>
        <span class="n">decoder_hidden</span> <span class="o">=</span> <span class="n">encoder_hidden</span>
        <span class="n">decoder_outputs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">attentions</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">MAX_LENGTH</span><span class="p">):</span>
            <span class="n">decoder_output</span><span class="p">,</span> <span class="n">decoder_hidden</span><span class="p">,</span> <span class="n">attn_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward_step</span><span class="p">(</span>
                <span class="n">decoder_input</span><span class="p">,</span> <span class="n">decoder_hidden</span><span class="p">,</span> <span class="n">encoder_outputs</span>
            <span class="p">)</span>
            <span class="n">decoder_outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">decoder_output</span><span class="p">)</span>
            <span class="n">attentions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">target_tensor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># Teacher forcing: Feed the target as the next input</span>
                <span class="n">decoder_input</span> <span class="o">=</span> <span class="n">target_tensor</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># Teacher forcing</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Without teacher forcing: use its own predictions as the next input</span>
                <span class="n">_</span><span class="p">,</span> <span class="n">topi</span> <span class="o">=</span> <span class="n">decoder_output</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">decoder_input</span> <span class="o">=</span> <span class="n">topi</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>  <span class="c1"># detach from history as input</span>

        <span class="n">decoder_outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">decoder_outputs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">decoder_outputs</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">decoder_outputs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">attentions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">attentions</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">decoder_outputs</span><span class="p">,</span> <span class="n">decoder_hidden</span><span class="p">,</span> <span class="n">attentions</span>


    <span class="k">def</span> <span class="nf">forward_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">hidden</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">):</span>
        <span class="n">embedded</span> <span class="o">=</span>  <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>

        <span class="n">query</span> <span class="o">=</span> <span class="n">hidden</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">context</span><span class="p">,</span> <span class="n">attn_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">)</span>
        <span class="n">input_gru</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">embedded</span><span class="p">,</span> <span class="n">context</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

        <span class="n">output</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gru</span><span class="p">(</span><span class="n">input_gru</span><span class="p">,</span> <span class="n">hidden</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">hidden</span><span class="p">,</span> <span class="n">attn_weights</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[15]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">indexesFromSentence</span><span class="p">(</span><span class="n">lang</span><span class="p">,</span> <span class="n">sentence</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">lang</span><span class="o">.</span><span class="n">word2index</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">sentence</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">' '</span><span class="p">)]</span>

<span class="k">def</span> <span class="nf">tensorFromSentence</span><span class="p">(</span><span class="n">lang</span><span class="p">,</span> <span class="n">sentence</span><span class="p">):</span>
    <span class="n">indexes</span> <span class="o">=</span> <span class="n">indexesFromSentence</span><span class="p">(</span><span class="n">lang</span><span class="p">,</span> <span class="n">sentence</span><span class="p">)</span>
    <span class="n">indexes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">EOS_token</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">indexes</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">tensorsFromPair</span><span class="p">(</span><span class="n">pair</span><span class="p">):</span>
    <span class="n">input_tensor</span> <span class="o">=</span> <span class="n">tensorFromSentence</span><span class="p">(</span><span class="n">input_lang</span><span class="p">,</span> <span class="n">pair</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">target_tensor</span> <span class="o">=</span> <span class="n">tensorFromSentence</span><span class="p">(</span><span class="n">output_lang</span><span class="p">,</span> <span class="n">pair</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">target_tensor</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">get_dataloader</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
    <span class="n">input_lang</span><span class="p">,</span> <span class="n">output_lang</span><span class="p">,</span> <span class="n">pairs</span> <span class="o">=</span> <span class="n">prepareData</span><span class="p">(</span><span class="s1">'eng'</span><span class="p">,</span> <span class="s1">'heb'</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pairs</span><span class="p">)</span>
    <span class="n">input_ids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="n">MAX_LENGTH</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">target_ids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="n">MAX_LENGTH</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">tgt</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">pairs</span><span class="p">):</span>
        <span class="n">inp_ids</span> <span class="o">=</span> <span class="n">indexesFromSentence</span><span class="p">(</span><span class="n">input_lang</span><span class="p">,</span> <span class="n">inp</span><span class="p">)</span>
        <span class="n">tgt_ids</span> <span class="o">=</span> <span class="n">indexesFromSentence</span><span class="p">(</span><span class="n">output_lang</span><span class="p">,</span> <span class="n">tgt</span><span class="p">)</span>
        <span class="n">inp_ids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">EOS_token</span><span class="p">)</span>
        <span class="n">tgt_ids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">EOS_token</span><span class="p">)</span>
        <span class="n">input_ids</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="p">:</span><span class="nb">len</span><span class="p">(</span><span class="n">inp_ids</span><span class="p">)]</span> <span class="o">=</span> <span class="n">inp_ids</span>
        <span class="n">target_ids</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="p">:</span><span class="nb">len</span><span class="p">(</span><span class="n">tgt_ids</span><span class="p">)]</span> <span class="o">=</span> <span class="n">tgt_ids</span>

    <span class="n">train_data</span> <span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
                               <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">target_ids</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>

    <span class="n">train_sampler</span> <span class="o">=</span> <span class="n">RandomSampler</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span>
    <span class="n">train_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">train_sampler</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">input_lang</span><span class="p">,</span> <span class="n">output_lang</span><span class="p">,</span> <span class="n">train_dataloader</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[16]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">train_epoch</span><span class="p">(</span><span class="n">dataloader</span><span class="p">,</span> <span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">encoder_optimizer</span><span class="p">,</span>
          <span class="n">decoder_optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">):</span>

    <span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
        <span class="n">input_tensor</span><span class="p">,</span> <span class="n">target_tensor</span> <span class="o">=</span> <span class="n">data</span>

        <span class="n">encoder_optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">decoder_optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="n">encoder_outputs</span><span class="p">,</span> <span class="n">encoder_hidden</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span>
        <span class="n">decoder_outputs</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">decoder</span><span class="p">(</span><span class="n">encoder_outputs</span><span class="p">,</span> <span class="n">encoder_hidden</span><span class="p">,</span> <span class="n">target_tensor</span><span class="p">)</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span>
            <span class="n">decoder_outputs</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">decoder_outputs</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)),</span>
            <span class="n">target_tensor</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

        <span class="n">encoder_optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">decoder_optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">total_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataloader</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[17]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">math</span>

<span class="k">def</span> <span class="nf">asMinutes</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">s</span> <span class="o">/</span> <span class="mi">60</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">-=</span> <span class="n">m</span> <span class="o">*</span> <span class="mi">60</span>
    <span class="k">return</span> <span class="s1">'</span><span class="si">%d</span><span class="s1">m </span><span class="si">%d</span><span class="s1">s'</span> <span class="o">%</span> <span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">timeSince</span><span class="p">(</span><span class="n">since</span><span class="p">,</span> <span class="n">percent</span><span class="p">):</span>
    <span class="n">now</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">now</span> <span class="o">-</span> <span class="n">since</span>
    <span class="n">es</span> <span class="o">=</span> <span class="n">s</span> <span class="o">/</span> <span class="p">(</span><span class="n">percent</span><span class="p">)</span>
    <span class="n">rs</span> <span class="o">=</span> <span class="n">es</span> <span class="o">-</span> <span class="n">s</span>
    <span class="k">return</span> <span class="s1">'</span><span class="si">%s</span><span class="s1"> (- </span><span class="si">%s</span><span class="s1">)'</span> <span class="o">%</span> <span class="p">(</span><span class="n">asMinutes</span><span class="p">(</span><span class="n">s</span><span class="p">),</span> <span class="n">asMinutes</span><span class="p">(</span><span class="n">rs</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[18]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">,</span> <span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">n_epochs</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span>
               <span class="n">print_every</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">plot_every</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">plot_losses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">print_loss_total</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># Reset every print_every</span>
    <span class="n">plot_loss_total</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># Reset every plot_every</span>

    <span class="n">encoder_optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">encoder</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
    <span class="n">decoder_optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">decoder</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_epochs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">train_epoch</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">,</span> <span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">encoder_optimizer</span><span class="p">,</span> <span class="n">decoder_optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">)</span>
        <span class="n">print_loss_total</span> <span class="o">+=</span> <span class="n">loss</span>
        <span class="n">plot_loss_total</span> <span class="o">+=</span> <span class="n">loss</span>

        <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="n">print_every</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">print_loss_avg</span> <span class="o">=</span> <span class="n">print_loss_total</span> <span class="o">/</span> <span class="n">print_every</span>
            <span class="n">print_loss_total</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">'</span><span class="si">%s</span><span class="s1"> (</span><span class="si">%d</span><span class="s1"> </span><span class="si">%d%%</span><span class="s1">) </span><span class="si">%.4f</span><span class="s1">'</span> <span class="o">%</span> <span class="p">(</span><span class="n">timeSince</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">epoch</span> <span class="o">/</span> <span class="n">n_epochs</span><span class="p">),</span>
                                        <span class="n">epoch</span><span class="p">,</span> <span class="n">epoch</span> <span class="o">/</span> <span class="n">n_epochs</span> <span class="o">*</span> <span class="mi">100</span><span class="p">,</span> <span class="n">print_loss_avg</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="n">plot_every</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">plot_loss_avg</span> <span class="o">=</span> <span class="n">plot_loss_total</span> <span class="o">/</span> <span class="n">plot_every</span>
            <span class="n">plot_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">plot_loss_avg</span><span class="p">)</span>
            <span class="n">plot_loss_total</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="n">showPlot</span><span class="p">(</span><span class="n">plot_losses</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[19]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">switch_backend</span><span class="p">(</span><span class="s1">'agg'</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">matplotlib.ticker</span> <span class="k">as</span> <span class="nn">ticker</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">showPlot</span><span class="p">(</span><span class="n">points</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
    <span class="c1"># this locator puts ticks at regular intervals</span>
    <span class="n">loc</span> <span class="o">=</span> <span class="n">ticker</span><span class="o">.</span><span class="n">MultipleLocator</span><span class="p">(</span><span class="n">base</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_major_locator</span><span class="p">(</span><span class="n">loc</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">points</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[20]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">sentence</span><span class="p">,</span> <span class="n">input_lang</span><span class="p">,</span> <span class="n">output_lang</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">input_tensor</span> <span class="o">=</span> <span class="n">tensorFromSentence</span><span class="p">(</span><span class="n">input_lang</span><span class="p">,</span> <span class="n">sentence</span><span class="p">)</span>

        <span class="n">encoder_outputs</span><span class="p">,</span> <span class="n">encoder_hidden</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span>
        <span class="n">decoder_outputs</span><span class="p">,</span> <span class="n">decoder_hidden</span><span class="p">,</span> <span class="n">decoder_attn</span> <span class="o">=</span> <span class="n">decoder</span><span class="p">(</span><span class="n">encoder_outputs</span><span class="p">,</span> <span class="n">encoder_hidden</span><span class="p">)</span>

        <span class="n">_</span><span class="p">,</span> <span class="n">topi</span> <span class="o">=</span> <span class="n">decoder_outputs</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">decoded_ids</span> <span class="o">=</span> <span class="n">topi</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>

        <span class="n">decoded_words</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">decoded_ids</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">idx</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">==</span> <span class="n">EOS_token</span><span class="p">:</span>
                <span class="n">decoded_words</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s1">'&lt;EOS&gt;'</span><span class="p">)</span>
                <span class="k">break</span>
            <span class="n">decoded_words</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">output_lang</span><span class="o">.</span><span class="n">index2word</span><span class="p">[</span><span class="n">idx</span><span class="o">.</span><span class="n">item</span><span class="p">()])</span>
    <span class="k">return</span> <span class="n">decoded_words</span><span class="p">,</span> <span class="n">decoder_attn</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[21]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">evaluateRandomly</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">pair</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">pairs</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">'&gt;'</span><span class="p">,</span> <span class="n">pair</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">'='</span><span class="p">,</span> <span class="n">pair</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">output_words</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">pair</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">input_lang</span><span class="p">,</span> <span class="n">output_lang</span><span class="p">)</span>
        <span class="n">output_sentence</span> <span class="o">=</span> <span class="s1">' '</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">output_words</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">'&lt;'</span><span class="p">,</span> <span class="n">output_sentence</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">''</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[22]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>

<span class="n">input_lang</span><span class="p">,</span> <span class="n">output_lang</span><span class="p">,</span> <span class="n">train_dataloader</span> <span class="o">=</span> <span class="n">get_dataloader</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>

<span class="n">encoder</span> <span class="o">=</span> <span class="n">EncoderRNN</span><span class="p">(</span><span class="n">input_lang</span><span class="o">.</span><span class="n">n_words</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">decoder</span> <span class="o">=</span> <span class="n">AttnDecoderRNN</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_lang</span><span class="o">.</span><span class="n">n_words</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">train</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">,</span> <span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="mi">80</span><span class="p">,</span> <span class="n">print_every</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">plot_every</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Reading lines...
Read 128133 sentence pairs
Trimmed to 9320 sentence pairs
Counting words...
Counted words:
heb 7519
eng 3067
0m 37s (- 9m 26s) (5 6%) 1.7979
1m 9s (- 8m 6s) (10 12%) 0.9870
1m 41s (- 7m 18s) (15 18%) 0.6160
2m 13s (- 6m 39s) (20 25%) 0.3841
2m 44s (- 6m 2s) (25 31%) 0.2363
3m 16s (- 5m 27s) (30 37%) 0.1482
3m 48s (- 4m 54s) (35 43%) 0.0975
4m 20s (- 4m 20s) (40 50%) 0.0719
4m 52s (- 3m 47s) (45 56%) 0.0569
5m 23s (- 3m 14s) (50 62%) 0.0470
5m 55s (- 2m 41s) (55 68%) 0.0410
6m 27s (- 2m 9s) (60 75%) 0.0363
6m 59s (- 1m 36s) (65 81%) 0.0333
7m 31s (- 1m 4s) (70 87%) 0.0310
8m 2s (- 0m 32s) (75 93%) 0.0286
8m 34s (- 0m 0s) (80 100%) 0.0278
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[23]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">encoder</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">decoder</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">evaluateRandomly</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>&gt;    .
= he is in good physical condition
&lt; he is in good physical condition &lt;EOS&gt;

&gt;       .
= i m sure tom regrets that now
&lt; i m sure tom regrets that now &lt;EOS&gt;

&gt;        .
= you re now old enough to support yourself
&lt; you are now old enough to support yourself &lt;EOS&gt;

&gt;  .
= i m back from vacation
&lt; i m back from vacation &lt;EOS&gt;

&gt;    .
= i m sure tom is on his way
&lt; tom s sure his custom motorcycle motorcycle their defenseless buried

&gt;  .
= i m upset
&lt; i m troubled &lt;EOS&gt;

&gt;       .
= i m not able to translate this sentence
&lt; i m unable to translate this sentence &lt;EOS&gt;

&gt;  .
= i m outraged
&lt; i m outraged &lt;EOS&gt;

&gt;    .
= he is the father of two children
&lt; he is the father of two children french fries &lt;EOS&gt;

&gt;     .
= he is about my age
&lt; he is about my age &lt;EOS&gt;

</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[31]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">showAttention</span><span class="p">(</span><span class="n">input_sentence</span><span class="p">,</span> <span class="n">output_words</span><span class="p">,</span> <span class="n">attentions</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">attentions</span><span class="p">):</span>
        <span class="n">attention_data</span> <span class="o">=</span> <span class="n">attentions</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">attention_data</span> <span class="o">=</span> <span class="n">attentions</span>

    <span class="c1"># Flip the attention matrix horizontally for RTL text</span>
    <span class="n">attention_data</span> <span class="o">=</span> <span class="n">attention_data</span><span class="p">[:,</span> <span class="p">::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="n">cax</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">matshow</span><span class="p">(</span><span class="n">attention_data</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'bone'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">cax</span><span class="p">)</span>

    <span class="c1"># Reverse the order of input labels for RTL</span>
    <span class="n">input_labels</span> <span class="o">=</span> <span class="p">[</span><span class="s1">''</span><span class="p">]</span> <span class="o">+</span> <span class="n">input_sentence</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">' '</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="s1">'&lt;EOS&gt;'</span><span class="p">]</span>
    <span class="n">output_labels</span> <span class="o">=</span> <span class="p">[</span><span class="s1">''</span><span class="p">]</span> <span class="o">+</span> <span class="n">output_words</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">input_labels</span><span class="p">)))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">output_labels</span><span class="p">)))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">input_labels</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="n">output_labels</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="n">block</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">evaluateAndShowAttention</span><span class="p">(</span><span class="n">input_sentence</span><span class="p">):</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">output_words</span><span class="p">,</span> <span class="n">attentions</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">input_sentence</span><span class="p">,</span> <span class="n">input_lang</span><span class="p">,</span> <span class="n">output_lang</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">'Input:'</span><span class="p">,</span> <span class="n">input_sentence</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">'Output:'</span><span class="p">,</span> <span class="s1">' '</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">output_words</span><span class="p">))</span>
        <span class="n">showAttention</span><span class="p">(</span><span class="n">input_sentence</span><span class="p">,</span> <span class="n">output_words</span><span class="p">,</span> <span class="n">attentions</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:</span><span class="nb">len</span><span class="p">(</span><span class="n">output_words</span><span class="p">),</span> <span class="p">:])</span>
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Error occurred: </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">evaluateAndShowAttention</span><span class="p">(</span><span class="s1">'  .'</span><span class="p">)</span>

<span class="n">evaluateAndShowAttention</span><span class="p">(</span><span class="s1">'  .'</span><span class="p">)</span>

<span class="n">evaluateAndShowAttention</span><span class="p">(</span><span class="s1">'  .'</span><span class="p">)</span>

<span class="n">evaluateAndShowAttention</span><span class="p">(</span><span class="s1">' .'</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Input:   .
Output: i m not an expert mother &lt;EOS&gt;
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedImage jp-OutputArea-output" tabindex="0">
<img alt="No description has been provided for this image" class="" src="data:image/png;base64,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
"/>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Input:   .
Output: she is a pleasant person a rose &lt;EOS&gt;
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedImage jp-OutputArea-output" tabindex="0">
<img alt="No description has been provided for this image" class="" src="data:image/png;base64,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
"/>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Input:   .
Output: he s outraged something &lt;EOS&gt;
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedImage jp-OutputArea-output" tabindex="0">
<img alt="No description has been provided for this image" class="" src="data:image/png;base64,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
"/>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Input:  .
Output: you re energetic &lt;EOS&gt;
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedImage jp-OutputArea-output" tabindex="0">
<img alt="No description has been provided for this image" class="" src="data:image/png;base64,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
"/>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[27]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="o">%</span><span class="k">matplotlib</span> inline
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[23]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">TensorDataset</span><span class="p">,</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">RandomSampler</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">from</span> <span class="nn">nltk.translate.bleu_score</span> <span class="kn">import</span> <span class="n">sentence_bleu</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">matplotlib.ticker</span> <span class="k">as</span> <span class="nn">ticker</span>

<span class="c1"># Constants</span>
<span class="n">SOS_token</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">EOS_token</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">MAX_LENGTH</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>

<span class="c1"># Data preparation functions</span>
<span class="k">def</span> <span class="nf">get_dataloader</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">):</span>
    <span class="n">input_lang</span><span class="p">,</span> <span class="n">output_lang</span><span class="p">,</span> <span class="n">pairs</span> <span class="o">=</span> <span class="n">prepareData</span><span class="p">(</span><span class="s1">'eng'</span><span class="p">,</span> <span class="s1">'heb'</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pairs</span><span class="p">)</span>
    <span class="n">input_ids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="n">MAX_LENGTH</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">target_ids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="n">MAX_LENGTH</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">tgt</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">pairs</span><span class="p">):</span>
        <span class="n">inp_ids</span> <span class="o">=</span> <span class="n">indexesFromSentence</span><span class="p">(</span><span class="n">input_lang</span><span class="p">,</span> <span class="n">inp</span><span class="p">)</span>
        <span class="n">tgt_ids</span> <span class="o">=</span> <span class="n">indexesFromSentence</span><span class="p">(</span><span class="n">output_lang</span><span class="p">,</span> <span class="n">tgt</span><span class="p">)</span>
        <span class="n">inp_ids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">EOS_token</span><span class="p">)</span>
        <span class="n">tgt_ids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">EOS_token</span><span class="p">)</span>
        <span class="n">input_ids</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="p">:</span><span class="nb">len</span><span class="p">(</span><span class="n">inp_ids</span><span class="p">)]</span> <span class="o">=</span> <span class="n">inp_ids</span>
        <span class="n">target_ids</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="p">:</span><span class="nb">len</span><span class="p">(</span><span class="n">tgt_ids</span><span class="p">)]</span> <span class="o">=</span> <span class="n">tgt_ids</span>

    <span class="n">train_data</span> <span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
                              <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">target_ids</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>

    <span class="n">train_sampler</span> <span class="o">=</span> <span class="n">RandomSampler</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span>
    <span class="n">train_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">train_sampler</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">input_lang</span><span class="p">,</span> <span class="n">output_lang</span><span class="p">,</span> <span class="n">train_dataloader</span><span class="p">,</span> <span class="n">pairs</span>

<span class="c1"># Model definitions (same as before)</span>
<span class="k">class</span> <span class="nc">ImprovedEncoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">n_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ImprovedEncoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_layers</span> <span class="o">=</span> <span class="n">n_layers</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_p</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">gru</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span>
            <span class="n">hidden_size</span><span class="p">,</span>
            <span class="n">hidden_size</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span>
            <span class="n">num_layers</span><span class="o">=</span><span class="n">n_layers</span><span class="p">,</span>
            <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">bidirectional</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">dropout</span><span class="o">=</span><span class="n">dropout_p</span> <span class="k">if</span> <span class="n">n_layers</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="mi">0</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="n">embedded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
        <span class="n">embedded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">embedded</span><span class="p">)</span>

        <span class="n">output</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gru</span><span class="p">(</span><span class="n">embedded</span><span class="p">)</span>
        <span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reshape_hidden</span><span class="p">(</span><span class="n">hidden</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">hidden</span>

    <span class="k">def</span> <span class="nf">_reshape_hidden</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden</span><span class="p">):</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">hidden</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">hidden</span> <span class="o">=</span> <span class="n">hidden</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_layers</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="o">//</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">hidden</span> <span class="o">=</span> <span class="n">hidden</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
        <span class="n">hidden</span> <span class="o">=</span> <span class="n">hidden</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_layers</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">hidden</span>

<span class="k">class</span> <span class="nc">ScaledDotProductAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ScaledDotProductAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">([</span><span class="n">hidden_size</span><span class="p">]))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span>

        <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1e10</span><span class="p">)</span>

        <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">context</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attention_weights</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">context</span><span class="p">,</span> <span class="n">attention_weights</span>

<span class="k">class</span> <span class="nc">ImprovedDecoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">n_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ImprovedDecoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span> <span class="o">=</span> <span class="n">output_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_layers</span> <span class="o">=</span> <span class="n">n_layers</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">ScaledDotProductAttention</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">gru</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span>
            <span class="mi">2</span> <span class="o">*</span> <span class="n">hidden_size</span><span class="p">,</span>
            <span class="n">hidden_size</span><span class="p">,</span>
            <span class="n">num_layers</span><span class="o">=</span><span class="n">n_layers</span><span class="p">,</span>
            <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">dropout</span><span class="o">=</span><span class="n">dropout_p</span> <span class="k">if</span> <span class="n">n_layers</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="mi">0</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">out</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_p</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">,</span> <span class="n">encoder_hidden</span><span class="p">,</span> <span class="n">target_tensor</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">encoder_outputs</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">decoder_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="n">SOS_token</span><span class="p">)</span>
        <span class="n">decoder_hidden</span> <span class="o">=</span> <span class="n">encoder_hidden</span>
        <span class="n">decoder_outputs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">attentions</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="n">prev_context</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">MAX_LENGTH</span><span class="p">):</span>
            <span class="n">decoder_output</span><span class="p">,</span> <span class="n">decoder_hidden</span><span class="p">,</span> <span class="n">attn_weights</span><span class="p">,</span> <span class="n">context</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward_step</span><span class="p">(</span>
                <span class="n">decoder_input</span><span class="p">,</span> <span class="n">decoder_hidden</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">,</span> <span class="n">prev_context</span>
            <span class="p">)</span>
            <span class="n">prev_context</span> <span class="o">=</span> <span class="n">context</span>

            <span class="n">decoder_outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">decoder_output</span><span class="p">)</span>
            <span class="n">attentions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">target_tensor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">decoder_input</span> <span class="o">=</span> <span class="n">target_tensor</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">_</span><span class="p">,</span> <span class="n">topi</span> <span class="o">=</span> <span class="n">decoder_output</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">decoder_input</span> <span class="o">=</span> <span class="n">topi</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>

                <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="n">di</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">==</span> <span class="n">EOS_token</span> <span class="k">for</span> <span class="n">di</span> <span class="ow">in</span> <span class="n">decoder_input</span><span class="p">):</span>
                    <span class="k">break</span>

        <span class="n">decoder_outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">decoder_outputs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">decoder_outputs</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">decoder_outputs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">attentions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">attentions</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">decoder_outputs</span><span class="p">,</span> <span class="n">decoder_hidden</span><span class="p">,</span> <span class="n">attentions</span>

    <span class="k">def</span> <span class="nf">forward_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">hidden</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">,</span> <span class="n">prev_context</span><span class="p">):</span>
        <span class="n">embedded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
        <span class="n">embedded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">embedded</span><span class="p">)</span>

        <span class="n">rnn_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">embedded</span><span class="p">,</span> <span class="n">prev_context</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

        <span class="n">output</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gru</span><span class="p">(</span><span class="n">rnn_input</span><span class="p">,</span> <span class="n">hidden</span><span class="p">)</span>

        <span class="n">context</span><span class="p">,</span> <span class="n">attn_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">)</span>

        <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">output</span><span class="p">,</span> <span class="n">context</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">hidden</span><span class="p">,</span> <span class="n">attn_weights</span><span class="p">,</span> <span class="n">context</span>

<span class="c1"># Training function</span>
<span class="k">def</span> <span class="nf">train_improved</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">,</span> <span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">n_epochs</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span>
                  <span class="n">print_every</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">plot_every</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">clip</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">plot_losses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">print_loss_total</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">plot_loss_total</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="n">encoder_optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">encoder</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
    <span class="n">decoder_optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">decoder</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>

    <span class="n">encoder_scheduler</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">ReduceLROnPlateau</span><span class="p">(</span><span class="n">encoder_optimizer</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">'min'</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">decoder_scheduler</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">ReduceLROnPlateau</span><span class="p">(</span><span class="n">decoder_optimizer</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">'min'</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">(</span><span class="n">ignore_index</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_epochs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">epoch_loss</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">train_dataloader</span><span class="p">:</span>
            <span class="n">input_tensor</span><span class="p">,</span> <span class="n">target_tensor</span> <span class="o">=</span> <span class="n">data</span>

            <span class="n">encoder_optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">decoder_optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

            <span class="n">encoder_outputs</span><span class="p">,</span> <span class="n">encoder_hidden</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span>
            <span class="n">decoder_outputs</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">decoder</span><span class="p">(</span><span class="n">encoder_outputs</span><span class="p">,</span> <span class="n">encoder_hidden</span><span class="p">,</span> <span class="n">target_tensor</span><span class="p">)</span>

            <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span>
                <span class="n">decoder_outputs</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">decoder_outputs</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)),</span>
                <span class="n">target_tensor</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="p">)</span>

            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">encoder</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">clip</span><span class="p">)</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">decoder</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">clip</span><span class="p">)</span>

            <span class="n">encoder_optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            <span class="n">decoder_optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

            <span class="n">epoch_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

        <span class="n">avg_loss</span> <span class="o">=</span> <span class="n">epoch_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">)</span>

        <span class="n">encoder_scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">avg_loss</span><span class="p">)</span>
        <span class="n">decoder_scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">avg_loss</span><span class="p">)</span>

        <span class="n">print_loss_total</span> <span class="o">+=</span> <span class="n">avg_loss</span>
        <span class="n">plot_loss_total</span> <span class="o">+=</span> <span class="n">avg_loss</span>

        <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="n">print_every</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">print_loss_avg</span> <span class="o">=</span> <span class="n">print_loss_total</span> <span class="o">/</span> <span class="n">print_every</span>
            <span class="n">print_loss_total</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">'</span><span class="si">%s</span><span class="s1"> (</span><span class="si">%d</span><span class="s1"> </span><span class="si">%d%%</span><span class="s1">) </span><span class="si">%.4f</span><span class="s1">'</span> <span class="o">%</span> <span class="p">(</span><span class="n">timeSince</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">epoch</span> <span class="o">/</span> <span class="n">n_epochs</span><span class="p">),</span>
                                        <span class="n">epoch</span><span class="p">,</span> <span class="n">epoch</span> <span class="o">/</span> <span class="n">n_epochs</span> <span class="o">*</span> <span class="mi">100</span><span class="p">,</span> <span class="n">print_loss_avg</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="n">plot_every</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">plot_loss_avg</span> <span class="o">=</span> <span class="n">plot_loss_total</span> <span class="o">/</span> <span class="n">plot_every</span>
            <span class="n">plot_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">plot_loss_avg</span><span class="p">)</span>
            <span class="n">plot_loss_total</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">return</span> <span class="n">plot_losses</span>

<span class="c1"># Helper function for timing</span>
<span class="k">def</span> <span class="nf">timeSince</span><span class="p">(</span><span class="n">since</span><span class="p">,</span> <span class="n">percent</span><span class="p">):</span>
    <span class="n">now</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">now</span> <span class="o">-</span> <span class="n">since</span>
    <span class="n">es</span> <span class="o">=</span> <span class="n">s</span> <span class="o">/</span> <span class="p">(</span><span class="n">percent</span><span class="p">)</span>
    <span class="n">rs</span> <span class="o">=</span> <span class="n">es</span> <span class="o">-</span> <span class="n">s</span>
    <span class="k">return</span> <span class="s1">'</span><span class="si">%s</span><span class="s1"> (- </span><span class="si">%s</span><span class="s1">)'</span> <span class="o">%</span> <span class="p">(</span><span class="n">asMinutes</span><span class="p">(</span><span class="n">s</span><span class="p">),</span> <span class="n">asMinutes</span><span class="p">(</span><span class="n">rs</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">asMinutes</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">s</span> <span class="o">/</span> <span class="mi">60</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">-=</span> <span class="n">m</span> <span class="o">*</span> <span class="mi">60</span>
    <span class="k">return</span> <span class="s1">'</span><span class="si">%d</span><span class="s1">m </span><span class="si">%d</span><span class="s1">s'</span> <span class="o">%</span> <span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>

<span class="c1"># Main execution</span>
<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="c1"># Get data and create dataloader</span>
    <span class="n">input_lang</span><span class="p">,</span> <span class="n">output_lang</span><span class="p">,</span> <span class="n">train_dataloader</span><span class="p">,</span> <span class="n">pairs</span> <span class="o">=</span> <span class="n">get_dataloader</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>

    <span class="c1"># Initialize models</span>
    <span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">256</span>
    <span class="n">improved_encoder</span> <span class="o">=</span> <span class="n">ImprovedEncoder</span><span class="p">(</span><span class="n">input_lang</span><span class="o">.</span><span class="n">n_words</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">improved_decoder</span> <span class="o">=</span> <span class="n">ImprovedDecoder</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_lang</span><span class="o">.</span><span class="n">n_words</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># Train the model</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Starting training..."</span><span class="p">)</span>
    <span class="n">improved_losses</span> <span class="o">=</span> <span class="n">train_improved</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">,</span> <span class="n">improved_encoder</span><span class="p">,</span> <span class="n">improved_decoder</span><span class="p">,</span>
                                   <span class="n">n_epochs</span><span class="o">=</span><span class="mi">80</span><span class="p">,</span> <span class="n">print_every</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">plot_every</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

    <span class="c1"># Save the trained models</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">improved_encoder</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="s1">'improved_encoder.pt'</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">improved_decoder</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="s1">'improved_decoder.pt'</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">improved_encoder</span><span class="p">,</span> <span class="n">improved_decoder</span><span class="p">,</span> <span class="n">improved_losses</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">"__main__"</span><span class="p">:</span>
    <span class="n">improved_encoder</span><span class="p">,</span> <span class="n">improved_decoder</span><span class="p">,</span> <span class="n">improved_losses</span> <span class="o">=</span> <span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Reading lines...
Read 128133 sentence pairs
Trimmed to 9320 sentence pairs
Counting words...
Counted words:
heb 7519
eng 3067
Starting training...
0m 42s (- 10m 30s) (5 6%) 1.9305
1m 24s (- 9m 52s) (10 12%) 0.4765
2m 6s (- 9m 9s) (15 18%) 0.1936
2m 48s (- 8m 25s) (20 25%) 0.1238
3m 31s (- 7m 44s) (25 31%) 0.0986
4m 12s (- 7m 1s) (30 37%) 0.0878
4m 54s (- 6m 18s) (35 43%) 0.0786
5m 37s (- 5m 37s) (40 50%) 0.0742
6m 19s (- 4m 54s) (45 56%) 0.0699
7m 0s (- 4m 12s) (50 62%) 0.0675
7m 45s (- 3m 31s) (55 68%) 0.0660
8m 27s (- 2m 49s) (60 75%) 0.0622
9m 9s (- 2m 6s) (65 81%) 0.0613
9m 52s (- 1m 24s) (70 87%) 0.0466
10m 33s (- 0m 42s) (75 93%) 0.0272
11m 15s (- 0m 0s) (80 100%) 0.0247
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[24]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Run the training</span>
<span class="n">improved_encoder</span><span class="p">,</span> <span class="n">improved_decoder</span><span class="p">,</span> <span class="n">improved_losses</span> <span class="o">=</span> <span class="n">main</span><span class="p">()</span>

<span class="c1"># After training, you can evaluate some translations</span>
<span class="k">def</span> <span class="nf">evaluate_examples</span><span class="p">():</span>
    <span class="n">improved_encoder</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="n">improved_decoder</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
            <span class="n">pair</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">pairs</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">'Input:'</span><span class="p">,</span> <span class="n">pair</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">'Target:'</span><span class="p">,</span> <span class="n">pair</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
            <span class="n">output_words</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">improved_encoder</span><span class="p">,</span> <span class="n">improved_decoder</span><span class="p">,</span> <span class="n">pair</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">input_lang</span><span class="p">,</span> <span class="n">output_lang</span><span class="p">)</span>
            <span class="n">output_sentence</span> <span class="o">=</span> <span class="s1">' '</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">output_words</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">'Output:'</span><span class="p">,</span> <span class="n">output_sentence</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">()</span>

<span class="n">evaluate_examples</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Reading lines...
Read 128133 sentence pairs
Trimmed to 9320 sentence pairs
Counting words...
Counted words:
heb 7519
eng 3067
Starting training...
0m 42s (- 10m 40s) (5 6%) 1.9347
1m 24s (- 9m 51s) (10 12%) 0.4789
2m 7s (- 9m 10s) (15 18%) 0.1916
2m 50s (- 8m 31s) (20 25%) 0.1222
3m 32s (- 7m 46s) (25 31%) 0.0990
4m 13s (- 7m 2s) (30 37%) 0.0867
4m 56s (- 6m 21s) (35 43%) 0.0800
5m 38s (- 5m 38s) (40 50%) 0.0731
6m 24s (- 4m 59s) (45 56%) 0.0685
7m 6s (- 4m 16s) (50 62%) 0.0524
7m 48s (- 3m 32s) (55 68%) 0.0305
8m 30s (- 2m 50s) (60 75%) 0.0270
9m 12s (- 2m 7s) (65 81%) 0.0253
9m 54s (- 1m 24s) (70 87%) 0.0243
10m 36s (- 0m 42s) (75 93%) 0.0228
11m 19s (- 0m 0s) (80 100%) 0.0225
Input:    .
Target: he is proud of his punctuality
Output: he is proud of his punctuality &lt;EOS&gt;

Input:      .
Target: i m supposed to eat with tom this evening
Output: i m supposed to eat with tom this evening &lt;EOS&gt;

Input:    .
Target: he is our english teacher
Output: he is our english teacher &lt;EOS&gt;

Input:      .
Target: i m old enough to look after myself
Output: i m old enough to look after myself &lt;EOS&gt;

Input:    .
Target: i m used to eating alone
Output: i used used to eating alone alone &lt;EOS&gt;

</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[25]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">calculate_metrics</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">test_pairs</span><span class="p">,</span> <span class="n">input_lang</span><span class="p">,</span> <span class="n">output_lang</span><span class="p">):</span>
    <span class="n">encoder</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="n">decoder</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

    <span class="n">total_pairs</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_pairs</span><span class="p">)</span>
    <span class="n">exact_matches</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">bleu_total</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">test_pairs</span><span class="p">:</span>
            <span class="n">input_sentence</span> <span class="o">=</span> <span class="n">pair</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">target_sentence</span> <span class="o">=</span> <span class="n">pair</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

            <span class="c1"># Get model output</span>
            <span class="n">output_words</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">input_sentence</span><span class="p">,</span> <span class="n">input_lang</span><span class="p">,</span> <span class="n">output_lang</span><span class="p">)</span>
            <span class="n">output_sentence</span> <span class="o">=</span> <span class="s1">' '</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">word</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">output_words</span> <span class="k">if</span> <span class="n">word</span> <span class="o">!=</span> <span class="s1">'&lt;EOS&gt;'</span><span class="p">])</span>

            <span class="c1"># Calculate exact match</span>
            <span class="k">if</span> <span class="n">output_sentence</span> <span class="o">==</span> <span class="n">target_sentence</span><span class="p">:</span>
                <span class="n">exact_matches</span> <span class="o">+=</span> <span class="mi">1</span>

            <span class="c1"># Calculate BLEU score</span>
            <span class="n">reference</span> <span class="o">=</span> <span class="p">[</span><span class="n">target_sentence</span><span class="o">.</span><span class="n">split</span><span class="p">()]</span>
            <span class="n">hypothesis</span> <span class="o">=</span> <span class="n">output_sentence</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
            <span class="n">bleu_score</span> <span class="o">=</span> <span class="n">sentence_bleu</span><span class="p">(</span><span class="n">reference</span><span class="p">,</span> <span class="n">hypothesis</span><span class="p">)</span>
            <span class="n">bleu_total</span> <span class="o">+=</span> <span class="n">bleu_score</span>

    <span class="c1"># Calculate metrics</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="p">(</span><span class="n">exact_matches</span> <span class="o">/</span> <span class="n">total_pairs</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span>
    <span class="n">avg_bleu</span> <span class="o">=</span> <span class="p">(</span><span class="n">bleu_total</span> <span class="o">/</span> <span class="n">total_pairs</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Metrics on </span><span class="si">{</span><span class="n">total_pairs</span><span class="si">}</span><span class="s2"> test pairs:"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Exact Match Accuracy: </span><span class="si">{</span><span class="n">accuracy</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Average BLEU Score: </span><span class="si">{</span><span class="n">avg_bleu</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">accuracy</span><span class="p">,</span> <span class="n">avg_bleu</span>

<span class="c1"># Add this at the end of your main() function:</span>
<span class="n">test_pairs</span> <span class="o">=</span> <span class="n">pairs</span><span class="p">[</span><span class="o">-</span><span class="mi">100</span><span class="p">:]</span>  <span class="c1"># Use last 100 pairs as test set</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Evaluating baseline model..."</span><span class="p">)</span>
<span class="n">baseline_accuracy</span><span class="p">,</span> <span class="n">baseline_bleu</span> <span class="o">=</span> <span class="n">calculate_metrics</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">test_pairs</span><span class="p">,</span> <span class="n">input_lang</span><span class="p">,</span> <span class="n">output_lang</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Evaluating improved model..."</span><span class="p">)</span>
<span class="n">improved_accuracy</span><span class="p">,</span> <span class="n">improved_bleu</span> <span class="o">=</span> <span class="n">calculate_metrics</span><span class="p">(</span><span class="n">improved_encoder</span><span class="p">,</span> <span class="n">improved_decoder</span><span class="p">,</span> <span class="n">test_pairs</span><span class="p">,</span> <span class="n">input_lang</span><span class="p">,</span> <span class="n">output_lang</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Comparison:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Baseline Model - Accuracy: </span><span class="si">{</span><span class="n">baseline_accuracy</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%, BLEU: </span><span class="si">{</span><span class="n">baseline_bleu</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Improved Model - Accuracy: </span><span class="si">{</span><span class="n">improved_accuracy</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%, BLEU: </span><span class="si">{</span><span class="n">improved_bleu</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Improvement - Accuracy: </span><span class="si">{</span><span class="n">improved_accuracy</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">baseline_accuracy</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%, BLEU: </span><span class="si">{</span><span class="n">improved_bleu</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">baseline_bleu</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>
Evaluating baseline model...
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>
Metrics on 100 test pairs:
Exact Match Accuracy: 51.00%
Average BLEU Score: 78.69

Evaluating improved model...

Metrics on 100 test pairs:
Exact Match Accuracy: 86.00%
Average BLEU Score: 94.41

Comparison:
Baseline Model - Accuracy: 51.00%, BLEU: 78.69
Improved Model - Accuracy: 86.00%, BLEU: 94.41
Improvement - Accuracy: 35.00%, BLEU: 15.72
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">google.colab</span> <span class="kn">import</span> <span class="n">drive</span>
<span class="n">drive</span><span class="o">.</span><span class="n">mount</span><span class="p">(</span><span class="s1">'/content/drive'</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="o">%%</span><span class="k">shell</span>
jupyter nbconvert --to html /content/drive/MyDrive/PS3_Attention_Please_2024_ID_209307396.ipynb
</pre></div>
</div>
</div>
</div>
</div>
</div>
</main>
</body>
</html>
