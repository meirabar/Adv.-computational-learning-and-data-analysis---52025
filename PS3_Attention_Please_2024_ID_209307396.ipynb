{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOpGoE2T-YXS"
      },
      "source": [
        "# Neural Machine Translation with Attention\n",
        "\n",
        "Advanced Learning Fall 2024.   \n",
        "Last updated: 2025-01-12\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpJdYve9cZa6"
      },
      "source": [
        "For SUBMISSION:   \n",
        "\n",
        "Please upload the complete and executed `ipynb` to your git repository. Verify that all of your output can be viewed directly from github, and provide a link to that git file below.\n",
        "\n",
        "~~~\n",
        "STUDENT ID: 209307396\n",
        "~~~\n",
        "\n",
        "~~~\n",
        "STUDENT GIT LINK: https://github.com/meirabar/Adv.-computational-learning-and-data-analysis---52025.git\n",
        "~~~\n",
        "In Addition, don't forget to add your ID to the files, and upload to moodle the html version:    \n",
        "  \n",
        "`PS3_Attention_2024_ID_[209307396].html`   \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eecp2PAf7qJq"
      },
      "source": [
        "In this problem set we are going to jump into the depths of `seq2seq` and `attention` and build a couple of PyTorch translation mechanisms with some  twists.     \n",
        "\n",
        "\n",
        "*   Part 1 consists of a somewhat unorthodox `seq2seq` model for simple arithmetics\n",
        "*   Part 2 consists of an `seq2seq - attention` language translation model. We will use it for Hebrew and English.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VpUCez9gOZn"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajNDsL5HlZN6"
      },
      "source": [
        "A **seq2seq** model (sequence-to-sequence model) is a type of neural network designed specifically to handle sequences of data. The model converts input sequences into other sequences of data. This makes them particularly useful for tasks involving language, where the input and output are naturally sequences of words.\n",
        "\n",
        "Here's a breakdown of how `seq2seq` models work:\n",
        "\n",
        "* The encoder takes the input sequence, like a sentence in English, and processes it to capture its meaning and context.\n",
        "\n",
        "* information is then passed to the decoder, which uses it to generate the output sequence, like a translation in French.\n",
        "\n",
        "* Attention mechanism (optional): Some `seq2seq` models also incorporate an attention mechanism. This allows the decoder to focus on specific parts of the input sequence that are most relevant to generating the next element in the output sequence.\n",
        "\n",
        "`seq2seq` models are used in many natural language processing (NLP) tasks.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbUDn4FObol7"
      },
      "source": [
        "imports: (feel free to add)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "crTe33wcD_Eg"
      },
      "outputs": [],
      "source": [
        "# from __future__ import unicode_literals, print_function, division\n",
        "# from io import open\n",
        "# import unicodedata\n",
        "import re\n",
        "import random\n",
        "import unicodedata\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import time\n",
        "import math\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiwtNgENbx2g"
      },
      "source": [
        "## Part 1: Seq2Seq Arithmetic model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1gWov3Gx67I"
      },
      "source": [
        "**Using RNN `seq2seq` model to \"learn\" simple arithmetics!**\n",
        "\n",
        "> Given the string \"54-7\", the model should return a prediction: \"47\".  \n",
        "> Given the string \"10+20\", the model should return a prediction: \"30\".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dxo92ZgTy6ED"
      },
      "source": [
        "- Watch Lukas Biewald's short [video](https://youtu.be/MqugtGD605k?si=rAH34ZTJyYDj-XJ1) explaining `seq2seq` models and his toy application (somewhat outdated).\n",
        "- You can find the code for his example [here](https://github.com/lukas/ml-class/blob/master/videos/seq2seq/train.py).    \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEu_5YvqFPai"
      },
      "source": [
        "1.1) Using Lukas' code, implement a `seq2seq` network that can learn how to solve **addition AND substraction** of two numbers of maximum length of 4, using the following steps (similar to the example):      \n",
        "\n",
        "* Generate data; X: queries (two numbers), and Y: answers   \n",
        "* One-hot encode X and Y,\n",
        "* Build a `seq2seq` network (with LSTM, RepeatVector, and TimeDistributed layers)\n",
        "* Train the model.\n",
        "* While training, sample from the validation set at random so we can visualize the generated solutions against the true solutions.    \n",
        "\n",
        "Notes:  \n",
        "* The code in the example is quite old and based on Keras. You might have to adapt some of the code to overcome methods/code that is not supported anymore. Hint: for the evaluation part, review the type and format of the \"correct\" output - this will help you fix the unsupported \"model.predict_classes\".\n",
        "* Please use the parameters in the code cell below to train the model.     \n",
        "* Instead of using a `wandb.config` object, please use a simple dictionary instead.   \n",
        "* You don't need to run the model for more than 50 iterations (epochs) to get a gist of what is happening and what the algorithm is doing.\n",
        "* Extra credit if you can implement the network in PyTorch (this is not difficult).    \n",
        "* Extra credit if you are able to significantly improve the model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Attention, Input, Dense, LSTM, RepeatVector, TimeDistributed, Layer, Concatenate\n",
        "\n",
        "\n",
        "class CharacterTable(object):\n",
        "    def __init__(self, chars):\n",
        "        self.chars = sorted(set(chars))\n",
        "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
        "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
        "\n",
        "    def encode(self, C, num_rows):\n",
        "        x = np.zeros((num_rows, len(self.chars)))\n",
        "        for i, c in enumerate(C):\n",
        "            x[i, self.char_indices[c]] = 1\n",
        "        return x\n",
        "\n",
        "    def decode(self, x, calc_argmax=True):\n",
        "        if calc_argmax:\n",
        "            x = x.argmax(axis=-1)\n",
        "        return ''.join(self.indices_char[x] for x in x)\n",
        "\n",
        "# Configuration parameters\n",
        "config = {\n",
        "    \"training_size\": 40000,\n",
        "    \"digits\": 4,\n",
        "    \"hidden_size\": 128,\n",
        "    \"batch_size\": 128,\n",
        "    \"iterations\": 50\n",
        "}\n",
        "\n",
        "# Define character set and maximum length\n",
        "chars = '0123456789-+ '\n",
        "maxlen = config[\"digits\"] + 1 + config[\"digits\"]\n",
        "ctable = CharacterTable(chars)\n",
        "\n",
        "# Generate training data\n",
        "questions = []\n",
        "expected = []\n",
        "seen = set()\n",
        "print('Generating data...')\n",
        "while len(questions) < config[\"training_size\"]:\n",
        "    f = lambda: int(''.join(np.random.choice(list('0123456789'))\n",
        "                    for i in range(np.random.randint(1, config[\"digits\"] + 1))))\n",
        "    a, b = f(), f()\n",
        "    key = tuple(sorted((a, b)))\n",
        "    if key in seen:\n",
        "        continue\n",
        "    seen.add(key)\n",
        "    q = '{}-{}'.format(a, b)\n",
        "    query = q + ' ' * (maxlen - len(q))\n",
        "    ans = str(a - b)\n",
        "    ans += ' ' * (config[\"digits\"] + 1 - len(ans))\n",
        "\n",
        "    questions.append(query)\n",
        "    expected.append(ans)\n",
        "\n",
        "print('Total subtraction questions:', len(questions))\n",
        "\n",
        "# Vectorize the data\n",
        "print('Vectorization...')\n",
        "x = np.zeros((len(questions), maxlen, len(chars)), dtype=np.bool_)\n",
        "y = np.zeros((len(questions), config[\"digits\"] + 1, len(chars)), dtype=np.bool_)\n",
        "for i, sentence in enumerate(questions):\n",
        "    x[i] = ctable.encode(sentence, maxlen)\n",
        "for i, sentence in enumerate(expected):\n",
        "    y[i] = ctable.encode(sentence, config[\"digits\"] + 1)\n",
        "\n",
        "# Shuffle the data\n",
        "indices = np.arange(len(y))\n",
        "np.random.shuffle(indices)\n",
        "x = x[indices]\n",
        "y = y[indices]\n",
        "\n",
        "# Split into training and validation sets\n",
        "split_at = len(x) - len(x) // 10\n",
        "(x_train, x_val) = x[:split_at], x[split_at:]\n",
        "(y_train, y_val) = y[:split_at], y[split_at:]\n",
        "\n",
        "# Build the model\n",
        "model = Sequential([\n",
        "    LSTM(config[\"hidden_size\"], input_shape=(maxlen, len(chars))),\n",
        "    RepeatVector(config[\"digits\"] + 1),\n",
        "    LSTM(config[\"hidden_size\"], return_sequences=True),\n",
        "    TimeDistributed(Dense(len(chars), activation='softmax'))\n",
        "])\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# Training loop with updated prediction handling\n",
        "for iteration in range(config[\"iterations\"]):\n",
        "    print()\n",
        "    print('-' * 50)\n",
        "    print('Iteration', iteration + 1)\n",
        "\n",
        "    model.fit(x_train, y_train,\n",
        "             batch_size=config[\"batch_size\"],\n",
        "             epochs=1,\n",
        "             validation_data=(x_val, y_val))\n",
        "\n",
        "    # Validate on 10 random samples\n",
        "    for i in range(10):\n",
        "        ind = np.random.randint(0, len(x_val))\n",
        "        rowx, rowy = x_val[np.array([ind])], y_val[np.array([ind])]\n",
        "\n",
        "        # Updated prediction handling - using argmax on the raw predictions\n",
        "        preds = model.predict(rowx, verbose=0)\n",
        "        pred_classes = np.argmax(preds[0], axis=-1)\n",
        "\n",
        "        q = ctable.decode(rowx[0])\n",
        "        correct = ctable.decode(rowy[0])\n",
        "        guess = ''.join(ctable.indices_char[x] for x in pred_classes)\n",
        "\n",
        "        print('Q:', q.strip(), end=' ')\n",
        "        print('T:', correct.strip(), end=' ')\n",
        "        if correct.strip() == guess.strip():\n",
        "            print('☑', end=' ')\n",
        "        else:\n",
        "            print('☒', end=' ')\n",
        "        print(guess.strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HxdO_FUmOh4q",
        "outputId": "65362a83-583c-4bed-d9bd-12bfb6b2f0be"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating data...\n",
            "Total subtraction questions: 40000\n",
            "Vectorization...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m72,704\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ repeat_vector (\u001b[38;5;33mRepeatVector\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m128\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m128\u001b[0m)              │         \u001b[38;5;34m131,584\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed (\u001b[38;5;33mTimeDistributed\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m13\u001b[0m)               │           \u001b[38;5;34m1,677\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">72,704</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ repeat_vector (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RepeatVector</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>)               │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,677</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m205,965\u001b[0m (804.55 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">205,965</span> (804.55 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m205,965\u001b[0m (804.55 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">205,965</span> (804.55 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------------------------------\n",
            "Iteration 1\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.2862 - loss: 2.1107 - val_accuracy: 0.3880 - val_loss: 1.6632\n",
            "Q: 3370-6 T: 3364 ☒ 669\n",
            "Q: 5-765 T: -760 ☒ -55\n",
            "Q: 937-33 T: 904 ☒ 25\n",
            "Q: 18-51 T: -33 ☒ -1\n",
            "Q: 1171-985 T: 186 ☒ -76\n",
            "Q: 8226-8 T: 8218 ☒ 6699\n",
            "Q: 2277-194 T: 2083 ☒ 176\n",
            "Q: 923-328 T: 595 ☒ -23\n",
            "Q: 3050-2 T: 3048 ☒ 669\n",
            "Q: 667-22 T: 645 ☒ 15\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 2\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.4010 - loss: 1.6302 - val_accuracy: 0.4196 - val_loss: 1.5818\n",
            "Q: 351-0 T: 351 ☒ 311\n",
            "Q: 7378-3999 T: 3379 ☒ -771\n",
            "Q: 305-7943 T: -7638 ☒ -3911\n",
            "Q: 6415-2493 T: 3922 ☒ -111\n",
            "Q: 612-1112 T: -500 ☒ -111\n",
            "Q: 77-3424 T: -3347 ☒ -2216\n",
            "Q: 824-6 T: 818 ☒ 877\n",
            "Q: 388-1 T: 387 ☒ 887\n",
            "Q: 772-48 T: 724 ☒ 777\n",
            "Q: 4-510 T: -506 ☒ -41\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 3\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.4308 - loss: 1.5495 - val_accuracy: 0.4438 - val_loss: 1.4984\n",
            "Q: 443-6 T: 437 ☒ 440\n",
            "Q: 557-918 T: -361 ☒ -50\n",
            "Q: 4898-37 T: 4861 ☒ 8877\n",
            "Q: 6567-189 T: 6378 ☒ 6602\n",
            "Q: 1677-251 T: 1426 ☒ 111\n",
            "Q: 929-57 T: 872 ☒ 999\n",
            "Q: 932-9925 T: -8993 ☒ -9905\n",
            "Q: 255-8 T: 247 ☒ 552\n",
            "Q: 800-350 T: 450 ☒ 110\n",
            "Q: 1780-5 T: 1775 ☒ 777\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 4\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.4520 - loss: 1.4719 - val_accuracy: 0.4639 - val_loss: 1.4379\n",
            "Q: 538-7604 T: -7066 ☒ -7477\n",
            "Q: 9485-33 T: 9452 ☒ 9988\n",
            "Q: 5051-44 T: 5007 ☒ 5443\n",
            "Q: 2-23 T: -21 ☒ -28\n",
            "Q: 346-1833 T: -1487 ☒ -3077\n",
            "Q: 84-615 T: -531 ☒ -443\n",
            "Q: 9036-4 T: 9032 ☒ 9999\n",
            "Q: 36-47 T: -11 ☒ -44\n",
            "Q: 44-968 T: -924 ☒ -888\n",
            "Q: 97-7214 T: -7117 ☒ -7277\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 5\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.4783 - loss: 1.3933 - val_accuracy: 0.4963 - val_loss: 1.3359\n",
            "Q: 8222-8546 T: -324 ☒ 221\n",
            "Q: 95-40 T: 55 ☒ 44\n",
            "Q: 4081-2363 T: 1718 ☒ 310\n",
            "Q: 362-947 T: -585 ☒ -323\n",
            "Q: 77-6 T: 71 ☒ 63\n",
            "Q: 719-46 T: 673 ☒ 733\n",
            "Q: 98-91 T: 7 ☒ 8\n",
            "Q: 7008-9954 T: -2946 ☒ -133\n",
            "Q: 73-2 T: 71 ☒ 77\n",
            "Q: 3760-530 T: 3230 ☒ 3113\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 6\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.5130 - loss: 1.3100 - val_accuracy: 0.5304 - val_loss: 1.2552\n",
            "Q: 5883-89 T: 5794 ☒ 8886\n",
            "Q: 91-1302 T: -1211 ☒ -1044\n",
            "Q: 1886-255 T: 1631 ☒ 1666\n",
            "Q: 4-9355 T: -9351 ☒ -9433\n",
            "Q: 4-5460 T: -5456 ☒ -4400\n",
            "Q: 5668-55 T: 5613 ☒ 5556\n",
            "Q: 8356-3 T: 8353 ☒ 8226\n",
            "Q: 3-3059 T: -3056 ☒ -3003\n",
            "Q: 885-4 T: 881 ☒ 886\n",
            "Q: 481-51 T: 430 ☒ 476\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 7\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.5386 - loss: 1.2424 - val_accuracy: 0.5485 - val_loss: 1.2139\n",
            "Q: 617-924 T: -307 ☒ -584\n",
            "Q: 5546-1 T: 5545 ☒ 5518\n",
            "Q: 1226-32 T: 1194 ☒ 1188\n",
            "Q: 5030-89 T: 4941 ☒ 5088\n",
            "Q: 4-4176 T: -4172 ☒ -4110\n",
            "Q: 505-59 T: 446 ☒ 408\n",
            "Q: 37-98 T: -61 ☒ -55\n",
            "Q: 7-5874 T: -5867 ☒ -5750\n",
            "Q: 647-46 T: 601 ☒ 638\n",
            "Q: 4274-326 T: 3948 ☒ 3888\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 8\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.5615 - loss: 1.1816 - val_accuracy: 0.5705 - val_loss: 1.1558\n",
            "Q: 373-3 T: 370 ☑ 370\n",
            "Q: 590-9 T: 581 ☒ 595\n",
            "Q: 48-2703 T: -2655 ☒ -2790\n",
            "Q: 4-399 T: -395 ☒ -397\n",
            "Q: 7-543 T: -536 ☒ -545\n",
            "Q: 7454-83 T: 7371 ☒ 7411\n",
            "Q: 7465-1 T: 7464 ☒ 7470\n",
            "Q: 84-766 T: -682 ☒ -650\n",
            "Q: 6180-8 T: 6172 ☒ 6000\n",
            "Q: 9854-749 T: 9105 ☒ 8033\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 9\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.5787 - loss: 1.1365 - val_accuracy: 0.5889 - val_loss: 1.1059\n",
            "Q: 36-684 T: -648 ☒ -635\n",
            "Q: 8990-656 T: 8334 ☒ 8043\n",
            "Q: 2440-896 T: 1544 ☒ 2455\n",
            "Q: 37-109 T: -72 ☒ -10\n",
            "Q: 5659-0 T: 5659 ☒ 5685\n",
            "Q: 356-6 T: 350 ☒ 352\n",
            "Q: 5099-77 T: 5022 ☒ 5043\n",
            "Q: 796-87 T: 709 ☒ 704\n",
            "Q: 558-933 T: -375 ☒ -374\n",
            "Q: 44-992 T: -948 ☒ -944\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 10\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.5927 - loss: 1.0957 - val_accuracy: 0.5929 - val_loss: 1.0827\n",
            "Q: 6-976 T: -970 ☒ -962\n",
            "Q: 517-7 T: 510 ☒ 516\n",
            "Q: 91-44 T: 47 ☒ 41\n",
            "Q: 61-1617 T: -1556 ☒ -1501\n",
            "Q: 45-28 T: 17 ☒ 2\n",
            "Q: 315-569 T: -254 ☒ -222\n",
            "Q: 35-36 T: -1 ☒ -\n",
            "Q: 7-3687 T: -3680 ☒ -3676\n",
            "Q: 40-924 T: -884 ☒ -894\n",
            "Q: 63-808 T: -745 ☒ -782\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 11\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.6061 - loss: 1.0597 - val_accuracy: 0.6062 - val_loss: 1.0603\n",
            "Q: 912-3 T: 909 ☒ 906\n",
            "Q: 5-9785 T: -9780 ☒ -9768\n",
            "Q: 7826-5 T: 7821 ☒ 7828\n",
            "Q: 69-9001 T: -8932 ☒ -8994\n",
            "Q: 448-94 T: 354 ☒ 365\n",
            "Q: 3-7610 T: -7607 ☒ -7608\n",
            "Q: 40-326 T: -286 ☒ -296\n",
            "Q: 1725-42 T: 1683 ☒ 1698\n",
            "Q: 5567-148 T: 5419 ☒ 5466\n",
            "Q: 820-158 T: 662 ☒ 666\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 12\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.6172 - loss: 1.0328 - val_accuracy: 0.6042 - val_loss: 1.0482\n",
            "Q: 9459-15 T: 9444 ☒ 9451\n",
            "Q: 170-907 T: -737 ☒ -690\n",
            "Q: 6716-766 T: 5950 ☒ 6099\n",
            "Q: 931-66 T: 865 ☒ 898\n",
            "Q: 8-4375 T: -4367 ☒ -4365\n",
            "Q: 34-7215 T: -7181 ☒ -7199\n",
            "Q: 328-2263 T: -1935 ☒ -1954\n",
            "Q: 1873-7866 T: -5993 ☒ -6990\n",
            "Q: 32-52 T: -20 ☒ -19\n",
            "Q: 6116-7 T: 6109 ☒ 6150\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 13\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.6259 - loss: 1.0030 - val_accuracy: 0.6259 - val_loss: 0.9996\n",
            "Q: 9534-74 T: 9460 ☑ 9460\n",
            "Q: 1906-1849 T: 57 ☒ 102\n",
            "Q: 4-238 T: -234 ☒ -233\n",
            "Q: 6-3204 T: -3198 ☒ -3204\n",
            "Q: 755-720 T: 35 ☒ -5\n",
            "Q: 921-476 T: 445 ☒ 362\n",
            "Q: 309-880 T: -571 ☒ -580\n",
            "Q: 7455-74 T: 7381 ☒ 7388\n",
            "Q: 6298-6 T: 6292 ☒ 6283\n",
            "Q: 732-1125 T: -393 ☒ -45\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 14\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.6369 - loss: 0.9770 - val_accuracy: 0.6247 - val_loss: 0.9939\n",
            "Q: 99-762 T: -663 ☒ -634\n",
            "Q: 5-6088 T: -6083 ☒ -6078\n",
            "Q: 812-98 T: 714 ☒ 735\n",
            "Q: 2630-2 T: 2628 ☒ 2626\n",
            "Q: 719-0 T: 719 ☒ 706\n",
            "Q: 42-8725 T: -8683 ☒ -8678\n",
            "Q: 6-683 T: -677 ☒ -674\n",
            "Q: 321-3798 T: -3477 ☒ -3544\n",
            "Q: 4-5045 T: -5041 ☒ -504\n",
            "Q: 1307-855 T: 452 ☒ 732\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 15\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.6475 - loss: 0.9519 - val_accuracy: 0.6315 - val_loss: 0.9742\n",
            "Q: 1-7442 T: -7441 ☒ -7440\n",
            "Q: 952-33 T: 919 ☒ 911\n",
            "Q: 52-698 T: -646 ☒ -631\n",
            "Q: 96-68 T: 28 ☒ 11\n",
            "Q: 8003-7 T: 7996 ☒ 8001\n",
            "Q: 3537-599 T: 2938 ☒ 3161\n",
            "Q: 17-102 T: -85 ☒ -92\n",
            "Q: 43-1894 T: -1851 ☒ -1852\n",
            "Q: 1-8667 T: -8666 ☒ -8665\n",
            "Q: 9-833 T: -824 ☒ -828\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 16\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.6547 - loss: 0.9298 - val_accuracy: 0.6517 - val_loss: 0.9282\n",
            "Q: 52-4120 T: -4068 ☒ -4079\n",
            "Q: 3962-74 T: 3888 ☒ 3813\n",
            "Q: 98-474 T: -376 ☒ -363\n",
            "Q: 0-5245 T: -5245 ☒ -5241\n",
            "Q: 372-23 T: 349 ☒ 353\n",
            "Q: 5108-6 T: 5102 ☒ 5006\n",
            "Q: 887-7 T: 880 ☒ 870\n",
            "Q: 270-9071 T: -8801 ☒ -8911\n",
            "Q: 4-723 T: -719 ☒ -729\n",
            "Q: 4060-7029 T: -2969 ☒ -2110\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 17\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.6641 - loss: 0.9029 - val_accuracy: 0.6557 - val_loss: 0.9150\n",
            "Q: 3446-144 T: 3302 ☒ 3300\n",
            "Q: 2588-1158 T: 1430 ☒ 1377\n",
            "Q: 131-3586 T: -3455 ☒ -3444\n",
            "Q: 143-204 T: -61 ☒ -91\n",
            "Q: 611-90 T: 521 ☒ 510\n",
            "Q: 113-67 T: 46 ☒ 64\n",
            "Q: 628-9 T: 619 ☒ 614\n",
            "Q: 9116-6 T: 9110 ☒ 9163\n",
            "Q: 12-442 T: -430 ☒ -429\n",
            "Q: 9044-2118 T: 6926 ☒ 7811\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 18\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.6692 - loss: 0.8889 - val_accuracy: 0.6601 - val_loss: 0.8968\n",
            "Q: 7465-1 T: 7464 ☒ 7453\n",
            "Q: 998-127 T: 871 ☒ 966\n",
            "Q: 3-7610 T: -7607 ☒ -7609\n",
            "Q: 5051-44 T: 5007 ☒ 5099\n",
            "Q: 205-734 T: -529 ☒ -566\n",
            "Q: 57-2565 T: -2508 ☒ -2500\n",
            "Q: 841-3 T: 838 ☒ 839\n",
            "Q: 4558-670 T: 3888 ☒ 3879\n",
            "Q: 5659-0 T: 5659 ☒ 5656\n",
            "Q: 255-8 T: 247 ☒ 248\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 19\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - accuracy: 0.6776 - loss: 0.8642 - val_accuracy: 0.6598 - val_loss: 0.8907\n",
            "Q: 886-9 T: 877 ☒ 876\n",
            "Q: 6724-70 T: 6654 ☒ 6666\n",
            "Q: 56-91 T: -35 ☑ -35\n",
            "Q: 18-378 T: -360 ☒ -369\n",
            "Q: 53-712 T: -659 ☒ -660\n",
            "Q: 11-29 T: -18 ☒ -17\n",
            "Q: 9255-874 T: 8381 ☒ 8860\n",
            "Q: 0-696 T: -696 ☒ -695\n",
            "Q: 8-974 T: -966 ☑ -966\n",
            "Q: 7976-293 T: 7683 ☒ 7586\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 20\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.6866 - loss: 0.8399 - val_accuracy: 0.6715 - val_loss: 0.8731\n",
            "Q: 4-8061 T: -8057 ☒ -8055\n",
            "Q: 6-9733 T: -9727 ☒ -9733\n",
            "Q: 1028-24 T: 1004 ☒ 1014\n",
            "Q: 8658-3777 T: 4881 ☒ 5099\n",
            "Q: 98-91 T: 7 ☒ -\n",
            "Q: 5-2296 T: -2291 ☒ -2292\n",
            "Q: 3241-668 T: 2573 ☒ 2655\n",
            "Q: 2948-8332 T: -5384 ☒ -5955\n",
            "Q: 796-607 T: 189 ☒ 135\n",
            "Q: 2075-8 T: 2067 ☒ 2077\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 21\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.6917 - loss: 0.8249 - val_accuracy: 0.6703 - val_loss: 0.8593\n",
            "Q: 638-6 T: 632 ☒ 635\n",
            "Q: 326-8 T: 318 ☒ 327\n",
            "Q: 6-50 T: -44 ☑ -44\n",
            "Q: 9080-808 T: 8272 ☒ 8900\n",
            "Q: 22-2937 T: -2915 ☒ -2943\n",
            "Q: 163-9 T: 154 ☒ 157\n",
            "Q: 1906-1849 T: 57 ☒ 125\n",
            "Q: 472-137 T: 335 ☒ 433\n",
            "Q: 854-9 T: 845 ☒ 856\n",
            "Q: 9-5771 T: -5762 ☒ -5768\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 22\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.6987 - loss: 0.8031 - val_accuracy: 0.6779 - val_loss: 0.8430\n",
            "Q: 7-1815 T: -1808 ☒ -1856\n",
            "Q: 5874-6 T: 5868 ☒ 5840\n",
            "Q: 8371-4 T: 8367 ☒ 8366\n",
            "Q: 415-167 T: 248 ☒ 262\n",
            "Q: 609-539 T: 70 ☒ 12\n",
            "Q: 2-7418 T: -7416 ☒ -7418\n",
            "Q: 559-5669 T: -5110 ☒ -5000\n",
            "Q: 9162-24 T: 9138 ☒ 9119\n",
            "Q: 3343-225 T: 3118 ☒ 3100\n",
            "Q: 31-415 T: -384 ☒ -389\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 23\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.7062 - loss: 0.7860 - val_accuracy: 0.6896 - val_loss: 0.8152\n",
            "Q: 862-5 T: 857 ☒ 856\n",
            "Q: 184-442 T: -258 ☒ -366\n",
            "Q: 824-502 T: 322 ☒ 369\n",
            "Q: 221-9185 T: -8964 ☒ -8055\n",
            "Q: 22-459 T: -437 ☒ -444\n",
            "Q: 6963-4763 T: 2200 ☒ 2099\n",
            "Q: 88-186 T: -98 ☒ -10\n",
            "Q: 5051-44 T: 5007 ☒ 5099\n",
            "Q: 4489-243 T: 4246 ☒ 4244\n",
            "Q: 5962-3500 T: 2462 ☒ 2464\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 24\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.7159 - loss: 0.7594 - val_accuracy: 0.6896 - val_loss: 0.7987\n",
            "Q: 82-31 T: 51 ☒ 50\n",
            "Q: 647-46 T: 601 ☒ 611\n",
            "Q: 2546-1533 T: 1013 ☒ 127\n",
            "Q: 30-513 T: -483 ☒ -470\n",
            "Q: 741-445 T: 296 ☒ 372\n",
            "Q: 548-884 T: -336 ☑ -336\n",
            "Q: 8650-984 T: 7666 ☒ 7755\n",
            "Q: 5-102 T: -97 ☒ -90\n",
            "Q: 3-51 T: -48 ☒ -47\n",
            "Q: 4898-37 T: 4861 ☒ 4852\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 25\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.7202 - loss: 0.7446 - val_accuracy: 0.7022 - val_loss: 0.7755\n",
            "Q: 5001-276 T: 4725 ☒ 4544\n",
            "Q: 2656-4161 T: -1505 ☒ -154\n",
            "Q: 788-342 T: 446 ☒ 445\n",
            "Q: 27-919 T: -892 ☒ -885\n",
            "Q: 26-245 T: -219 ☒ -218\n",
            "Q: 94-31 T: 63 ☑ 63\n",
            "Q: 51-56 T: -5 ☒ -4\n",
            "Q: 9-7928 T: -7919 ☒ -7910\n",
            "Q: 1237-726 T: 511 ☒ 665\n",
            "Q: 5071-837 T: 4234 ☒ 4235\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 26\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.7287 - loss: 0.7228 - val_accuracy: 0.7013 - val_loss: 0.7782\n",
            "Q: 336-0 T: 336 ☑ 336\n",
            "Q: 71-1602 T: -1531 ☒ -1551\n",
            "Q: 9-6891 T: -6882 ☒ -6889\n",
            "Q: 51-70 T: -19 ☒ -10\n",
            "Q: 2760-6 T: 2754 ☒ 2753\n",
            "Q: 369-9121 T: -8752 ☒ -8777\n",
            "Q: 115-7 T: 108 ☑ 108\n",
            "Q: 908-18 T: 890 ☒ 898\n",
            "Q: 1-3685 T: -3684 ☒ -3674\n",
            "Q: 151-5 T: 146 ☒ 147\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 27\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.7353 - loss: 0.7069 - val_accuracy: 0.7103 - val_loss: 0.7493\n",
            "Q: 455-2603 T: -2148 ☒ -2364\n",
            "Q: 188-218 T: -30 ☒ -88\n",
            "Q: 8-6034 T: -6026 ☑ -6026\n",
            "Q: 5449-265 T: 5184 ☒ 5174\n",
            "Q: 5-1104 T: -1099 ☒ -1011\n",
            "Q: 5883-89 T: 5794 ☒ 5795\n",
            "Q: 8233-948 T: 7285 ☒ 7455\n",
            "Q: 2-933 T: -931 ☑ -931\n",
            "Q: 78-1389 T: -1311 ☒ -1310\n",
            "Q: 3251-4078 T: -827 ☒ -1444\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 28\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.7400 - loss: 0.6893 - val_accuracy: 0.7130 - val_loss: 0.7375\n",
            "Q: 3704-0 T: 3704 ☒ 3703\n",
            "Q: 74-241 T: -167 ☒ -155\n",
            "Q: 0-7158 T: -7158 ☒ -7177\n",
            "Q: 9744-5 T: 9739 ☒ 9740\n",
            "Q: 606-84 T: 522 ☑ 522\n",
            "Q: 693-3247 T: -2554 ☒ -2653\n",
            "Q: 9901-7 T: 9894 ☑ 9894\n",
            "Q: 4740-5628 T: -888 ☒ -1898\n",
            "Q: 1-7442 T: -7441 ☒ -7440\n",
            "Q: 5785-4908 T: 877 ☒ -10\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 29\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.7497 - loss: 0.6662 - val_accuracy: 0.7262 - val_loss: 0.7023\n",
            "Q: 8-81 T: -73 ☑ -73\n",
            "Q: 570-474 T: 96 ☒ 124\n",
            "Q: 74-7261 T: -7187 ☒ -7104\n",
            "Q: 6467-467 T: 6000 ☒ 5000\n",
            "Q: 5-903 T: -898 ☑ -898\n",
            "Q: 64-1356 T: -1292 ☒ -1280\n",
            "Q: 8630-192 T: 8438 ☒ 8432\n",
            "Q: 75-611 T: -536 ☒ -544\n",
            "Q: 4-4176 T: -4172 ☑ -4172\n",
            "Q: 717-242 T: 475 ☒ 477\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 30\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.7563 - loss: 0.6455 - val_accuracy: 0.7277 - val_loss: 0.6921\n",
            "Q: 330-56 T: 274 ☒ 265\n",
            "Q: 45-250 T: -205 ☒ -206\n",
            "Q: 5768-607 T: 5161 ☒ 5999\n",
            "Q: 30-88 T: -58 ☒ -68\n",
            "Q: 2911-6 T: 2905 ☒ 2916\n",
            "Q: 7089-5 T: 7084 ☒ 7074\n",
            "Q: 60-265 T: -205 ☒ -215\n",
            "Q: 1767-62 T: 1705 ☒ 1613\n",
            "Q: 7868-77 T: 7791 ☒ 7899\n",
            "Q: 186-4149 T: -3963 ☒ -4957\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 31\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.7662 - loss: 0.6223 - val_accuracy: 0.7394 - val_loss: 0.6752\n",
            "Q: 92-97 T: -5 ☒ -14\n",
            "Q: 5668-55 T: 5613 ☑ 5613\n",
            "Q: 562-887 T: -325 ☒ -336\n",
            "Q: 82-567 T: -485 ☒ -484\n",
            "Q: 42-93 T: -51 ☒ -40\n",
            "Q: 73-389 T: -316 ☒ -325\n",
            "Q: 68-204 T: -136 ☒ -144\n",
            "Q: 3977-6 T: 3971 ☒ 3972\n",
            "Q: 169-64 T: 105 ☒ 12\n",
            "Q: 606-71 T: 535 ☒ 545\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 32\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.7718 - loss: 0.6069 - val_accuracy: 0.7417 - val_loss: 0.6613\n",
            "Q: 686-79 T: 607 ☒ 697\n",
            "Q: 906-3361 T: -2455 ☒ -2664\n",
            "Q: 41-805 T: -764 ☒ -765\n",
            "Q: 984-843 T: 141 ☒ 110\n",
            "Q: 3-3059 T: -3056 ☒ -3055\n",
            "Q: 822-7181 T: -6359 ☒ -6278\n",
            "Q: 885-4 T: 881 ☑ 881\n",
            "Q: 279-843 T: -564 ☒ -565\n",
            "Q: 34-5 T: 29 ☒ 28\n",
            "Q: 0-8444 T: -8444 ☑ -8444\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 33\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.7808 - loss: 0.5859 - val_accuracy: 0.7484 - val_loss: 0.6440\n",
            "Q: 15-51 T: -36 ☑ -36\n",
            "Q: 824-502 T: 322 ☒ 321\n",
            "Q: 34-4015 T: -3981 ☒ -3980\n",
            "Q: 2-487 T: -485 ☑ -485\n",
            "Q: 252-6 T: 246 ☒ 247\n",
            "Q: 5172-20 T: 5152 ☒ 5263\n",
            "Q: 6-5695 T: -5689 ☑ -5689\n",
            "Q: 194-896 T: -702 ☒ -700\n",
            "Q: 81-8664 T: -8583 ☒ -8694\n",
            "Q: 9-9255 T: -9246 ☒ -9247\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 34\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.7883 - loss: 0.5686 - val_accuracy: 0.7614 - val_loss: 0.6179\n",
            "Q: 2-6661 T: -6659 ☑ -6659\n",
            "Q: 8578-60 T: 8518 ☒ 8491\n",
            "Q: 6487-16 T: 6471 ☒ 6470\n",
            "Q: 1-122 T: -121 ☒ -122\n",
            "Q: 8401-7 T: 8394 ☒ 8393\n",
            "Q: 5-1104 T: -1099 ☒ -1011\n",
            "Q: 4798-399 T: 4399 ☒ 4400\n",
            "Q: 65-72 T: -7 ☒ -8\n",
            "Q: 3690-4 T: 3686 ☒ 3685\n",
            "Q: 840-4633 T: -3793 ☒ -3729\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 35\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.7979 - loss: 0.5460 - val_accuracy: 0.7686 - val_loss: 0.6053\n",
            "Q: 9-48 T: -39 ☒ -49\n",
            "Q: 45-1289 T: -1244 ☒ -1243\n",
            "Q: 13-1223 T: -1210 ☒ -1209\n",
            "Q: 87-248 T: -161 ☑ -161\n",
            "Q: 631-54 T: 577 ☑ 577\n",
            "Q: 8162-33 T: 8129 ☒ 8110\n",
            "Q: 5091-6 T: 5085 ☒ 5065\n",
            "Q: 27-916 T: -889 ☒ -888\n",
            "Q: 115-7 T: 108 ☑ 108\n",
            "Q: 3029-1 T: 3028 ☒ 3008\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 36\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.8040 - loss: 0.5313 - val_accuracy: 0.7650 - val_loss: 0.6036\n",
            "Q: 6708-67 T: 6641 ☒ 6642\n",
            "Q: 7738-32 T: 7706 ☒ 7616\n",
            "Q: 128-528 T: -400 ☒ -490\n",
            "Q: 53-16 T: 37 ☑ 37\n",
            "Q: 76-24 T: 52 ☑ 52\n",
            "Q: 2299-147 T: 2152 ☒ 2208\n",
            "Q: 588-85 T: 503 ☒ 403\n",
            "Q: 8-2609 T: -2601 ☒ -2691\n",
            "Q: 473-562 T: -89 ☒ -18\n",
            "Q: 6-50 T: -44 ☑ -44\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 37\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.8142 - loss: 0.5091 - val_accuracy: 0.7784 - val_loss: 0.5747\n",
            "Q: 68-6558 T: -6490 ☑ -6490\n",
            "Q: 953-7 T: 946 ☑ 946\n",
            "Q: 219-2028 T: -1809 ☒ -1900\n",
            "Q: 7581-7407 T: 174 ☒ 152\n",
            "Q: 0-414 T: -414 ☑ -414\n",
            "Q: 4-2466 T: -2462 ☒ -2562\n",
            "Q: 95-94 T: 1 ☒ 0\n",
            "Q: 8866-0 T: 8866 ☒ 8864\n",
            "Q: 866-300 T: 566 ☒ 577\n",
            "Q: 18-3925 T: -3907 ☒ -3917\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 38\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.8174 - loss: 0.4967 - val_accuracy: 0.7819 - val_loss: 0.5655\n",
            "Q: 88-73 T: 15 ☒ 25\n",
            "Q: 160-553 T: -393 ☒ -333\n",
            "Q: 330-56 T: 274 ☒ 264\n",
            "Q: 635-1 T: 634 ☑ 634\n",
            "Q: 49-821 T: -772 ☑ -772\n",
            "Q: 17-48 T: -31 ☒ -30\n",
            "Q: 986-77 T: 909 ☒ 919\n",
            "Q: 16-179 T: -163 ☑ -163\n",
            "Q: 820-158 T: 662 ☒ 643\n",
            "Q: 63-57 T: 6 ☒ 4\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 39\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.8266 - loss: 0.4794 - val_accuracy: 0.7957 - val_loss: 0.5396\n",
            "Q: 2002-276 T: 1726 ☒ 1656\n",
            "Q: 49-5091 T: -5042 ☒ -5051\n",
            "Q: 477-9534 T: -9057 ☒ -9066\n",
            "Q: 406-9514 T: -9108 ☒ -9033\n",
            "Q: 9-2752 T: -2743 ☒ -2733\n",
            "Q: 487-0 T: 487 ☑ 487\n",
            "Q: 314-759 T: -445 ☒ -465\n",
            "Q: 2793-7076 T: -4283 ☒ -4354\n",
            "Q: 725-18 T: 707 ☑ 707\n",
            "Q: 5001-276 T: 4725 ☒ 4534\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 40\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.8331 - loss: 0.4634 - val_accuracy: 0.7962 - val_loss: 0.5324\n",
            "Q: 5145-112 T: 5033 ☒ 4023\n",
            "Q: 5512-739 T: 4773 ☒ 4733\n",
            "Q: 7669-5 T: 7664 ☑ 7664\n",
            "Q: 241-50 T: 191 ☒ 101\n",
            "Q: 9717-5415 T: 4302 ☒ 4653\n",
            "Q: 792-406 T: 386 ☒ 366\n",
            "Q: 288-4734 T: -4446 ☒ -4345\n",
            "Q: 85-139 T: -54 ☑ -54\n",
            "Q: 1171-985 T: 186 ☒ 975\n",
            "Q: 9-117 T: -108 ☑ -108\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 41\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.8408 - loss: 0.4440 - val_accuracy: 0.8040 - val_loss: 0.5157\n",
            "Q: 5014-369 T: 4645 ☒ 4545\n",
            "Q: 2440-896 T: 1544 ☒ 1454\n",
            "Q: 382-835 T: -453 ☑ -453\n",
            "Q: 444-55 T: 389 ☑ 389\n",
            "Q: 9-72 T: -63 ☑ -63\n",
            "Q: 99-4942 T: -4843 ☒ -4893\n",
            "Q: 35-267 T: -232 ☑ -232\n",
            "Q: 6-3533 T: -3527 ☑ -3527\n",
            "Q: 9752-555 T: 9197 ☒ 9372\n",
            "Q: 44-11 T: 33 ☑ 33\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 42\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.8472 - loss: 0.4287 - val_accuracy: 0.8104 - val_loss: 0.4997\n",
            "Q: 24-915 T: -891 ☑ -891\n",
            "Q: 517-7 T: 510 ☒ 500\n",
            "Q: 0-408 T: -408 ☑ -408\n",
            "Q: 738-93 T: 645 ☑ 645\n",
            "Q: 8016-5225 T: 2791 ☒ 3819\n",
            "Q: 7-5672 T: -5665 ☑ -5665\n",
            "Q: 611-0 T: 611 ☒ 610\n",
            "Q: 5-3476 T: -3471 ☑ -3471\n",
            "Q: 10-94 T: -84 ☒ -85\n",
            "Q: 1334-2772 T: -1438 ☒ -158\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 43\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.8570 - loss: 0.4070 - val_accuracy: 0.8144 - val_loss: 0.4861\n",
            "Q: 978-655 T: 323 ☑ 323\n",
            "Q: 358-71 T: 287 ☑ 287\n",
            "Q: 4033-2 T: 4031 ☑ 4031\n",
            "Q: 5659-0 T: 5659 ☑ 5659\n",
            "Q: 5276-4 T: 5272 ☑ 5272\n",
            "Q: 161-89 T: 72 ☑ 72\n",
            "Q: 7283-9322 T: -2039 ☒ -1499\n",
            "Q: 3343-225 T: 3118 ☒ 3202\n",
            "Q: 152-523 T: -371 ☒ -381\n",
            "Q: 195-8 T: 187 ☑ 187\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 44\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.8595 - loss: 0.4002 - val_accuracy: 0.8242 - val_loss: 0.4690\n",
            "Q: 3208-0 T: 3208 ☑ 3208\n",
            "Q: 7939-38 T: 7901 ☒ 7910\n",
            "Q: 8878-5 T: 8873 ☒ 8853\n",
            "Q: 633-46 T: 587 ☑ 587\n",
            "Q: 3-7368 T: -7365 ☑ -7365\n",
            "Q: 88-40 T: 48 ☑ 48\n",
            "Q: 8-349 T: -341 ☑ -341\n",
            "Q: 7-5132 T: -5125 ☒ -5124\n",
            "Q: 322-4182 T: -3860 ☒ -3880\n",
            "Q: 1-1701 T: -1700 ☑ -1700\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 45\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.8682 - loss: 0.3775 - val_accuracy: 0.8266 - val_loss: 0.4604\n",
            "Q: 706-8685 T: -7979 ☒ -7090\n",
            "Q: 3698-41 T: 3657 ☑ 3657\n",
            "Q: 20-3270 T: -3250 ☑ -3250\n",
            "Q: 8-974 T: -966 ☑ -966\n",
            "Q: 0-5260 T: -5260 ☒ -5261\n",
            "Q: 1207-7 T: 1200 ☒ 1190\n",
            "Q: 88-63 T: 25 ☒ 24\n",
            "Q: 96-810 T: -714 ☒ -715\n",
            "Q: 63-66 T: -3 ☑ -3\n",
            "Q: 5962-3500 T: 2462 ☒ 2491\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 46\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.8718 - loss: 0.3686 - val_accuracy: 0.8282 - val_loss: 0.4580\n",
            "Q: 7-295 T: -288 ☑ -288\n",
            "Q: 4614-95 T: 4519 ☒ 4529\n",
            "Q: 32-5554 T: -5522 ☒ -5523\n",
            "Q: 9-1046 T: -1037 ☑ -1037\n",
            "Q: 77-5213 T: -5136 ☒ -5146\n",
            "Q: 111-95 T: 16 ☒ 1\n",
            "Q: 124-6 T: 118 ☑ 118\n",
            "Q: 951-66 T: 885 ☑ 885\n",
            "Q: 0-6626 T: -6626 ☑ -6626\n",
            "Q: 760-984 T: -224 ☒ -234\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 47\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.8772 - loss: 0.3540 - val_accuracy: 0.8349 - val_loss: 0.4421\n",
            "Q: 1238-931 T: 307 ☒ 462\n",
            "Q: 5833-57 T: 5776 ☒ 5766\n",
            "Q: 6219-471 T: 5748 ☒ 5821\n",
            "Q: 904-9 T: 895 ☒ 894\n",
            "Q: 79-693 T: -614 ☑ -614\n",
            "Q: 5690-4190 T: 1500 ☒ 1490\n",
            "Q: 1851-35 T: 1816 ☒ 1856\n",
            "Q: 8376-5971 T: 2405 ☒ 3654\n",
            "Q: 35-907 T: -872 ☒ -862\n",
            "Q: 450-60 T: 390 ☑ 390\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 48\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.8795 - loss: 0.3457 - val_accuracy: 0.8336 - val_loss: 0.4450\n",
            "Q: 4863-2 T: 4861 ☑ 4861\n",
            "Q: 1137-8 T: 1129 ☑ 1129\n",
            "Q: 2-3272 T: -3270 ☑ -3270\n",
            "Q: 8830-737 T: 8093 ☒ 8023\n",
            "Q: 285-56 T: 229 ☑ 229\n",
            "Q: 2681-4496 T: -1815 ☒ -1826\n",
            "Q: 5898-9 T: 5889 ☑ 5889\n",
            "Q: 7743-19 T: 7724 ☑ 7724\n",
            "Q: 0-8444 T: -8444 ☒ -8445\n",
            "Q: 7-599 T: -592 ☑ -592\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 49\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.8876 - loss: 0.3308 - val_accuracy: 0.8409 - val_loss: 0.4264\n",
            "Q: 5-313 T: -308 ☑ -308\n",
            "Q: 254-55 T: 199 ☒ 190\n",
            "Q: 221-5 T: 216 ☑ 216\n",
            "Q: 85-139 T: -54 ☑ -54\n",
            "Q: 501-9 T: 492 ☑ 492\n",
            "Q: 2-23 T: -21 ☑ -21\n",
            "Q: 4247-81 T: 4166 ☒ 4176\n",
            "Q: 3379-8473 T: -5094 ☒ -5044\n",
            "Q: 3-8272 T: -8269 ☒ -8270\n",
            "Q: 642-50 T: 592 ☑ 592\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 50\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.8892 - loss: 0.3185 - val_accuracy: 0.8473 - val_loss: 0.4105\n",
            "Q: 290-953 T: -663 ☑ -663\n",
            "Q: 725-575 T: 150 ☒ 189\n",
            "Q: 4779-8 T: 4771 ☒ 4760\n",
            "Q: 8703-42 T: 8661 ☒ 8671\n",
            "Q: 1078-8 T: 1070 ☒ 1060\n",
            "Q: 3519-5 T: 3514 ☑ 3514\n",
            "Q: 20-8928 T: -8908 ☑ -8908\n",
            "Q: 0-32 T: -32 ☑ -32\n",
            "Q: 523-10 T: 513 ☑ 513\n",
            "Q: 989-41 T: 948 ☑ 948\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXJQqZbEbRup"
      },
      "source": [
        "1.2).\n",
        "\n",
        "a) Do you think this model performs well?  Why or why not?     \n",
        "b) What are its limitations?   \n",
        "c) What would you do to improve it?    \n",
        "d) Can you apply an attention mechanism to this model? Why or why not?   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSl-5h6KWn0u"
      },
      "source": [
        "## 1.2) my answer:\n",
        "a) The model demonstrates moderate but not exceptional performance in handling arithmetic subtraction tasks. With a validation accuracy of around 85%, it successfully handles basic subtractions and some complex cases, but struggles with situations requiring precise calculations, especially those involving carrying operations or close number differences. While it can reliably handle simple cases and negative numbers, its accuracy drops when dealing with more nuanced calculations, particularly those involving large numbers with small differences or problems requiring multiple carrying operations. This level of performance suggests that while the model has learned the basic patterns of subtraction, it hasn't fully mastered the more intricate aspects of arithmetic operations that would be necessary for real-world applications.\n",
        "\n",
        "b)\n",
        "\n",
        "Fixed Input Length: The model is constrained to handle only numbers up to a certain digit length (4 digits in this case)\n",
        "Single Operation: It's specialized for subtraction only and can't handle other arithmetic operations\n",
        "No Explicit Carrying Mechanism: The LSTM has to learn carrying/borrowing implicitly\n",
        "Limited Generalization: The validation accuracy suggests it may not generalize perfectly to unseen number combinations\n",
        "\n",
        "c)\n",
        "\n",
        "The architecture could be strengthened by adding residual connections and attention mechanisms, helping the model better handle complex calculations. The training process could be improved through curriculum learning, starting with simple problems and gradually increasing difficulty, while also expanding the training data to include more challenging edge cases like numbers requiring multiple carries or those with similar digits.\n",
        "\n",
        "d)\n",
        "\n",
        "Yes, an attention mechanism can be applied to this model, and it would be particularly beneficial for arithmetic operations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wvRhhOcgmrQ"
      },
      "source": [
        "1.3).  \n",
        "\n",
        "Add attention to the model. Evaluate the performance against the `seq2seq` you trained above. Which one is performing better?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def attention_model(maxlen, n_chars, config):\n",
        "\n",
        "    input_layer = Input(shape=(maxlen, n_chars))\n",
        "\n",
        "    # Encoder\n",
        "    encoder = LSTM(config[\"hidden_size\"], return_sequences=True, return_state=True)(input_layer)\n",
        "    encoder_outputs, state_h, state_c = encoder\n",
        "\n",
        "    # Decoder\n",
        "    decoder = RepeatVector(config[\"digits\"] + 1)(state_h)\n",
        "    decoder = LSTM(config[\"hidden_size\"], return_sequences=True)(decoder)\n",
        "\n",
        "    attention = Attention()([decoder, encoder_outputs])\n",
        "\n",
        "    decoder_combined = tf.keras.layers.Concatenate()([decoder, attention])\n",
        "\n",
        "    output = TimeDistributed(Dense(n_chars, activation='softmax'))(decoder_combined)\n",
        "\n",
        "    model = tf.keras.Model(inputs=input_layer, outputs=output)\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                 optimizer='adam',\n",
        "                 metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "model = attention_model(\n",
        "    maxlen=maxlen,\n",
        "    n_chars=len(chars),\n",
        "    config=config\n",
        ")\n",
        "model.summary()\n",
        "\n",
        "# Training loop with updated prediction handling\n",
        "for iteration in range(config[\"iterations\"]):\n",
        "    print()\n",
        "    print('-' * 50)\n",
        "    print('Iteration', iteration + 1)\n",
        "\n",
        "    model.fit(x_train, y_train,\n",
        "             batch_size=config[\"batch_size\"],\n",
        "             epochs=1,\n",
        "             validation_data=(x_val, y_val))\n",
        "\n",
        "    # Validate on 10 random samples\n",
        "    for i in range(10):\n",
        "        ind = np.random.randint(0, len(x_val))\n",
        "        rowx, rowy = x_val[np.array([ind])], y_val[np.array([ind])]\n",
        "\n",
        "        # Updated prediction handling - using argmax on the raw predictions\n",
        "        preds = model.predict(rowx, verbose=0)\n",
        "        pred_classes = np.argmax(preds[0], axis=-1)\n",
        "\n",
        "        q = ctable.decode(rowx[0])\n",
        "        correct = ctable.decode(rowy[0])\n",
        "        guess = ''.join(ctable.indices_char[x] for x in pred_classes)\n",
        "\n",
        "        print('Q:', q.strip(), end=' ')\n",
        "        print('T:', correct.strip(), end=' ')\n",
        "        if correct.strip() == guess.strip():\n",
        "            print('☑', end=' ')\n",
        "        else:\n",
        "            print('☒', end=' ')\n",
        "        print(guess.strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qgv4LEVBOlXh",
        "outputId": "022ad96f-6f19-42db-bea8-6205e1655532"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_3             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m13\u001b[0m)          │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm_4 (\u001b[38;5;33mLSTM\u001b[0m)             │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m128\u001b[0m),       │         \u001b[38;5;34m72,704\u001b[0m │ input_layer_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│                           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,    │                │                        │\n",
              "│                           │ \u001b[38;5;34m128\u001b[0m)]                  │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ repeat_vector_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ lstm_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m]           │\n",
              "│ (\u001b[38;5;33mRepeatVector\u001b[0m)            │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm_5 (\u001b[38;5;33mLSTM\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │        \u001b[38;5;34m131,584\u001b[0m │ repeat_vector_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ attention_1 (\u001b[38;5;33mAttention\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ lstm_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],          │\n",
              "│                           │                        │                │ lstm_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate (\u001b[38;5;33mConcatenate\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ lstm_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],          │\n",
              "│                           │                        │                │ attention_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ time_distributed_1        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m13\u001b[0m)          │          \u001b[38;5;34m3,341\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)         │                        │                │                        │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_3             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>)          │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)             │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>),       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">72,704</span> │ input_layer_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│                           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,    │                │                        │\n",
              "│                           │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)]                  │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ repeat_vector_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>]           │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RepeatVector</span>)            │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │ repeat_vector_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ attention_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Attention</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],          │\n",
              "│                           │                        │                │ lstm_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],          │\n",
              "│                           │                        │                │ attention_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ time_distributed_1        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">3,341</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)         │                        │                │                        │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m207,629\u001b[0m (811.05 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">207,629</span> (811.05 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m207,629\u001b[0m (811.05 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">207,629</span> (811.05 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------------------------------\n",
            "Iteration 1\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 11ms/step - accuracy: 0.3007 - loss: 2.0927 - val_accuracy: 0.4025 - val_loss: 1.7654\n",
            "Q: 568-26 T: 542 ☒ 566\n",
            "Q: 6978-50 T: 6928 ☒ 666\n",
            "Q: 5160-322 T: 4838 ☒ 250\n",
            "Q: 2-3873 T: -3871 ☒ -2222\n",
            "Q: 4523-9 T: 4514 ☒ 540\n",
            "Q: 186-4149 T: -3963 ☒ -100\n",
            "Q: 2-5897 T: -5895 ☒ -2222\n",
            "Q: 3760-530 T: 3230 ☒ 266\n",
            "Q: 557-33 T: 524 ☒ 550\n",
            "Q: 4-8061 T: -8057 ☒ -1620\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 2\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.4218 - loss: 1.6356 - val_accuracy: 0.4406 - val_loss: 1.5686\n",
            "Q: 93-46 T: 47 ☒ 33\n",
            "Q: 6724-70 T: 6654 ☒ 6666\n",
            "Q: 8878-5 T: 8873 ☒ 8888\n",
            "Q: 83-2584 T: -2501 ☒ -2208\n",
            "Q: 48-7204 T: -7156 ☒ -4708\n",
            "Q: 2411-97 T: 2314 ☒ 1112\n",
            "Q: 794-3326 T: -2532 ☒ -309\n",
            "Q: 8692-67 T: 8625 ☒ 6888\n",
            "Q: 805-8504 T: -7699 ☒ -500\n",
            "Q: 43-44 T: -1 ☒ 33\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 3\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.4463 - loss: 1.5381 - val_accuracy: 0.4561 - val_loss: 1.4884\n",
            "Q: 153-6179 T: -6026 ☒ -6000\n",
            "Q: 94-55 T: 39 ☒ 44\n",
            "Q: 28-1997 T: -1969 ☒ -8802\n",
            "Q: 64-103 T: -39 ☒ -30\n",
            "Q: 65-438 T: -373 ☒ -358\n",
            "Q: 63-632 T: -569 ☒ -208\n",
            "Q: 212-74 T: 138 ☒ 21\n",
            "Q: 46-7461 T: -7415 ☒ -6644\n",
            "Q: 5987-96 T: 5891 ☒ 8885\n",
            "Q: 1269-3839 T: -2570 ☒ -2211\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 4\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.4588 - loss: 1.4701 - val_accuracy: 0.4721 - val_loss: 1.4239\n",
            "Q: 20-408 T: -388 ☒ -407\n",
            "Q: 493-281 T: 212 ☒ 349\n",
            "Q: 115-471 T: -356 ☒ -11\n",
            "Q: 22-2937 T: -2915 ☒ -2222\n",
            "Q: 8929-20 T: 8909 ☒ 8889\n",
            "Q: 163-4740 T: -4577 ☒ -4666\n",
            "Q: 526-925 T: -399 ☒ -555\n",
            "Q: 339-68 T: 271 ☒ 33\n",
            "Q: 4371-267 T: 4104 ☒ 3477\n",
            "Q: 58-7341 T: -7283 ☒ -8555\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 5\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.4791 - loss: 1.4040 - val_accuracy: 0.4878 - val_loss: 1.3676\n",
            "Q: 966-24 T: 942 ☒ 666\n",
            "Q: 68-932 T: -864 ☒ -866\n",
            "Q: 55-4859 T: -4804 ☒ -5485\n",
            "Q: 4-6998 T: -6994 ☒ -9993\n",
            "Q: 296-9202 T: -8906 ☒ -9096\n",
            "Q: 950-7 T: 943 ☒ 999\n",
            "Q: 246-3 T: 243 ☒ 222\n",
            "Q: 13-362 T: -349 ☒ -336\n",
            "Q: 4642-9207 T: -4565 ☒ -4444\n",
            "Q: 216-2766 T: -2550 ☒ -2566\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 6\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.5012 - loss: 1.3418 - val_accuracy: 0.5145 - val_loss: 1.3166\n",
            "Q: 84-980 T: -896 ☒ -800\n",
            "Q: 6-6334 T: -6328 ☒ -6320\n",
            "Q: 7-837 T: -830 ☒ -870\n",
            "Q: 3992-2421 T: 1571 ☒ 299\n",
            "Q: 67-4982 T: -4915 ☒ -4766\n",
            "Q: 46-3746 T: -3700 ☒ -3340\n",
            "Q: 2-487 T: -485 ☒ -442\n",
            "Q: 97-7214 T: -7117 ☒ -7000\n",
            "Q: 33-195 T: -162 ☒ -103\n",
            "Q: 97-2745 T: -2648 ☒ -2299\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 7\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.5224 - loss: 1.2892 - val_accuracy: 0.5337 - val_loss: 1.2579\n",
            "Q: 312-3524 T: -3212 ☒ -3099\n",
            "Q: 42-92 T: -50 ☒ -44\n",
            "Q: 444-55 T: 389 ☒ 444\n",
            "Q: 234-8 T: 226 ☒ 222\n",
            "Q: 2940-1034 T: 1906 ☒ 2999\n",
            "Q: 9189-4718 T: 4471 ☒ 1999\n",
            "Q: 67-244 T: -177 ☒ -266\n",
            "Q: 161-89 T: 72 ☒ 11\n",
            "Q: 5-349 T: -344 ☒ -330\n",
            "Q: 4416-338 T: 4078 ☒ 4009\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 8\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.5445 - loss: 1.2325 - val_accuracy: 0.5506 - val_loss: 1.2200\n",
            "Q: 122-58 T: 64 ☒ 14\n",
            "Q: 0-6533 T: -6533 ☒ -6330\n",
            "Q: 1-3535 T: -3534 ☒ -3531\n",
            "Q: 388-9092 T: -8704 ☒ -8833\n",
            "Q: 5-7792 T: -7787 ☒ -7740\n",
            "Q: 8878-5 T: 8873 ☒ 8888\n",
            "Q: 25-242 T: -217 ☒ -110\n",
            "Q: 2781-273 T: 2508 ☒ 2744\n",
            "Q: 524-69 T: 455 ☒ 478\n",
            "Q: 241-50 T: 191 ☒ 144\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 9\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.5588 - loss: 1.1976 - val_accuracy: 0.5627 - val_loss: 1.1680\n",
            "Q: 130-50 T: 80 ☒ 19\n",
            "Q: 167-65 T: 102 ☒ 111\n",
            "Q: 3884-89 T: 3795 ☒ 3783\n",
            "Q: 5575-726 T: 4849 ☒ 4709\n",
            "Q: 9-4410 T: -4401 ☒ -4429\n",
            "Q: 4670-1 T: 4669 ☒ 4666\n",
            "Q: 5512-739 T: 4773 ☒ 5508\n",
            "Q: 3-9722 T: -9719 ☒ -9935\n",
            "Q: 5-766 T: -761 ☑ -761\n",
            "Q: 924-59 T: 865 ☒ 999\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 10\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.5729 - loss: 1.1505 - val_accuracy: 0.5783 - val_loss: 1.1409\n",
            "Q: 7794-524 T: 7270 ☒ 7449\n",
            "Q: 706-640 T: 66 ☒ 14\n",
            "Q: 8601-8 T: 8593 ☒ 8600\n",
            "Q: 22-2937 T: -2915 ☒ -2988\n",
            "Q: 7051-69 T: 6982 ☒ 7099\n",
            "Q: 167-8452 T: -8285 ☒ -8268\n",
            "Q: 412-51 T: 361 ☒ 366\n",
            "Q: 7-6598 T: -6591 ☒ -6588\n",
            "Q: 420-1212 T: -792 ☒ -100\n",
            "Q: 5950-61 T: 5889 ☒ 5777\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 11\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.5864 - loss: 1.1183 - val_accuracy: 0.5944 - val_loss: 1.0974\n",
            "Q: 3613-9350 T: -5737 ☒ -5333\n",
            "Q: 6613-3 T: 6610 ☒ 6613\n",
            "Q: 4476-84 T: 4392 ☒ 4413\n",
            "Q: 8880-445 T: 8435 ☒ 8388\n",
            "Q: 3045-39 T: 3006 ☑ 3006\n",
            "Q: 5892-1 T: 5891 ☒ 5882\n",
            "Q: 59-65 T: -6 ☒ 11\n",
            "Q: 11-930 T: -919 ☒ -918\n",
            "Q: 6160-41 T: 6119 ☒ 6161\n",
            "Q: 5-744 T: -739 ☒ -745\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 12\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.5978 - loss: 1.0890 - val_accuracy: 0.5943 - val_loss: 1.0857\n",
            "Q: 76-803 T: -727 ☒ -722\n",
            "Q: 9-9353 T: -9344 ☒ -9354\n",
            "Q: 1-125 T: -124 ☒ -122\n",
            "Q: 2-5609 T: -5607 ☒ -5599\n",
            "Q: 9155-2 T: 9153 ☒ 9184\n",
            "Q: 894-6241 T: -5347 ☒ -5444\n",
            "Q: 7455-74 T: 7381 ☒ 7366\n",
            "Q: 82-10 T: 72 ☒ 70\n",
            "Q: 237-9293 T: -9056 ☒ -9095\n",
            "Q: 4-5688 T: -5684 ☒ -5666\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 13\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.6058 - loss: 1.0620 - val_accuracy: 0.6059 - val_loss: 1.0533\n",
            "Q: 714-566 T: 148 ☒ 118\n",
            "Q: 5705-6 T: 5699 ☒ 5772\n",
            "Q: 686-54 T: 632 ☒ 615\n",
            "Q: 3156-2596 T: 560 ☒ 108\n",
            "Q: 199-7 T: 192 ☒ 184\n",
            "Q: 1298-6 T: 1292 ☒ 1188\n",
            "Q: 6-976 T: -970 ☒ -969\n",
            "Q: 820-158 T: 662 ☒ 672\n",
            "Q: 58-464 T: -406 ☒ -408\n",
            "Q: 659-7574 T: -6915 ☒ -6188\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 14\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.6177 - loss: 1.0349 - val_accuracy: 0.6042 - val_loss: 1.0465\n",
            "Q: 7-9290 T: -9283 ☑ -9283\n",
            "Q: 1586-77 T: 1509 ☒ 1537\n",
            "Q: 4134-52 T: 4082 ☒ 4126\n",
            "Q: 3433-9138 T: -5705 ☒ -5600\n",
            "Q: 221-5 T: 216 ☒ 227\n",
            "Q: 3156-2596 T: 560 ☒ 100\n",
            "Q: 304-4 T: 300 ☒ 309\n",
            "Q: 6647-99 T: 6548 ☒ 6463\n",
            "Q: 41-812 T: -771 ☒ -766\n",
            "Q: 33-329 T: -296 ☑ -296\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 15\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.6240 - loss: 1.0123 - val_accuracy: 0.6219 - val_loss: 1.0132\n",
            "Q: 2-443 T: -441 ☒ -442\n",
            "Q: 6702-4 T: 6698 ☒ 6600\n",
            "Q: 111-95 T: 16 ☒ 15\n",
            "Q: 7592-2013 T: 5579 ☒ 5699\n",
            "Q: 5-850 T: -845 ☒ -840\n",
            "Q: 28-220 T: -192 ☒ -199\n",
            "Q: 44-3049 T: -3005 ☒ -3990\n",
            "Q: 2-645 T: -643 ☑ -643\n",
            "Q: 1366-70 T: 1296 ☒ 1335\n",
            "Q: 439-858 T: -419 ☒ -480\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 16\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.6346 - loss: 0.9879 - val_accuracy: 0.6251 - val_loss: 0.9979\n",
            "Q: 500-7289 T: -6789 ☒ -6888\n",
            "Q: 1-660 T: -659 ☒ -660\n",
            "Q: 4467-455 T: 4012 ☒ 4903\n",
            "Q: 36-136 T: -100 ☒ -10\n",
            "Q: 5258-525 T: 4733 ☒ 4965\n",
            "Q: 470-541 T: -71 ☒ -15\n",
            "Q: 20-408 T: -388 ☒ -399\n",
            "Q: 9233-1492 T: 7741 ☒ 8880\n",
            "Q: 0-5007 T: -5007 ☒ -5009\n",
            "Q: 1463-8 T: 1455 ☒ 1458\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 17\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.6410 - loss: 0.9700 - val_accuracy: 0.6302 - val_loss: 0.9843\n",
            "Q: 2404-6191 T: -3787 ☒ -2522\n",
            "Q: 55-948 T: -893 ☒ -895\n",
            "Q: 97-954 T: -857 ☒ -856\n",
            "Q: 8512-63 T: 8449 ☒ 8416\n",
            "Q: 3-12 T: -9 ☒ -1\n",
            "Q: 482-5822 T: -5340 ☒ -5977\n",
            "Q: 4241-4047 T: 194 ☒ -76\n",
            "Q: 72-597 T: -525 ☒ -511\n",
            "Q: 6383-4398 T: 1985 ☒ 2863\n",
            "Q: 162-417 T: -255 ☒ -266\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 18\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.6488 - loss: 0.9471 - val_accuracy: 0.6394 - val_loss: 0.9615\n",
            "Q: 874-923 T: -49 ☒ -1\n",
            "Q: 53-9831 T: -9778 ☒ -9884\n",
            "Q: 8171-3852 T: 4319 ☒ 4699\n",
            "Q: 3749-463 T: 3286 ☒ 3112\n",
            "Q: 7-3226 T: -3219 ☒ -3222\n",
            "Q: 1262-9752 T: -8490 ☒ -7585\n",
            "Q: 3-253 T: -250 ☒ -251\n",
            "Q: 44-9758 T: -9714 ☒ -9624\n",
            "Q: 230-70 T: 160 ☒ 252\n",
            "Q: 65-946 T: -881 ☒ -880\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 19\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.6554 - loss: 0.9307 - val_accuracy: 0.6441 - val_loss: 0.9502\n",
            "Q: 3760-530 T: 3230 ☒ 3169\n",
            "Q: 3059-204 T: 2855 ☒ 2966\n",
            "Q: 41-805 T: -764 ☒ -752\n",
            "Q: 9891-6253 T: 3638 ☒ 3099\n",
            "Q: 221-9185 T: -8964 ☒ -9064\n",
            "Q: 6166-3690 T: 2476 ☒ 2889\n",
            "Q: 26-91 T: -65 ☑ -65\n",
            "Q: 7772-344 T: 7428 ☒ 7319\n",
            "Q: 1-1069 T: -1068 ☒ -1055\n",
            "Q: 2-8436 T: -8434 ☒ -8444\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 20\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.6600 - loss: 0.9158 - val_accuracy: 0.6485 - val_loss: 0.9325\n",
            "Q: 21-527 T: -506 ☒ -505\n",
            "Q: 124-6 T: 118 ☒ 124\n",
            "Q: 6761-4 T: 6757 ☒ 6651\n",
            "Q: 4523-9 T: 4514 ☒ 4496\n",
            "Q: 2825-4262 T: -1437 ☒ -144\n",
            "Q: 0-8984 T: -8984 ☒ -8897\n",
            "Q: 374-8645 T: -8271 ☒ -8292\n",
            "Q: 575-2 T: 573 ☒ 575\n",
            "Q: 7331-91 T: 7240 ☒ 7245\n",
            "Q: 901-9325 T: -8424 ☒ -8525\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 21\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.6688 - loss: 0.8924 - val_accuracy: 0.6539 - val_loss: 0.9182\n",
            "Q: 672-9412 T: -8740 ☒ -8766\n",
            "Q: 552-8841 T: -8289 ☒ -8251\n",
            "Q: 45-971 T: -926 ☒ -935\n",
            "Q: 724-2724 T: -2000 ☒ -2902\n",
            "Q: 340-229 T: 111 ☒ 104\n",
            "Q: 2295-155 T: 2140 ☒ 2148\n",
            "Q: 4389-5 T: 4384 ☑ 4384\n",
            "Q: 9901-622 T: 9279 ☒ 9080\n",
            "Q: 14-28 T: -14 ☑ -14\n",
            "Q: 79-164 T: -85 ☒ -88\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 22\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.6732 - loss: 0.8801 - val_accuracy: 0.6560 - val_loss: 0.9148\n",
            "Q: 7709-4 T: 7705 ☒ 7704\n",
            "Q: 173-56 T: 117 ☒ 111\n",
            "Q: 7520-629 T: 6891 ☒ 6793\n",
            "Q: 6-5695 T: -5689 ☒ -5681\n",
            "Q: 85-6606 T: -6521 ☒ -6500\n",
            "Q: 59-2 T: 57 ☑ 57\n",
            "Q: 3387-5 T: 3382 ☒ 3389\n",
            "Q: 50-4531 T: -4481 ☒ -4409\n",
            "Q: 558-933 T: -375 ☒ -345\n",
            "Q: 8299-8 T: 8291 ☒ 8289\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 23\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.6805 - loss: 0.8608 - val_accuracy: 0.6632 - val_loss: 0.8903\n",
            "Q: 8-3032 T: -3024 ☒ -3032\n",
            "Q: 578-6974 T: -6396 ☒ -6222\n",
            "Q: 753-3 T: 750 ☒ 752\n",
            "Q: 65-438 T: -373 ☒ -379\n",
            "Q: 8565-1 T: 8564 ☒ 8556\n",
            "Q: 7379-5862 T: 1517 ☒ 2199\n",
            "Q: 896-0 T: 896 ☒ 886\n",
            "Q: 9-2752 T: -2743 ☒ -2729\n",
            "Q: 9135-35 T: 9100 ☒ 9176\n",
            "Q: 2-3222 T: -3220 ☒ -3229\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 24\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.6843 - loss: 0.8489 - val_accuracy: 0.6590 - val_loss: 0.8947\n",
            "Q: 953-7 T: 946 ☑ 946\n",
            "Q: 1307-855 T: 452 ☒ 465\n",
            "Q: 692-69 T: 623 ☒ 613\n",
            "Q: 7-6443 T: -6436 ☒ -6437\n",
            "Q: 3-83 T: -80 ☒ -70\n",
            "Q: 8163-4 T: 8159 ☒ 8153\n",
            "Q: 57-99 T: -42 ☒ -43\n",
            "Q: 53-57 T: -4 ☒ -1\n",
            "Q: 9-8239 T: -8230 ☒ -8219\n",
            "Q: 30-4326 T: -4296 ☒ -4203\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 25\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.6892 - loss: 0.8364 - val_accuracy: 0.6703 - val_loss: 0.8752\n",
            "Q: 8-7890 T: -7882 ☒ -7889\n",
            "Q: 4-709 T: -705 ☒ -706\n",
            "Q: 61-377 T: -316 ☒ -321\n",
            "Q: 552-385 T: 167 ☒ 15\n",
            "Q: 5172-3 T: 5169 ☒ 5150\n",
            "Q: 43-7398 T: -7355 ☒ -7338\n",
            "Q: 85-61 T: 24 ☒ 23\n",
            "Q: 2-5897 T: -5895 ☒ -5887\n",
            "Q: 6-894 T: -888 ☒ -889\n",
            "Q: 24-910 T: -886 ☒ -887\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 26\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.6964 - loss: 0.8182 - val_accuracy: 0.6744 - val_loss: 0.8605\n",
            "Q: 7729-4288 T: 3441 ☒ 3499\n",
            "Q: 15-3709 T: -3694 ☒ -3699\n",
            "Q: 732-6586 T: -5854 ☒ -5999\n",
            "Q: 9228-848 T: 8380 ☒ 8433\n",
            "Q: 7881-26 T: 7855 ☒ 7838\n",
            "Q: 156-9 T: 147 ☒ 158\n",
            "Q: 2609-877 T: 1732 ☒ 1725\n",
            "Q: 71-99 T: -28 ☒ -22\n",
            "Q: 304-4 T: 300 ☒ 397\n",
            "Q: 295-0 T: 295 ☑ 295\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 27\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.6989 - loss: 0.8084 - val_accuracy: 0.6775 - val_loss: 0.8450\n",
            "Q: 5091-6 T: 5085 ☒ 5092\n",
            "Q: 9-603 T: -594 ☒ -590\n",
            "Q: 549-89 T: 460 ☒ 458\n",
            "Q: 688-567 T: 121 ☒ 200\n",
            "Q: 5-3762 T: -3757 ☒ -3761\n",
            "Q: 442-17 T: 425 ☒ 424\n",
            "Q: 289-0 T: 289 ☒ 285\n",
            "Q: 549-89 T: 460 ☒ 458\n",
            "Q: 75-992 T: -917 ☒ -915\n",
            "Q: 9379-0 T: 9379 ☒ 9364\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 28\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.7084 - loss: 0.7861 - val_accuracy: 0.6807 - val_loss: 0.8414\n",
            "Q: 617-65 T: 552 ☒ 558\n",
            "Q: 4-1878 T: -1874 ☑ -1874\n",
            "Q: 870-13 T: 857 ☒ 858\n",
            "Q: 3830-1660 T: 2170 ☒ 2226\n",
            "Q: 583-758 T: -175 ☒ -189\n",
            "Q: 470-541 T: -71 ☒ -84\n",
            "Q: 46-773 T: -727 ☒ -738\n",
            "Q: 4283-8 T: 4275 ☒ 4276\n",
            "Q: 5527-96 T: 5431 ☒ 5464\n",
            "Q: 4102-5024 T: -922 ☒ -1122\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 29\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.7115 - loss: 0.7775 - val_accuracy: 0.6723 - val_loss: 0.8633\n",
            "Q: 4-834 T: -830 ☒ -831\n",
            "Q: 103-660 T: -557 ☒ -556\n",
            "Q: 505-59 T: 446 ☒ 445\n",
            "Q: 75-720 T: -645 ☒ -648\n",
            "Q: 2-2922 T: -2920 ☒ -2919\n",
            "Q: 4947-4 T: 4943 ☒ 4941\n",
            "Q: 49-941 T: -892 ☒ -880\n",
            "Q: 7-118 T: -111 ☑ -111\n",
            "Q: 86-720 T: -634 ☒ -633\n",
            "Q: 8830-737 T: 8093 ☒ 8069\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 30\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.7146 - loss: 0.7662 - val_accuracy: 0.6893 - val_loss: 0.8192\n",
            "Q: 5741-8122 T: -2381 ☒ -2433\n",
            "Q: 3169-42 T: 3127 ☒ 3103\n",
            "Q: 0-12 T: -12 ☒ -11\n",
            "Q: 82-567 T: -485 ☒ -496\n",
            "Q: 504-7 T: 497 ☑ 497\n",
            "Q: 66-4703 T: -4637 ☒ -4643\n",
            "Q: 6852-5292 T: 1560 ☒ 1044\n",
            "Q: 29-8196 T: -8167 ☒ -8162\n",
            "Q: 122-9 T: 113 ☒ 115\n",
            "Q: 354-503 T: -149 ☒ -167\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 31\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.7213 - loss: 0.7501 - val_accuracy: 0.6935 - val_loss: 0.8071\n",
            "Q: 5-8430 T: -8425 ☒ -8428\n",
            "Q: 1317-2 T: 1315 ☒ 1336\n",
            "Q: 0-870 T: -870 ☑ -870\n",
            "Q: 9-383 T: -374 ☒ -375\n",
            "Q: 400-92 T: 308 ☒ 312\n",
            "Q: 98-20 T: 78 ☒ 77\n",
            "Q: 460-4 T: 456 ☒ 455\n",
            "Q: 9271-1472 T: 7799 ☒ 8820\n",
            "Q: 628-8 T: 620 ☒ 611\n",
            "Q: 5430-552 T: 4878 ☒ 4880\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 32\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.7255 - loss: 0.7400 - val_accuracy: 0.6965 - val_loss: 0.7937\n",
            "Q: 1780-5 T: 1775 ☒ 1770\n",
            "Q: 258-27 T: 231 ☒ 230\n",
            "Q: 672-72 T: 600 ☒ 603\n",
            "Q: 80-2009 T: -1929 ☒ -1904\n",
            "Q: 85-965 T: -880 ☑ -880\n",
            "Q: 386-895 T: -509 ☒ -596\n",
            "Q: 3503-5 T: 3498 ☒ 3406\n",
            "Q: 1207-7 T: 1200 ☒ 1109\n",
            "Q: 9589-530 T: 9059 ☒ 9933\n",
            "Q: 8164-7166 T: 998 ☒ 113\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 33\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.7303 - loss: 0.7250 - val_accuracy: 0.6958 - val_loss: 0.8007\n",
            "Q: 377-500 T: -123 ☒ -216\n",
            "Q: 9351-5 T: 9346 ☒ 9344\n",
            "Q: 2-883 T: -881 ☒ -880\n",
            "Q: 760-984 T: -224 ☒ -206\n",
            "Q: 3863-31 T: 3832 ☒ 3802\n",
            "Q: 3-9722 T: -9719 ☒ -9720\n",
            "Q: 388-9092 T: -8704 ☒ -8909\n",
            "Q: 713-153 T: 560 ☒ 569\n",
            "Q: 6611-9590 T: -2979 ☒ -3108\n",
            "Q: 7283-9322 T: -2039 ☒ -2199\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 34\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.7349 - loss: 0.7150 - val_accuracy: 0.7006 - val_loss: 0.7814\n",
            "Q: 4-6185 T: -6181 ☒ -6184\n",
            "Q: 192-5 T: 187 ☒ 186\n",
            "Q: 4682-60 T: 4622 ☒ 4613\n",
            "Q: 36-11 T: 25 ☒ 35\n",
            "Q: 82-9216 T: -9134 ☒ -9157\n",
            "Q: 27-919 T: -892 ☒ -997\n",
            "Q: 7049-11 T: 7038 ☒ 7021\n",
            "Q: 4752-8029 T: -3277 ☒ -3367\n",
            "Q: 1-1701 T: -1700 ☑ -1700\n",
            "Q: 3-1572 T: -1569 ☒ -1571\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 35\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.7381 - loss: 0.7047 - val_accuracy: 0.7032 - val_loss: 0.7775\n",
            "Q: 924-59 T: 865 ☒ 877\n",
            "Q: 28-1997 T: -1969 ☒ -1964\n",
            "Q: 297-3 T: 294 ☒ 295\n",
            "Q: 896-0 T: 896 ☒ 895\n",
            "Q: 5633-554 T: 5079 ☒ 5980\n",
            "Q: 2-6661 T: -6659 ☒ -6650\n",
            "Q: 3872-857 T: 3015 ☒ 3066\n",
            "Q: 5-3476 T: -3471 ☑ -3471\n",
            "Q: 952-238 T: 714 ☒ 715\n",
            "Q: 2603-66 T: 2537 ☒ 2566\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 36\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.7433 - loss: 0.6916 - val_accuracy: 0.7043 - val_loss: 0.7811\n",
            "Q: 1354-215 T: 1139 ☒ 1223\n",
            "Q: 315-87 T: 228 ☒ 225\n",
            "Q: 6-120 T: -114 ☒ -105\n",
            "Q: 402-69 T: 333 ☒ 344\n",
            "Q: 9-9003 T: -8994 ☒ -8001\n",
            "Q: 84-1582 T: -1498 ☒ -1572\n",
            "Q: 400-341 T: 59 ☒ 66\n",
            "Q: 10-3639 T: -3629 ☒ -3622\n",
            "Q: 965-296 T: 669 ☒ 678\n",
            "Q: 31-4841 T: -4810 ☒ -4818\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 37\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.7441 - loss: 0.6899 - val_accuracy: 0.7122 - val_loss: 0.7497\n",
            "Q: 8878-5 T: 8873 ☑ 8873\n",
            "Q: 30-705 T: -675 ☒ -672\n",
            "Q: 2465-4 T: 2461 ☒ 2452\n",
            "Q: 3415-10 T: 3405 ☒ 3404\n",
            "Q: 0-817 T: -817 ☑ -817\n",
            "Q: 5789-534 T: 5255 ☒ 5244\n",
            "Q: 9320-3226 T: 6094 ☒ 5994\n",
            "Q: 269-8 T: 261 ☒ 260\n",
            "Q: 9373-2 T: 9371 ☑ 9371\n",
            "Q: 17-102 T: -85 ☒ -91\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 38\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.7519 - loss: 0.6682 - val_accuracy: 0.7038 - val_loss: 0.7785\n",
            "Q: 331-9328 T: -8997 ☒ -9011\n",
            "Q: 99-762 T: -663 ☒ -669\n",
            "Q: 150-8194 T: -8044 ☒ -8011\n",
            "Q: 5748-583 T: 5165 ☒ 5074\n",
            "Q: 738-697 T: 41 ☒ -2\n",
            "Q: 183-8223 T: -8040 ☒ -8041\n",
            "Q: 175-58 T: 117 ☒ 118\n",
            "Q: 7481-895 T: 6586 ☒ 6666\n",
            "Q: 1332-87 T: 1245 ☒ 1254\n",
            "Q: 965-296 T: 669 ☒ 671\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 39\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.7554 - loss: 0.6587 - val_accuracy: 0.7127 - val_loss: 0.7430\n",
            "Q: 21-9109 T: -9088 ☒ -9098\n",
            "Q: 1-125 T: -124 ☑ -124\n",
            "Q: 38-8168 T: -8130 ☒ -8129\n",
            "Q: 80-602 T: -522 ☑ -522\n",
            "Q: 678-417 T: 261 ☒ 249\n",
            "Q: 54-8602 T: -8548 ☒ -8564\n",
            "Q: 268-7 T: 261 ☒ 260\n",
            "Q: 33-40 T: -7 ☑ -7\n",
            "Q: 5898-9 T: 5889 ☒ 5880\n",
            "Q: 3953-20 T: 3933 ☒ 3934\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 40\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.7593 - loss: 0.6490 - val_accuracy: 0.7166 - val_loss: 0.7398\n",
            "Q: 48-385 T: -337 ☒ -338\n",
            "Q: 7426-82 T: 7344 ☒ 7364\n",
            "Q: 561-6 T: 555 ☑ 555\n",
            "Q: 812-98 T: 714 ☑ 714\n",
            "Q: 1-7442 T: -7441 ☒ -7440\n",
            "Q: 1871-8218 T: -6347 ☒ -6390\n",
            "Q: 5510-1 T: 5509 ☒ 5516\n",
            "Q: 574-8 T: 566 ☑ 566\n",
            "Q: 611-0 T: 611 ☒ 612\n",
            "Q: 85-178 T: -93 ☒ -90\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 41\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.7643 - loss: 0.6370 - val_accuracy: 0.7264 - val_loss: 0.7227\n",
            "Q: 61-885 T: -824 ☒ -825\n",
            "Q: 218-4341 T: -4123 ☒ -4144\n",
            "Q: 1514-99 T: 1415 ☒ 1451\n",
            "Q: 45-5303 T: -5258 ☒ -5268\n",
            "Q: 367-4576 T: -4209 ☒ -4200\n",
            "Q: 5-658 T: -653 ☑ -653\n",
            "Q: 80-999 T: -919 ☒ -920\n",
            "Q: 1725-42 T: 1683 ☒ 1682\n",
            "Q: 12-3840 T: -3828 ☑ -3828\n",
            "Q: 8862-65 T: 8797 ☒ 8708\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 42\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.7680 - loss: 0.6273 - val_accuracy: 0.7208 - val_loss: 0.7277\n",
            "Q: 917-18 T: 899 ☒ 909\n",
            "Q: 951-5254 T: -4303 ☒ -4390\n",
            "Q: 202-98 T: 104 ☒ 111\n",
            "Q: 659-670 T: -11 ☒ -10\n",
            "Q: 43-7398 T: -7355 ☒ -7346\n",
            "Q: 8371-4 T: 8367 ☒ 8374\n",
            "Q: 918-113 T: 805 ☒ 796\n",
            "Q: 6-9903 T: -9897 ☒ -9896\n",
            "Q: 42-92 T: -50 ☑ -50\n",
            "Q: 629-4 T: 625 ☒ 624\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 43\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.7717 - loss: 0.6150 - val_accuracy: 0.7176 - val_loss: 0.7255\n",
            "Q: 4341-3314 T: 1027 ☒ 100\n",
            "Q: 5941-363 T: 5578 ☒ 5661\n",
            "Q: 7976-293 T: 7683 ☒ 7573\n",
            "Q: 12-5877 T: -5865 ☒ -5867\n",
            "Q: 8388-981 T: 7407 ☒ 7400\n",
            "Q: 8064-5 T: 8059 ☒ 8050\n",
            "Q: 6060-2 T: 6058 ☒ 6056\n",
            "Q: 7-619 T: -612 ☒ -613\n",
            "Q: 145-4515 T: -4370 ☒ -4383\n",
            "Q: 10-16 T: -6 ☑ -6\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 44\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.7748 - loss: 0.6035 - val_accuracy: 0.7300 - val_loss: 0.7026\n",
            "Q: 4041-374 T: 3667 ☒ 3781\n",
            "Q: 293-3072 T: -2779 ☒ -2825\n",
            "Q: 696-8016 T: -7320 ☒ -7325\n",
            "Q: 172-618 T: -446 ☒ -433\n",
            "Q: 5-599 T: -594 ☒ -593\n",
            "Q: 775-774 T: 1 ☒ 18\n",
            "Q: 17-64 T: -47 ☑ -47\n",
            "Q: 5-5503 T: -5498 ☑ -5498\n",
            "Q: 6837-89 T: 6748 ☒ 6756\n",
            "Q: 10-16 T: -6 ☒ -4\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 45\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.7802 - loss: 0.5914 - val_accuracy: 0.7272 - val_loss: 0.7004\n",
            "Q: 4344-0 T: 4344 ☒ 4343\n",
            "Q: 9-485 T: -476 ☑ -476\n",
            "Q: 24-2442 T: -2418 ☒ -2428\n",
            "Q: 7-170 T: -163 ☑ -163\n",
            "Q: 27-919 T: -892 ☒ -900\n",
            "Q: 78-1389 T: -1311 ☒ -1300\n",
            "Q: 8830-737 T: 8093 ☒ 8146\n",
            "Q: 2-4387 T: -4385 ☑ -4385\n",
            "Q: 25-856 T: -831 ☒ -832\n",
            "Q: 8759-3643 T: 5116 ☒ 6020\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 46\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.7834 - loss: 0.5855 - val_accuracy: 0.7359 - val_loss: 0.6874\n",
            "Q: 8082-7 T: 8075 ☒ 8074\n",
            "Q: 9084-792 T: 8292 ☒ 8260\n",
            "Q: 406-9514 T: -9108 ☒ -9211\n",
            "Q: 1334-2772 T: -1438 ☒ -1531\n",
            "Q: 6-9733 T: -9727 ☒ -9728\n",
            "Q: 694-7 T: 687 ☒ 686\n",
            "Q: 4-238 T: -234 ☒ -233\n",
            "Q: 617-65 T: 552 ☒ 555\n",
            "Q: 3349-3229 T: 120 ☒ 109\n",
            "Q: 8-95 T: -87 ☑ -87\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 47\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.7887 - loss: 0.5710 - val_accuracy: 0.7358 - val_loss: 0.6808\n",
            "Q: 918-113 T: 805 ☒ 706\n",
            "Q: 552-8841 T: -8289 ☒ -8311\n",
            "Q: 7377-4 T: 7373 ☒ 7374\n",
            "Q: 788-342 T: 446 ☒ 443\n",
            "Q: 922-17 T: 905 ☒ 807\n",
            "Q: 37-109 T: -72 ☒ -63\n",
            "Q: 31-743 T: -712 ☒ -711\n",
            "Q: 6-699 T: -693 ☒ -692\n",
            "Q: 4457-611 T: 3846 ☒ 3853\n",
            "Q: 4670-1 T: 4669 ☒ 4667\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 48\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.7927 - loss: 0.5584 - val_accuracy: 0.7400 - val_loss: 0.6753\n",
            "Q: 524-213 T: 311 ☒ 323\n",
            "Q: 60-224 T: -164 ☒ -163\n",
            "Q: 9775-3 T: 9772 ☒ 9773\n",
            "Q: 98-136 T: -38 ☒ -37\n",
            "Q: 379-30 T: 349 ☒ 347\n",
            "Q: 85-178 T: -93 ☒ -10\n",
            "Q: 2104-8663 T: -6559 ☒ -6543\n",
            "Q: 661-91 T: 570 ☒ 583\n",
            "Q: 90-216 T: -126 ☒ -134\n",
            "Q: 8759-3643 T: 5116 ☒ 4144\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 49\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.7972 - loss: 0.5478 - val_accuracy: 0.7357 - val_loss: 0.6705\n",
            "Q: 916-4931 T: -4015 ☒ -394\n",
            "Q: 506-6024 T: -5518 ☒ -5544\n",
            "Q: 0-5007 T: -5007 ☒ -5005\n",
            "Q: 2609-877 T: 1732 ☒ 1722\n",
            "Q: 522-1837 T: -1315 ☒ -2333\n",
            "Q: 167-841 T: -674 ☒ -675\n",
            "Q: 740-78 T: 662 ☒ 653\n",
            "Q: 0-104 T: -104 ☑ -104\n",
            "Q: 41-305 T: -264 ☒ -273\n",
            "Q: 2299-147 T: 2152 ☒ 2242\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 50\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.8006 - loss: 0.5382 - val_accuracy: 0.7447 - val_loss: 0.6629\n",
            "Q: 0-832 T: -832 ☑ -832\n",
            "Q: 7213-20 T: 7193 ☑ 7193\n",
            "Q: 1388-3 T: 1385 ☒ 1384\n",
            "Q: 7829-490 T: 7339 ☒ 7292\n",
            "Q: 8169-105 T: 8064 ☒ 8099\n",
            "Q: 4-7409 T: -7405 ☑ -7405\n",
            "Q: 269-8 T: 261 ☒ 260\n",
            "Q: 5175-96 T: 5079 ☒ 5086\n",
            "Q: 3050-2 T: 3048 ☒ 3044\n",
            "Q: 795-11 T: 784 ☒ 774\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtEJK5IZkk8j"
      },
      "source": [
        "1.4)\n",
        "\n",
        "Using any neural network architecture of your liking, build  a model with the aim to beat the best performing model in 1.1 or 1.3. Compare your results in a meaningful way, and add a short explanation to why you think/thought your suggested network is better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwZKyzoBKl4G"
      },
      "outputs": [],
      "source": [
        "config = {}\n",
        "config[\"training_size\"] = 40000\n",
        "config[\"digits\"] = 4\n",
        "config[\"hidden_size\"] = 128\n",
        "config[\"batch_size\"] = 128\n",
        "config[\"iterations\"] = 50\n",
        "chars = '0123456789-+ '"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6YxgNvo0W_o"
      },
      "source": [
        "SOLUTION:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejEBMDx4XXHd",
        "outputId": "2e075644-585b-42da-87c9-2b265815d6ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Generating data...\n",
            "Total arithmetic questions: 40000\n",
            "Starting training...\n",
            "Epoch 1/250:\n",
            "Train Loss: 2.4428, Train Accuracy: 0.0001, Val Loss: 2.4005, Val Accuracy: 0.0000\n",
            "\n",
            "Validation Examples:\n",
            "Question: 4576-3464 | True: 1112 | Predicted: -113 | ✗\n",
            "Question: 3250-4777 | True: -1527 | Predicted: --13 | ✗\n",
            "Question: 9670+3822 | True: 13492 | Predicted: 1111 | ✗\n",
            "Question: 6353-6167 | True: 186 | Predicted: 1-13 | ✗\n",
            "Question: 3854-5930 | True: -2076 | Predicted: -113 | ✗\n",
            "\n",
            "Epoch 2/250:\n",
            "Train Loss: 2.3925, Train Accuracy: 0.0001, Val Loss: 2.3864, Val Accuracy: 0.0003\n",
            "Epoch 3/250:\n",
            "Train Loss: 2.3869, Train Accuracy: 0.0001, Val Loss: 2.3832, Val Accuracy: 0.0003\n",
            "Epoch 4/250:\n",
            "Train Loss: 2.3839, Train Accuracy: 0.0001, Val Loss: 2.3805, Val Accuracy: 0.0003\n",
            "Epoch 5/250:\n",
            "Train Loss: 2.3812, Train Accuracy: 0.0001, Val Loss: 2.3787, Val Accuracy: 0.0003\n",
            "Epoch 6/250:\n",
            "Train Loss: 2.3789, Train Accuracy: 0.0001, Val Loss: 2.3755, Val Accuracy: 0.0000\n",
            "\n",
            "Validation Examples:\n",
            "Question: 9781+6054 | True: 15835 | Predicted: 1730 | ✗\n",
            "Question: 9773+2037 | True: 11810 | Predicted: 1774 | ✗\n",
            "Question: 4368+631 | True: 4999 | Predicted: 5344 | ✗\n",
            "Question: 1588-3161 | True: -1573 | Predicted: -113 | ✗\n",
            "Question: 9755-8441 | True: 1314 | Predicted: 3211 | ✗\n",
            "\n",
            "Epoch 7/250:\n",
            "Train Loss: 2.3760, Train Accuracy: 0.0002, Val Loss: 2.3719, Val Accuracy: 0.0005\n",
            "Epoch 8/250:\n",
            "Train Loss: 2.3729, Train Accuracy: 0.0003, Val Loss: 2.3698, Val Accuracy: 0.0005\n",
            "Epoch 9/250:\n",
            "Train Loss: 2.3694, Train Accuracy: 0.0002, Val Loss: 2.3655, Val Accuracy: 0.0005\n",
            "Epoch 10/250:\n",
            "Train Loss: 2.3646, Train Accuracy: 0.0003, Val Loss: 2.3602, Val Accuracy: 0.0003\n",
            "Epoch 11/250:\n",
            "Train Loss: 2.3594, Train Accuracy: 0.0004, Val Loss: 2.3546, Val Accuracy: 0.0010\n",
            "\n",
            "Validation Examples:\n",
            "Question: 5412-9155 | True: -3743 | Predicted: -433 | ✗\n",
            "Question: 4368+631 | True: 4999 | Predicted: 5445 | ✗\n",
            "Question: 2322-823 | True: 1499 | Predicted: -111 | ✗\n",
            "Question: 731+6313 | True: 7044 | Predicted: 744 | ✗\n",
            "Question: 9598+499 | True: 10097 | Predicted: 1003 | ✗\n",
            "\n",
            "Epoch 12/250:\n",
            "Train Loss: 2.3534, Train Accuracy: 0.0004, Val Loss: 2.3482, Val Accuracy: 0.0018\n",
            "Epoch 13/250:\n",
            "Train Loss: 2.3464, Train Accuracy: 0.0008, Val Loss: 2.3370, Val Accuracy: 0.0015\n",
            "Epoch 14/250:\n",
            "Train Loss: 2.3358, Train Accuracy: 0.0016, Val Loss: 2.3216, Val Accuracy: 0.0040\n",
            "Epoch 15/250:\n",
            "Train Loss: 2.3231, Train Accuracy: 0.0029, Val Loss: 2.3072, Val Accuracy: 0.0030\n",
            "Epoch 16/250:\n",
            "Train Loss: 2.3095, Train Accuracy: 0.0037, Val Loss: 2.2913, Val Accuracy: 0.0067\n",
            "\n",
            "Validation Examples:\n",
            "Question: 7662+9916 | True: 17578 | Predicted: 16581 | ✗\n",
            "Question: 1065+6292 | True: 7357 | Predicted: 8053 | ✗\n",
            "Question: 9926+9154 | True: 19080 | Predicted: 11310 | ✗\n",
            "Question: 7654-8257 | True: -603 | Predicted: -111 | ✗\n",
            "Question: 535-9502 | True: -8967 | Predicted: -35 | ✗\n",
            "\n",
            "Epoch 17/250:\n",
            "Train Loss: 2.2966, Train Accuracy: 0.0064, Val Loss: 2.2755, Val Accuracy: 0.0092\n",
            "Epoch 18/250:\n",
            "Train Loss: 2.2819, Train Accuracy: 0.0093, Val Loss: 2.2569, Val Accuracy: 0.0123\n",
            "Epoch 19/250:\n",
            "Train Loss: 2.2646, Train Accuracy: 0.0126, Val Loss: 2.2309, Val Accuracy: 0.0260\n",
            "Epoch 20/250:\n",
            "Train Loss: 2.2447, Train Accuracy: 0.0184, Val Loss: 2.2007, Val Accuracy: 0.0365\n",
            "Epoch 21/250:\n",
            "Train Loss: 2.2207, Train Accuracy: 0.0267, Val Loss: 2.1683, Val Accuracy: 0.0545\n",
            "\n",
            "Validation Examples:\n",
            "Question: 8276+7719 | True: 15995 | Predicted: 15655 | ✗\n",
            "Question: 4591-8521 | True: -3930 | Predicted: -332 | ✗\n",
            "Question: 1820+8376 | True: 10196 | Predicted: 1096 | ✗\n",
            "Question: 3806+2871 | True: 6677 | Predicted: 5697 | ✗\n",
            "Question: 1305-1007 | True: 298 | Predicted: -302 | ✗\n",
            "\n",
            "Epoch 22/250:\n",
            "Train Loss: 2.1939, Train Accuracy: 0.0375, Val Loss: 2.1422, Val Accuracy: 0.0725\n",
            "Epoch 23/250:\n",
            "Train Loss: 2.1728, Train Accuracy: 0.0482, Val Loss: 2.1246, Val Accuracy: 0.0833\n",
            "Epoch 24/250:\n",
            "Train Loss: 2.1570, Train Accuracy: 0.0589, Val Loss: 2.1162, Val Accuracy: 0.0998\n",
            "Epoch 25/250:\n",
            "Train Loss: 2.1464, Train Accuracy: 0.0661, Val Loss: 2.1074, Val Accuracy: 0.0998\n",
            "Epoch 26/250:\n",
            "Train Loss: 2.1383, Train Accuracy: 0.0734, Val Loss: 2.0995, Val Accuracy: 0.1062\n",
            "\n",
            "Validation Examples:\n",
            "Question: 795-590 | True: 205 | Predicted: -45 | ✗\n",
            "Question: 7256-4636 | True: 2620 | Predicted: 2610 | ✗\n",
            "Question: 5794+3258 | True: 9052 | Predicted: 8042 | ✗\n",
            "Question: 8452-4275 | True: 4177 | Predicted: 4287 | ✗\n",
            "Question: 2534+8695 | True: 11229 | Predicted: 11299 | ✗\n",
            "\n",
            "Epoch 27/250:\n",
            "Train Loss: 2.1295, Train Accuracy: 0.0786, Val Loss: 2.0945, Val Accuracy: 0.1100\n",
            "Epoch 28/250:\n",
            "Train Loss: 2.1238, Train Accuracy: 0.0841, Val Loss: 2.0889, Val Accuracy: 0.1175\n",
            "Epoch 29/250:\n",
            "Train Loss: 2.1165, Train Accuracy: 0.0930, Val Loss: 2.0810, Val Accuracy: 0.1275\n",
            "Epoch 30/250:\n",
            "Train Loss: 2.1109, Train Accuracy: 0.0984, Val Loss: 2.0761, Val Accuracy: 0.1353\n",
            "Epoch 31/250:\n",
            "Train Loss: 2.1057, Train Accuracy: 0.1034, Val Loss: 2.0713, Val Accuracy: 0.1403\n",
            "\n",
            "Validation Examples:\n",
            "Question: 3165+9934 | True: 13099 | Predicted: 13099 | ✓\n",
            "Question: 9513+8096 | True: 17609 | Predicted: 17099 | ✗\n",
            "Question: 5832+6287 | True: 12119 | Predicted: 12199 | ✗\n",
            "Question: 8888+6989 | True: 15877 | Predicted: 16777 | ✗\n",
            "Question: 3506+3678 | True: 7184 | Predicted: 7274 | ✗\n",
            "\n",
            "Epoch 32/250:\n",
            "Train Loss: 2.0985, Train Accuracy: 0.1147, Val Loss: 2.0620, Val Accuracy: 0.1600\n",
            "Epoch 33/250:\n",
            "Train Loss: 2.0920, Train Accuracy: 0.1244, Val Loss: 2.0538, Val Accuracy: 0.1787\n",
            "Epoch 34/250:\n",
            "Train Loss: 2.0856, Train Accuracy: 0.1351, Val Loss: 2.0473, Val Accuracy: 0.1945\n",
            "Epoch 35/250:\n",
            "Train Loss: 2.0792, Train Accuracy: 0.1446, Val Loss: 2.0380, Val Accuracy: 0.2230\n",
            "Epoch 36/250:\n",
            "Train Loss: 2.0724, Train Accuracy: 0.1571, Val Loss: 2.0300, Val Accuracy: 0.2320\n",
            "\n",
            "Validation Examples:\n",
            "Question: 5843+2504 | True: 8347 | Predicted: 8347 | ✓\n",
            "Question: 831+7006 | True: 7837 | Predicted: 7517 | ✗\n",
            "Question: 8786-8985 | True: -199 | Predicted: -111 | ✗\n",
            "Question: 1315+1202 | True: 2517 | Predicted: 2517 | ✓\n",
            "Question: 9136-6772 | True: 2364 | Predicted: 2464 | ✗\n",
            "\n",
            "Epoch 37/250:\n",
            "Train Loss: 2.0655, Train Accuracy: 0.1659, Val Loss: 2.0203, Val Accuracy: 0.2567\n",
            "Epoch 38/250:\n",
            "Train Loss: 2.0587, Train Accuracy: 0.1821, Val Loss: 2.0143, Val Accuracy: 0.2660\n",
            "Epoch 39/250:\n",
            "Train Loss: 2.0517, Train Accuracy: 0.1953, Val Loss: 2.0040, Val Accuracy: 0.2945\n",
            "Epoch 40/250:\n",
            "Train Loss: 2.0446, Train Accuracy: 0.2090, Val Loss: 1.9945, Val Accuracy: 0.3137\n",
            "Epoch 41/250:\n",
            "Train Loss: 2.0354, Train Accuracy: 0.2266, Val Loss: 1.9879, Val Accuracy: 0.3365\n",
            "\n",
            "Validation Examples:\n",
            "Question: 7423-5607 | True: 1816 | Predicted: 1816 | ✓\n",
            "Question: 1972-1330 | True: 642 | Predicted: -642 | ✗\n",
            "Question: 7019-468 | True: 6551 | Predicted: 6551 | ✓\n",
            "Question: 6699+516 | True: 7215 | Predicted: 7215 | ✓\n",
            "Question: 3189-1765 | True: 1424 | Predicted: 1424 | ✓\n",
            "\n",
            "Epoch 42/250:\n",
            "Train Loss: 2.0265, Train Accuracy: 0.2461, Val Loss: 1.9713, Val Accuracy: 0.3772\n",
            "Epoch 43/250:\n",
            "Train Loss: 2.0158, Train Accuracy: 0.2672, Val Loss: 1.9616, Val Accuracy: 0.4030\n",
            "Epoch 44/250:\n",
            "Train Loss: 2.0051, Train Accuracy: 0.2896, Val Loss: 1.9469, Val Accuracy: 0.4235\n",
            "Epoch 45/250:\n",
            "Train Loss: 1.9959, Train Accuracy: 0.3061, Val Loss: 1.9337, Val Accuracy: 0.4612\n",
            "Epoch 46/250:\n",
            "Train Loss: 1.9826, Train Accuracy: 0.3281, Val Loss: 1.9196, Val Accuracy: 0.4963\n",
            "\n",
            "Validation Examples:\n",
            "Question: 9743-6443 | True: 3300 | Predicted: 3300 | ✓\n",
            "Question: 3882-8747 | True: -4865 | Predicted: -405 | ✗\n",
            "Question: 2057-9018 | True: -6961 | Predicted: -713 | ✗\n",
            "Question: 8953-1330 | True: 7623 | Predicted: 7623 | ✓\n",
            "Question: 6324+3916 | True: 10240 | Predicted: 10480 | ✗\n",
            "\n",
            "Epoch 47/250:\n",
            "Train Loss: 1.9723, Train Accuracy: 0.3459, Val Loss: 1.9055, Val Accuracy: 0.5245\n",
            "Epoch 48/250:\n",
            "Train Loss: 1.9607, Train Accuracy: 0.3748, Val Loss: 1.8985, Val Accuracy: 0.5357\n",
            "Epoch 49/250:\n",
            "Train Loss: 1.9500, Train Accuracy: 0.3922, Val Loss: 1.8860, Val Accuracy: 0.5567\n",
            "Epoch 50/250:\n",
            "Train Loss: 1.9344, Train Accuracy: 0.4153, Val Loss: 1.8524, Val Accuracy: 0.6342\n",
            "Epoch 51/250:\n",
            "Train Loss: 1.9139, Train Accuracy: 0.4482, Val Loss: 1.8390, Val Accuracy: 0.6657\n",
            "\n",
            "Validation Examples:\n",
            "Question: 2441+1237 | True: 3678 | Predicted: 3678 | ✓\n",
            "Question: 4647+7207 | True: 11854 | Predicted: 11854 | ✓\n",
            "Question: 8380+4235 | True: 12615 | Predicted: 12615 | ✓\n",
            "Question: 2205+9242 | True: 11447 | Predicted: 11447 | ✓\n",
            "Question: 9235+1353 | True: 10588 | Predicted: 10588 | ✓\n",
            "\n",
            "Epoch 52/250:\n",
            "Train Loss: 1.9016, Train Accuracy: 0.4724, Val Loss: 1.8347, Val Accuracy: 0.6705\n",
            "Epoch 53/250:\n",
            "Train Loss: 1.8914, Train Accuracy: 0.4954, Val Loss: 1.8284, Val Accuracy: 0.6907\n",
            "Epoch 54/250:\n",
            "Train Loss: 1.8825, Train Accuracy: 0.5182, Val Loss: 1.8253, Val Accuracy: 0.6975\n",
            "Epoch 55/250:\n",
            "Train Loss: 1.8766, Train Accuracy: 0.5358, Val Loss: 1.8224, Val Accuracy: 0.7037\n",
            "Epoch 56/250:\n",
            "Train Loss: 1.8714, Train Accuracy: 0.5443, Val Loss: 1.8176, Val Accuracy: 0.7285\n",
            "\n",
            "Validation Examples:\n",
            "Question: 3177+5501 | True: 8678 | Predicted: 8678 | ✓\n",
            "Question: 9361-8041 | True: 1320 | Predicted: 1320 | ✓\n",
            "Question: 6632+3723 | True: 10355 | Predicted: 10355 | ✓\n",
            "Question: 2123-8523 | True: -6400 | Predicted: -6400 | ✓\n",
            "Question: 5015-7480 | True: -2465 | Predicted: -2465 | ✓\n",
            "\n",
            "Epoch 57/250:\n",
            "Train Loss: 1.8670, Train Accuracy: 0.5609, Val Loss: 1.8154, Val Accuracy: 0.7292\n",
            "Epoch 58/250:\n",
            "Train Loss: 1.8622, Train Accuracy: 0.5709, Val Loss: 1.8138, Val Accuracy: 0.7355\n",
            "Epoch 59/250:\n",
            "Train Loss: 1.8570, Train Accuracy: 0.5842, Val Loss: 1.8112, Val Accuracy: 0.7372\n",
            "Epoch 60/250:\n",
            "Train Loss: 1.8539, Train Accuracy: 0.5937, Val Loss: 1.8126, Val Accuracy: 0.7400\n",
            "Epoch 61/250:\n",
            "Train Loss: 1.8516, Train Accuracy: 0.6003, Val Loss: 1.8090, Val Accuracy: 0.7520\n",
            "\n",
            "Validation Examples:\n",
            "Question: 2112+3381 | True: 5493 | Predicted: 5493 | ✓\n",
            "Question: 8828+3584 | True: 12412 | Predicted: 12412 | ✓\n",
            "Question: 9665-1738 | True: 7927 | Predicted: 7927 | ✓\n",
            "Question: 2527-2207 | True: 320 | Predicted: -320 | ✗\n",
            "Question: 468-4045 | True: -3577 | Predicted: -2877 | ✗\n",
            "\n",
            "Epoch 62/250:\n",
            "Train Loss: 1.8481, Train Accuracy: 0.6081, Val Loss: 1.8066, Val Accuracy: 0.7468\n",
            "Epoch 63/250:\n",
            "Train Loss: 1.8469, Train Accuracy: 0.6150, Val Loss: 1.8083, Val Accuracy: 0.7455\n",
            "Epoch 64/250:\n",
            "Train Loss: 1.8442, Train Accuracy: 0.6225, Val Loss: 1.8077, Val Accuracy: 0.7475\n",
            "Epoch 65/250:\n",
            "Train Loss: 1.8426, Train Accuracy: 0.6228, Val Loss: 1.8110, Val Accuracy: 0.7388\n",
            "Epoch 66/250:\n",
            "Train Loss: 1.8396, Train Accuracy: 0.6337, Val Loss: 1.8045, Val Accuracy: 0.7505\n",
            "\n",
            "Validation Examples:\n",
            "Question: 8476-6124 | True: 2352 | Predicted: 2352 | ✓\n",
            "Question: 5386-7287 | True: -1901 | Predicted: -1901 | ✓\n",
            "Question: 5851+6943 | True: 12794 | Predicted: 12794 | ✓\n",
            "Question: 7223+846 | True: 8069 | Predicted: 8069 | ✓\n",
            "Question: 6439+1478 | True: 7917 | Predicted: 7917 | ✓\n",
            "\n",
            "Epoch 67/250:\n",
            "Train Loss: 1.8389, Train Accuracy: 0.6351, Val Loss: 1.8078, Val Accuracy: 0.7488\n",
            "Epoch 68/250:\n",
            "Train Loss: 1.8383, Train Accuracy: 0.6388, Val Loss: 1.8059, Val Accuracy: 0.7512\n",
            "Epoch 69/250:\n",
            "Train Loss: 1.8363, Train Accuracy: 0.6434, Val Loss: 1.8075, Val Accuracy: 0.7495\n",
            "Epoch 70/250:\n",
            "Train Loss: 1.8349, Train Accuracy: 0.6486, Val Loss: 1.8038, Val Accuracy: 0.7538\n",
            "Epoch 71/250:\n",
            "Train Loss: 1.8352, Train Accuracy: 0.6481, Val Loss: 1.8026, Val Accuracy: 0.7615\n",
            "\n",
            "Validation Examples:\n",
            "Question: 3699+2800 | True: 6499 | Predicted: 6499 | ✓\n",
            "Question: 8021-8099 | True: -78 | Predicted: -118 | ✗\n",
            "Question: 3024-3176 | True: -152 | Predicted: -122 | ✗\n",
            "Question: 3474-5435 | True: -1961 | Predicted: -1061 | ✗\n",
            "Question: 9892+6970 | True: 16862 | Predicted: 16862 | ✓\n",
            "\n",
            "Epoch 72/250:\n",
            "Train Loss: 1.8315, Train Accuracy: 0.6611, Val Loss: 1.8034, Val Accuracy: 0.7550\n",
            "Epoch 73/250:\n",
            "Train Loss: 1.8307, Train Accuracy: 0.6610, Val Loss: 1.8044, Val Accuracy: 0.7570\n",
            "Epoch 74/250:\n",
            "Train Loss: 1.8288, Train Accuracy: 0.6644, Val Loss: 1.7985, Val Accuracy: 0.7652\n",
            "Epoch 75/250:\n",
            "Train Loss: 1.8274, Train Accuracy: 0.6687, Val Loss: 1.8032, Val Accuracy: 0.7542\n",
            "Epoch 76/250:\n",
            "Train Loss: 1.8284, Train Accuracy: 0.6677, Val Loss: 1.7970, Val Accuracy: 0.7730\n",
            "\n",
            "Validation Examples:\n",
            "Question: 9597-6593 | True: 3004 | Predicted: 2004 | ✗\n",
            "Question: 2512+6751 | True: 9263 | Predicted: 9263 | ✓\n",
            "Question: 1414+2856 | True: 4270 | Predicted: 4270 | ✓\n",
            "Question: 4592-8376 | True: -3784 | Predicted: -3784 | ✓\n",
            "Question: 3528-5605 | True: -2077 | Predicted: -2087 | ✗\n",
            "\n",
            "Epoch 77/250:\n",
            "Train Loss: 1.8271, Train Accuracy: 0.6734, Val Loss: 1.7976, Val Accuracy: 0.7742\n",
            "Epoch 78/250:\n",
            "Train Loss: 1.8253, Train Accuracy: 0.6758, Val Loss: 1.8049, Val Accuracy: 0.7568\n",
            "Epoch 79/250:\n",
            "Train Loss: 1.8257, Train Accuracy: 0.6777, Val Loss: 1.7981, Val Accuracy: 0.7668\n",
            "Epoch 80/250:\n",
            "Train Loss: 1.8232, Train Accuracy: 0.6815, Val Loss: 1.8012, Val Accuracy: 0.7592\n",
            "Epoch 81/250:\n",
            "Train Loss: 1.8211, Train Accuracy: 0.6861, Val Loss: 1.7966, Val Accuracy: 0.7728\n",
            "\n",
            "Validation Examples:\n",
            "Question: 9770+5738 | True: 15508 | Predicted: 15508 | ✓\n",
            "Question: 8688-1442 | True: 7246 | Predicted: 7246 | ✓\n",
            "Question: 102-1002 | True: -900 | Predicted: -8000 | ✗\n",
            "Question: 6551-4981 | True: 1570 | Predicted: 1570 | ✓\n",
            "Question: 9862-8659 | True: 1203 | Predicted: 1203 | ✓\n",
            "\n",
            "Epoch 82/250:\n",
            "Train Loss: 1.8217, Train Accuracy: 0.6860, Val Loss: 1.7974, Val Accuracy: 0.7742\n",
            "Epoch 83/250:\n",
            "Train Loss: 1.8203, Train Accuracy: 0.6869, Val Loss: 1.7964, Val Accuracy: 0.7695\n",
            "Epoch 84/250:\n",
            "Train Loss: 1.8195, Train Accuracy: 0.6897, Val Loss: 1.7955, Val Accuracy: 0.7758\n",
            "Epoch 85/250:\n",
            "Train Loss: 1.8189, Train Accuracy: 0.6894, Val Loss: 1.7928, Val Accuracy: 0.7760\n",
            "Epoch 86/250:\n",
            "Train Loss: 1.8179, Train Accuracy: 0.6933, Val Loss: 1.7917, Val Accuracy: 0.7802\n",
            "\n",
            "Validation Examples:\n",
            "Question: 4757-7396 | True: -2639 | Predicted: -2639 | ✓\n",
            "Question: 9120-9010 | True: 110 | Predicted: 2988 | ✗\n",
            "Question: 3914-3206 | True: 708 | Predicted: 2708 | ✗\n",
            "Question: 7564+4824 | True: 12388 | Predicted: 12388 | ✓\n",
            "Question: 7956-7794 | True: 162 | Predicted: -162 | ✗\n",
            "\n",
            "Epoch 87/250:\n",
            "Train Loss: 1.8158, Train Accuracy: 0.6987, Val Loss: 1.7944, Val Accuracy: 0.7725\n",
            "Epoch 88/250:\n",
            "Train Loss: 1.8151, Train Accuracy: 0.6983, Val Loss: 1.7912, Val Accuracy: 0.7850\n",
            "Epoch 89/250:\n",
            "Train Loss: 1.8153, Train Accuracy: 0.6960, Val Loss: 1.7932, Val Accuracy: 0.7778\n",
            "Epoch 90/250:\n",
            "Train Loss: 1.8133, Train Accuracy: 0.7021, Val Loss: 1.7900, Val Accuracy: 0.7785\n",
            "Epoch 91/250:\n",
            "Train Loss: 1.8145, Train Accuracy: 0.6982, Val Loss: 1.7918, Val Accuracy: 0.7800\n",
            "\n",
            "Validation Examples:\n",
            "Question: 1412-1150 | True: 262 | Predicted: 2232 | ✗\n",
            "Question: 2407-3824 | True: -1417 | Predicted: -1417 | ✓\n",
            "Question: 3714-7975 | True: -4261 | Predicted: -4261 | ✓\n",
            "Question: 5789+678 | True: 6467 | Predicted: 6467 | ✓\n",
            "Question: 3326-1068 | True: 2258 | Predicted: 2258 | ✓\n",
            "\n",
            "Epoch 92/250:\n",
            "Train Loss: 1.8128, Train Accuracy: 0.7025, Val Loss: 1.7918, Val Accuracy: 0.7808\n",
            "Epoch 93/250:\n",
            "Train Loss: 1.8121, Train Accuracy: 0.7036, Val Loss: 1.7869, Val Accuracy: 0.7877\n",
            "Epoch 94/250:\n",
            "Train Loss: 1.8107, Train Accuracy: 0.7091, Val Loss: 1.7893, Val Accuracy: 0.7875\n",
            "Epoch 95/250:\n",
            "Train Loss: 1.8099, Train Accuracy: 0.7097, Val Loss: 1.7877, Val Accuracy: 0.7845\n",
            "Epoch 96/250:\n",
            "Train Loss: 1.8104, Train Accuracy: 0.7079, Val Loss: 1.7898, Val Accuracy: 0.7860\n",
            "\n",
            "Validation Examples:\n",
            "Question: 3522-1400 | True: 2122 | Predicted: 2122 | ✓\n",
            "Question: 39-6487 | True: -6448 | Predicted: -4437 | ✗\n",
            "Question: 6034-8796 | True: -2762 | Predicted: -2762 | ✓\n",
            "Question: 6758-8447 | True: -1689 | Predicted: -1689 | ✓\n",
            "Question: 8455+1023 | True: 9478 | Predicted: 9478 | ✓\n",
            "\n",
            "Epoch 97/250:\n",
            "Train Loss: 1.8095, Train Accuracy: 0.7100, Val Loss: 1.7892, Val Accuracy: 0.7827\n",
            "Epoch 98/250:\n",
            "Train Loss: 1.8094, Train Accuracy: 0.7117, Val Loss: 1.7906, Val Accuracy: 0.7835\n",
            "Epoch 99/250:\n",
            "Train Loss: 1.8079, Train Accuracy: 0.7119, Val Loss: 1.7872, Val Accuracy: 0.7917\n",
            "Epoch 100/250:\n",
            "Train Loss: 1.8070, Train Accuracy: 0.7202, Val Loss: 1.7837, Val Accuracy: 0.7957\n",
            "Epoch 101/250:\n",
            "Train Loss: 1.8050, Train Accuracy: 0.7202, Val Loss: 1.7860, Val Accuracy: 0.7915\n",
            "\n",
            "Validation Examples:\n",
            "Question: 6808+1114 | True: 7922 | Predicted: 7922 | ✓\n",
            "Question: 1111-9698 | True: -8587 | Predicted: -8587 | ✓\n",
            "Question: 4560-5119 | True: -559 | Predicted: -453 | ✗\n",
            "Question: 180-5103 | True: -4923 | Predicted: -4233 | ✗\n",
            "Question: 8363-7320 | True: 1043 | Predicted: 1043 | ✓\n",
            "\n",
            "Epoch 102/250:\n",
            "Train Loss: 1.8048, Train Accuracy: 0.7223, Val Loss: 1.7841, Val Accuracy: 0.7913\n",
            "Epoch 103/250:\n",
            "Train Loss: 1.8034, Train Accuracy: 0.7215, Val Loss: 1.7828, Val Accuracy: 0.7955\n",
            "Epoch 104/250:\n",
            "Train Loss: 1.8034, Train Accuracy: 0.7203, Val Loss: 1.7822, Val Accuracy: 0.7917\n",
            "Epoch 105/250:\n",
            "Train Loss: 1.8013, Train Accuracy: 0.7294, Val Loss: 1.7802, Val Accuracy: 0.8000\n",
            "Epoch 106/250:\n",
            "Train Loss: 1.8016, Train Accuracy: 0.7268, Val Loss: 1.7801, Val Accuracy: 0.8013\n",
            "\n",
            "Validation Examples:\n",
            "Question: 4354+2132 | True: 6486 | Predicted: 6486 | ✓\n",
            "Question: 9174+1205 | True: 10379 | Predicted: 10379 | ✓\n",
            "Question: 6625-5878 | True: 747 | Predicted: 8747 | ✗\n",
            "Question: 8959-3202 | True: 5757 | Predicted: 5757 | ✓\n",
            "Question: 617-845 | True: -228 | Predicted: -828 | ✗\n",
            "\n",
            "Epoch 107/250:\n",
            "Train Loss: 1.8019, Train Accuracy: 0.7239, Val Loss: 1.7798, Val Accuracy: 0.7995\n",
            "Epoch 108/250:\n",
            "Train Loss: 1.7992, Train Accuracy: 0.7328, Val Loss: 1.7784, Val Accuracy: 0.8057\n",
            "Epoch 109/250:\n",
            "Train Loss: 1.7996, Train Accuracy: 0.7286, Val Loss: 1.7789, Val Accuracy: 0.8010\n",
            "Epoch 110/250:\n",
            "Train Loss: 1.7996, Train Accuracy: 0.7319, Val Loss: 1.7796, Val Accuracy: 0.8040\n",
            "Epoch 111/250:\n",
            "Train Loss: 1.7987, Train Accuracy: 0.7302, Val Loss: 1.7783, Val Accuracy: 0.8005\n",
            "\n",
            "Validation Examples:\n",
            "Question: 3064-77 | True: 2987 | Predicted: 2987 | ✓\n",
            "Question: 8131+7320 | True: 15451 | Predicted: 15451 | ✓\n",
            "Question: 6518+1556 | True: 8074 | Predicted: 8074 | ✓\n",
            "Question: 1416-4596 | True: -3180 | Predicted: -3180 | ✓\n",
            "Question: 6518+642 | True: 7160 | Predicted: 7160 | ✓\n",
            "\n",
            "Epoch 112/250:\n",
            "Train Loss: 1.7975, Train Accuracy: 0.7326, Val Loss: 1.7802, Val Accuracy: 0.8047\n",
            "Epoch 113/250:\n",
            "Train Loss: 1.7959, Train Accuracy: 0.7384, Val Loss: 1.7757, Val Accuracy: 0.8075\n",
            "Epoch 114/250:\n",
            "Train Loss: 1.7960, Train Accuracy: 0.7389, Val Loss: 1.7774, Val Accuracy: 0.8067\n",
            "Epoch 115/250:\n",
            "Train Loss: 1.7943, Train Accuracy: 0.7412, Val Loss: 1.7763, Val Accuracy: 0.8197\n",
            "Epoch 116/250:\n",
            "Train Loss: 1.7945, Train Accuracy: 0.7425, Val Loss: 1.7734, Val Accuracy: 0.8193\n",
            "\n",
            "Validation Examples:\n",
            "Question: 6240+5980 | True: 12220 | Predicted: 12220 | ✓\n",
            "Question: 3116-2493 | True: 623 | Predicted: 8623 | ✗\n",
            "Question: 1658-2350 | True: -692 | Predicted: -699 | ✗\n",
            "Question: 1987-7899 | True: -5912 | Predicted: -5912 | ✓\n",
            "Question: 8117-620 | True: 7497 | Predicted: 7497 | ✓\n",
            "\n",
            "Epoch 117/250:\n",
            "Train Loss: 1.7918, Train Accuracy: 0.7505, Val Loss: 1.7726, Val Accuracy: 0.8243\n",
            "Epoch 118/250:\n",
            "Train Loss: 1.7922, Train Accuracy: 0.7498, Val Loss: 1.7737, Val Accuracy: 0.8180\n",
            "Epoch 119/250:\n",
            "Train Loss: 1.7918, Train Accuracy: 0.7475, Val Loss: 1.7766, Val Accuracy: 0.8153\n",
            "Epoch 120/250:\n",
            "Train Loss: 1.7917, Train Accuracy: 0.7493, Val Loss: 1.7710, Val Accuracy: 0.8220\n",
            "Epoch 121/250:\n",
            "Train Loss: 1.7921, Train Accuracy: 0.7455, Val Loss: 1.7740, Val Accuracy: 0.8165\n",
            "\n",
            "Validation Examples:\n",
            "Question: 8482+8270 | True: 16752 | Predicted: 16752 | ✓\n",
            "Question: 377-2588 | True: -2211 | Predicted: -2811 | ✗\n",
            "Question: 2458+4193 | True: 6651 | Predicted: 6651 | ✓\n",
            "Question: 7726-8708 | True: -982 | Predicted: -0982 | ✗\n",
            "Question: 6387-9672 | True: -3285 | Predicted: -3285 | ✓\n",
            "\n",
            "Epoch 122/250:\n",
            "Train Loss: 1.7918, Train Accuracy: 0.7487, Val Loss: 1.7706, Val Accuracy: 0.8270\n",
            "Epoch 123/250:\n",
            "Train Loss: 1.7904, Train Accuracy: 0.7512, Val Loss: 1.7691, Val Accuracy: 0.8263\n",
            "Epoch 124/250:\n",
            "Train Loss: 1.7890, Train Accuracy: 0.7552, Val Loss: 1.7717, Val Accuracy: 0.8263\n",
            "Epoch 125/250:\n",
            "Train Loss: 1.7882, Train Accuracy: 0.7595, Val Loss: 1.7693, Val Accuracy: 0.8325\n",
            "Epoch 126/250:\n",
            "Train Loss: 1.7876, Train Accuracy: 0.7585, Val Loss: 1.7701, Val Accuracy: 0.8225\n",
            "\n",
            "Validation Examples:\n",
            "Question: 9109+8187 | True: 17296 | Predicted: 17296 | ✓\n",
            "Question: 7757+6935 | True: 14692 | Predicted: 14692 | ✓\n",
            "Question: 6163+2951 | True: 9114 | Predicted: 9114 | ✓\n",
            "Question: 8094+1985 | True: 10079 | Predicted: 1079 | ✗\n",
            "Question: 4648+4889 | True: 9537 | Predicted: 9537 | ✓\n",
            "\n",
            "Epoch 127/250:\n",
            "Train Loss: 1.7877, Train Accuracy: 0.7595, Val Loss: 1.7692, Val Accuracy: 0.8315\n",
            "Epoch 128/250:\n",
            "Train Loss: 1.7867, Train Accuracy: 0.7615, Val Loss: 1.7740, Val Accuracy: 0.8163\n",
            "Epoch 129/250:\n",
            "Train Loss: 1.7867, Train Accuracy: 0.7607, Val Loss: 1.7705, Val Accuracy: 0.8240\n",
            "Epoch 130/250:\n",
            "Train Loss: 1.7872, Train Accuracy: 0.7563, Val Loss: 1.7691, Val Accuracy: 0.8303\n",
            "Epoch 131/250:\n",
            "Train Loss: 1.7869, Train Accuracy: 0.7604, Val Loss: 1.7684, Val Accuracy: 0.8297\n",
            "\n",
            "Validation Examples:\n",
            "Question: 1885-4702 | True: -2817 | Predicted: -2817 | ✓\n",
            "Question: 8253+7305 | True: 15558 | Predicted: 15558 | ✓\n",
            "Question: 895-2592 | True: -1697 | Predicted: -1377 | ✗\n",
            "Question: 7016+9520 | True: 16536 | Predicted: 16536 | ✓\n",
            "Question: 7243-7060 | True: 183 | Predicted: 1727 | ✗\n",
            "\n",
            "Epoch 132/250:\n",
            "Train Loss: 1.7869, Train Accuracy: 0.7584, Val Loss: 1.7684, Val Accuracy: 0.8303\n",
            "Epoch 133/250:\n",
            "Train Loss: 1.7859, Train Accuracy: 0.7644, Val Loss: 1.7674, Val Accuracy: 0.8305\n",
            "Epoch 134/250:\n",
            "Train Loss: 1.7852, Train Accuracy: 0.7622, Val Loss: 1.7673, Val Accuracy: 0.8330\n",
            "Epoch 135/250:\n",
            "Train Loss: 1.7832, Train Accuracy: 0.7679, Val Loss: 1.7656, Val Accuracy: 0.8347\n",
            "Epoch 136/250:\n",
            "Train Loss: 1.7837, Train Accuracy: 0.7652, Val Loss: 1.7687, Val Accuracy: 0.8310\n",
            "\n",
            "Validation Examples:\n",
            "Question: 1949+6985 | True: 8934 | Predicted: 8934 | ✓\n",
            "Question: 1984-2056 | True: -72 | Predicted: -077 | ✗\n",
            "Question: 3403-6269 | True: -2866 | Predicted: -2866 | ✓\n",
            "Question: 3921+1200 | True: 5121 | Predicted: 5121 | ✓\n",
            "Question: 9118-8411 | True: 707 | Predicted: 8707 | ✗\n",
            "\n",
            "Epoch 137/250:\n",
            "Train Loss: 1.7834, Train Accuracy: 0.7674, Val Loss: 1.7681, Val Accuracy: 0.8317\n",
            "Epoch 138/250:\n",
            "Train Loss: 1.7827, Train Accuracy: 0.7690, Val Loss: 1.7707, Val Accuracy: 0.8285\n",
            "Epoch 139/250:\n",
            "Train Loss: 1.7831, Train Accuracy: 0.7706, Val Loss: 1.7649, Val Accuracy: 0.8345\n",
            "Epoch 140/250:\n",
            "Train Loss: 1.7820, Train Accuracy: 0.7713, Val Loss: 1.7678, Val Accuracy: 0.8253\n",
            "Epoch 141/250:\n",
            "Train Loss: 1.7838, Train Accuracy: 0.7652, Val Loss: 1.7718, Val Accuracy: 0.8200\n",
            "\n",
            "Validation Examples:\n",
            "Question: 7605-6572 | True: 1033 | Predicted: 1033 | ✓\n",
            "Question: 1922+1840 | True: 3762 | Predicted: 3762 | ✓\n",
            "Question: 5686-1221 | True: 4465 | Predicted: 4465 | ✓\n",
            "Question: 5443+9179 | True: 14622 | Predicted: 14622 | ✓\n",
            "Question: 806+7894 | True: 8700 | Predicted: 8700 | ✓\n",
            "\n",
            "Epoch 142/250:\n",
            "Train Loss: 1.7842, Train Accuracy: 0.7658, Val Loss: 1.7663, Val Accuracy: 0.8355\n",
            "Epoch 143/250:\n",
            "Train Loss: 1.7813, Train Accuracy: 0.7724, Val Loss: 1.7639, Val Accuracy: 0.8347\n",
            "Epoch 144/250:\n",
            "Train Loss: 1.7812, Train Accuracy: 0.7724, Val Loss: 1.7665, Val Accuracy: 0.8335\n",
            "Epoch 145/250:\n",
            "Train Loss: 1.7804, Train Accuracy: 0.7740, Val Loss: 1.7658, Val Accuracy: 0.8305\n",
            "Epoch 146/250:\n",
            "Train Loss: 1.7797, Train Accuracy: 0.7753, Val Loss: 1.7640, Val Accuracy: 0.8323\n",
            "\n",
            "Validation Examples:\n",
            "Question: 3653-4168 | True: -515 | Predicted: -515 | ✓\n",
            "Question: 4998+7793 | True: 12791 | Predicted: 12791 | ✓\n",
            "Question: 6333-7326 | True: -993 | Predicted: -193 | ✗\n",
            "Question: 8027-6731 | True: 1296 | Predicted: 1296 | ✓\n",
            "Question: 7408-7541 | True: -133 | Predicted: -143 | ✗\n",
            "\n",
            "Epoch 147/250:\n",
            "Train Loss: 1.7781, Train Accuracy: 0.7787, Val Loss: 1.7654, Val Accuracy: 0.8317\n",
            "Epoch 148/250:\n",
            "Train Loss: 1.7765, Train Accuracy: 0.7817, Val Loss: 1.7631, Val Accuracy: 0.8345\n",
            "Epoch 149/250:\n",
            "Train Loss: 1.7773, Train Accuracy: 0.7770, Val Loss: 1.7620, Val Accuracy: 0.8333\n",
            "Epoch 150/250:\n",
            "Train Loss: 1.7781, Train Accuracy: 0.7742, Val Loss: 1.7647, Val Accuracy: 0.8297\n",
            "Epoch 151/250:\n",
            "Train Loss: 1.7778, Train Accuracy: 0.7751, Val Loss: 1.7623, Val Accuracy: 0.8350\n",
            "\n",
            "Validation Examples:\n",
            "Question: 5002+2275 | True: 7277 | Predicted: 7277 | ✓\n",
            "Question: 7012+9313 | True: 16325 | Predicted: 16325 | ✓\n",
            "Question: 7440+6452 | True: 13892 | Predicted: 13892 | ✓\n",
            "Question: 8449-7304 | True: 1145 | Predicted: 1145 | ✓\n",
            "Question: 2018-3544 | True: -1526 | Predicted: -1526 | ✓\n",
            "\n",
            "Epoch 152/250:\n",
            "Train Loss: 1.7760, Train Accuracy: 0.7783, Val Loss: 1.7608, Val Accuracy: 0.8405\n",
            "Epoch 153/250:\n",
            "Train Loss: 1.7755, Train Accuracy: 0.7823, Val Loss: 1.7602, Val Accuracy: 0.8390\n",
            "Epoch 154/250:\n",
            "Train Loss: 1.7763, Train Accuracy: 0.7774, Val Loss: 1.7625, Val Accuracy: 0.8343\n",
            "Epoch 155/250:\n",
            "Train Loss: 1.7756, Train Accuracy: 0.7817, Val Loss: 1.7622, Val Accuracy: 0.8367\n",
            "Epoch 156/250:\n",
            "Train Loss: 1.7756, Train Accuracy: 0.7819, Val Loss: 1.7620, Val Accuracy: 0.8390\n",
            "\n",
            "Validation Examples:\n",
            "Question: 5487+4047 | True: 9534 | Predicted: 9534 | ✓\n",
            "Question: 7976+1545 | True: 9521 | Predicted: 9521 | ✓\n",
            "Question: 2306+2612 | True: 4918 | Predicted: 4918 | ✓\n",
            "Question: 390-7305 | True: -6915 | Predicted: -6115 | ✗\n",
            "Question: 9145+1800 | True: 10945 | Predicted: 10945 | ✓\n",
            "\n",
            "Epoch 157/250:\n",
            "Train Loss: 1.7740, Train Accuracy: 0.7842, Val Loss: 1.7621, Val Accuracy: 0.8335\n",
            "Epoch 158/250:\n",
            "Train Loss: 1.7747, Train Accuracy: 0.7830, Val Loss: 1.7594, Val Accuracy: 0.8405\n",
            "Epoch 159/250:\n",
            "Train Loss: 1.7751, Train Accuracy: 0.7796, Val Loss: 1.7627, Val Accuracy: 0.8290\n",
            "Epoch 160/250:\n",
            "Train Loss: 1.7743, Train Accuracy: 0.7825, Val Loss: 1.7594, Val Accuracy: 0.8387\n",
            "Epoch 161/250:\n",
            "Train Loss: 1.7736, Train Accuracy: 0.7833, Val Loss: 1.7608, Val Accuracy: 0.8355\n",
            "\n",
            "Validation Examples:\n",
            "Question: 8103+6493 | True: 14596 | Predicted: 14596 | ✓\n",
            "Question: 9585+3149 | True: 12734 | Predicted: 12734 | ✓\n",
            "Question: 3811-6103 | True: -2292 | Predicted: -2292 | ✓\n",
            "Question: 4576-4146 | True: 430 | Predicted: 2560 | ✗\n",
            "Question: 6200-3362 | True: 2838 | Predicted: 2838 | ✓\n",
            "\n",
            "Epoch 162/250:\n",
            "Train Loss: 1.7731, Train Accuracy: 0.7879, Val Loss: 1.7611, Val Accuracy: 0.8403\n",
            "Epoch 163/250:\n",
            "Train Loss: 1.7759, Train Accuracy: 0.7809, Val Loss: 1.7600, Val Accuracy: 0.8385\n",
            "Epoch 164/250:\n",
            "Train Loss: 1.7727, Train Accuracy: 0.7866, Val Loss: 1.7600, Val Accuracy: 0.8407\n",
            "Epoch 165/250:\n",
            "Train Loss: 1.7722, Train Accuracy: 0.7887, Val Loss: 1.7592, Val Accuracy: 0.8393\n",
            "Epoch 166/250:\n",
            "Train Loss: 1.7712, Train Accuracy: 0.7901, Val Loss: 1.7609, Val Accuracy: 0.8333\n",
            "\n",
            "Validation Examples:\n",
            "Question: 2777-4695 | True: -1918 | Predicted: -1918 | ✓\n",
            "Question: 8455+1023 | True: 9478 | Predicted: 9478 | ✓\n",
            "Question: 7105-5738 | True: 1367 | Predicted: 1367 | ✓\n",
            "Question: 6456+4340 | True: 10796 | Predicted: 10796 | ✓\n",
            "Question: 2654+8375 | True: 11029 | Predicted: 11029 | ✓\n",
            "\n",
            "Epoch 167/250:\n",
            "Train Loss: 1.7709, Train Accuracy: 0.7882, Val Loss: 1.7574, Val Accuracy: 0.8413\n",
            "Epoch 168/250:\n",
            "Train Loss: 1.7705, Train Accuracy: 0.7914, Val Loss: 1.7592, Val Accuracy: 0.8365\n",
            "Epoch 169/250:\n",
            "Train Loss: 1.7698, Train Accuracy: 0.7923, Val Loss: 1.7595, Val Accuracy: 0.8345\n",
            "Epoch 170/250:\n",
            "Train Loss: 1.7706, Train Accuracy: 0.7874, Val Loss: 1.7584, Val Accuracy: 0.8360\n",
            "Epoch 171/250:\n",
            "Train Loss: 1.7708, Train Accuracy: 0.7857, Val Loss: 1.7570, Val Accuracy: 0.8410\n",
            "\n",
            "Validation Examples:\n",
            "Question: 7594+452 | True: 8046 | Predicted: 7046 | ✗\n",
            "Question: 2504-2169 | True: 335 | Predicted: 1365 | ✗\n",
            "Question: 6361-7259 | True: -898 | Predicted: -894 | ✗\n",
            "Question: 4162+1654 | True: 5816 | Predicted: 5816 | ✓\n",
            "Question: 269+9484 | True: 9753 | Predicted: 9753 | ✓\n",
            "\n",
            "Epoch 172/250:\n",
            "Train Loss: 1.7704, Train Accuracy: 0.7894, Val Loss: 1.7558, Val Accuracy: 0.8468\n",
            "Epoch 173/250:\n",
            "Train Loss: 1.7701, Train Accuracy: 0.7917, Val Loss: 1.7573, Val Accuracy: 0.8420\n",
            "Epoch 174/250:\n",
            "Train Loss: 1.7693, Train Accuracy: 0.7921, Val Loss: 1.7566, Val Accuracy: 0.8440\n",
            "Epoch 175/250:\n",
            "Train Loss: 1.7687, Train Accuracy: 0.7935, Val Loss: 1.7580, Val Accuracy: 0.8397\n",
            "Epoch 176/250:\n",
            "Train Loss: 1.7693, Train Accuracy: 0.7912, Val Loss: 1.7588, Val Accuracy: 0.8350\n",
            "\n",
            "Validation Examples:\n",
            "Question: 1957-2716 | True: -759 | Predicted: -759 | ✓\n",
            "Question: 2572-9361 | True: -6789 | Predicted: -6789 | ✓\n",
            "Question: 2337-5182 | True: -2845 | Predicted: -2845 | ✓\n",
            "Question: 8491-8539 | True: -48 | Predicted: -048 | ✗\n",
            "Question: 8548-8329 | True: 219 | Predicted: 1789 | ✗\n",
            "\n",
            "Epoch 177/250:\n",
            "Train Loss: 1.7687, Train Accuracy: 0.7934, Val Loss: 1.7556, Val Accuracy: 0.8455\n",
            "Epoch 178/250:\n",
            "Train Loss: 1.7684, Train Accuracy: 0.7940, Val Loss: 1.7572, Val Accuracy: 0.8417\n",
            "Epoch 179/250:\n",
            "Train Loss: 1.7683, Train Accuracy: 0.7939, Val Loss: 1.7558, Val Accuracy: 0.8427\n",
            "Epoch 180/250:\n",
            "Train Loss: 1.7687, Train Accuracy: 0.7942, Val Loss: 1.7584, Val Accuracy: 0.8350\n",
            "Epoch 181/250:\n",
            "Train Loss: 1.7682, Train Accuracy: 0.7959, Val Loss: 1.7588, Val Accuracy: 0.8375\n",
            "\n",
            "Validation Examples:\n",
            "Question: 282+4661 | True: 4943 | Predicted: 4943 | ✓\n",
            "Question: 2014-736 | True: 1278 | Predicted: 1278 | ✓\n",
            "Question: 298-9152 | True: -8854 | Predicted: -8554 | ✗\n",
            "Question: 5176-2310 | True: 2866 | Predicted: 2866 | ✓\n",
            "Question: 6954-5646 | True: 1308 | Predicted: 1308 | ✓\n",
            "\n",
            "Epoch 182/250:\n",
            "Train Loss: 1.7671, Train Accuracy: 0.7963, Val Loss: 1.7543, Val Accuracy: 0.8482\n",
            "Epoch 183/250:\n",
            "Train Loss: 1.7683, Train Accuracy: 0.7961, Val Loss: 1.7597, Val Accuracy: 0.8403\n",
            "Epoch 184/250:\n",
            "Train Loss: 1.7669, Train Accuracy: 0.7989, Val Loss: 1.7533, Val Accuracy: 0.8502\n",
            "Epoch 185/250:\n",
            "Train Loss: 1.7662, Train Accuracy: 0.7992, Val Loss: 1.7567, Val Accuracy: 0.8387\n",
            "Epoch 186/250:\n",
            "Train Loss: 1.7659, Train Accuracy: 0.8026, Val Loss: 1.7538, Val Accuracy: 0.8485\n",
            "\n",
            "Validation Examples:\n",
            "Question: 3218+1332 | True: 4550 | Predicted: 4550 | ✓\n",
            "Question: 8477+7694 | True: 16171 | Predicted: 16171 | ✓\n",
            "Question: 9890+4036 | True: 13926 | Predicted: 13926 | ✓\n",
            "Question: 4513-6837 | True: -2324 | Predicted: -2324 | ✓\n",
            "Question: 367-8504 | True: -8137 | Predicted: -8337 | ✗\n",
            "\n",
            "Epoch 187/250:\n",
            "Train Loss: 1.7649, Train Accuracy: 0.8036, Val Loss: 1.7555, Val Accuracy: 0.8468\n",
            "Epoch 188/250:\n",
            "Train Loss: 1.7654, Train Accuracy: 0.8011, Val Loss: 1.7571, Val Accuracy: 0.8395\n",
            "Epoch 189/250:\n",
            "Train Loss: 1.7657, Train Accuracy: 0.8004, Val Loss: 1.7545, Val Accuracy: 0.8438\n",
            "Epoch 190/250:\n",
            "Train Loss: 1.7640, Train Accuracy: 0.8033, Val Loss: 1.7531, Val Accuracy: 0.8528\n",
            "Epoch 191/250:\n",
            "Train Loss: 1.7634, Train Accuracy: 0.8063, Val Loss: 1.7524, Val Accuracy: 0.8538\n",
            "\n",
            "Validation Examples:\n",
            "Question: 7605-6572 | True: 1033 | Predicted: 1033 | ✓\n",
            "Question: 711+8986 | True: 9697 | Predicted: 9697 | ✓\n",
            "Question: 2340+8550 | True: 10890 | Predicted: 10890 | ✓\n",
            "Question: 8306+6900 | True: 15206 | Predicted: 15206 | ✓\n",
            "Question: 2436-8478 | True: -6042 | Predicted: -6042 | ✓\n",
            "\n",
            "Epoch 192/250:\n",
            "Train Loss: 1.7658, Train Accuracy: 0.7997, Val Loss: 1.7543, Val Accuracy: 0.8468\n",
            "Epoch 193/250:\n",
            "Train Loss: 1.7638, Train Accuracy: 0.8069, Val Loss: 1.7534, Val Accuracy: 0.8485\n",
            "Epoch 194/250:\n",
            "Train Loss: 1.7636, Train Accuracy: 0.8050, Val Loss: 1.7527, Val Accuracy: 0.8542\n",
            "Epoch 195/250:\n",
            "Train Loss: 1.7632, Train Accuracy: 0.8061, Val Loss: 1.7526, Val Accuracy: 0.8525\n",
            "Epoch 196/250:\n",
            "Train Loss: 1.7618, Train Accuracy: 0.8110, Val Loss: 1.7519, Val Accuracy: 0.8578\n",
            "\n",
            "Validation Examples:\n",
            "Question: 4504-555 | True: 3949 | Predicted: 3949 | ✓\n",
            "Question: 959-9611 | True: -8652 | Predicted: -8552 | ✗\n",
            "Question: 6682+7948 | True: 14630 | Predicted: 14630 | ✓\n",
            "Question: 3049-9647 | True: -6598 | Predicted: -6698 | ✗\n",
            "Question: 4446-3053 | True: 1393 | Predicted: 1393 | ✓\n",
            "\n",
            "Epoch 197/250:\n",
            "Train Loss: 1.7642, Train Accuracy: 0.8026, Val Loss: 1.7519, Val Accuracy: 0.8555\n",
            "Epoch 198/250:\n",
            "Train Loss: 1.7650, Train Accuracy: 0.8030, Val Loss: 1.7560, Val Accuracy: 0.8450\n",
            "Epoch 199/250:\n",
            "Train Loss: 1.7630, Train Accuracy: 0.8070, Val Loss: 1.7531, Val Accuracy: 0.8515\n",
            "Epoch 200/250:\n",
            "Train Loss: 1.7614, Train Accuracy: 0.8113, Val Loss: 1.7511, Val Accuracy: 0.8552\n",
            "Epoch 201/250:\n",
            "Train Loss: 1.7630, Train Accuracy: 0.8079, Val Loss: 1.7523, Val Accuracy: 0.8515\n",
            "\n",
            "Validation Examples:\n",
            "Question: 3666+9401 | True: 13067 | Predicted: 13067 | ✓\n",
            "Question: 3290+7122 | True: 10412 | Predicted: 10412 | ✓\n",
            "Question: 8828+3584 | True: 12412 | Predicted: 12412 | ✓\n",
            "Question: 3545-593 | True: 2952 | Predicted: 2952 | ✓\n",
            "Question: 9358+2583 | True: 11941 | Predicted: 11941 | ✓\n",
            "\n",
            "Epoch 202/250:\n",
            "Train Loss: 1.7631, Train Accuracy: 0.8061, Val Loss: 1.7521, Val Accuracy: 0.8518\n",
            "Epoch 203/250:\n",
            "Train Loss: 1.7621, Train Accuracy: 0.8100, Val Loss: 1.7506, Val Accuracy: 0.8575\n",
            "Epoch 204/250:\n",
            "Train Loss: 1.7616, Train Accuracy: 0.8120, Val Loss: 1.7518, Val Accuracy: 0.8538\n",
            "Epoch 205/250:\n",
            "Train Loss: 1.7621, Train Accuracy: 0.8104, Val Loss: 1.7527, Val Accuracy: 0.8530\n",
            "Epoch 206/250:\n",
            "Train Loss: 1.7613, Train Accuracy: 0.8138, Val Loss: 1.7507, Val Accuracy: 0.8568\n",
            "\n",
            "Validation Examples:\n",
            "Question: 420+3095 | True: 3515 | Predicted: 3515 | ✓\n",
            "Question: 249+4477 | True: 4726 | Predicted: 4726 | ✓\n",
            "Question: 9478-9435 | True: 43 | Predicted: -957 | ✗\n",
            "Question: 791+3661 | True: 4452 | Predicted: 4452 | ✓\n",
            "Question: 7601+5765 | True: 13366 | Predicted: 13366 | ✓\n",
            "\n",
            "Epoch 207/250:\n",
            "Train Loss: 1.7628, Train Accuracy: 0.8080, Val Loss: 1.7537, Val Accuracy: 0.8475\n",
            "Epoch 208/250:\n",
            "Train Loss: 1.7627, Train Accuracy: 0.8096, Val Loss: 1.7522, Val Accuracy: 0.8560\n",
            "Epoch 209/250:\n",
            "Train Loss: 1.7614, Train Accuracy: 0.8119, Val Loss: 1.7528, Val Accuracy: 0.8515\n",
            "Epoch 210/250:\n",
            "Train Loss: 1.7606, Train Accuracy: 0.8140, Val Loss: 1.7537, Val Accuracy: 0.8482\n",
            "Epoch 211/250:\n",
            "Train Loss: 1.7615, Train Accuracy: 0.8114, Val Loss: 1.7512, Val Accuracy: 0.8550\n",
            "\n",
            "Validation Examples:\n",
            "Question: 2005-703 | True: 1302 | Predicted: 1302 | ✓\n",
            "Question: 8662-5208 | True: 3454 | Predicted: 3454 | ✓\n",
            "Question: 7808-7162 | True: 646 | Predicted: 2646 | ✗\n",
            "Question: 138+133 | True: 271 | Predicted: 127 | ✗\n",
            "Question: 9027-5509 | True: 3518 | Predicted: 3518 | ✓\n",
            "\n",
            "Epoch 212/250:\n",
            "Train Loss: 1.7599, Train Accuracy: 0.8149, Val Loss: 1.7502, Val Accuracy: 0.8598\n",
            "Epoch 213/250:\n",
            "Train Loss: 1.7623, Train Accuracy: 0.8089, Val Loss: 1.7518, Val Accuracy: 0.8528\n",
            "Epoch 214/250:\n",
            "Train Loss: 1.7620, Train Accuracy: 0.8101, Val Loss: 1.7543, Val Accuracy: 0.8495\n",
            "Epoch 215/250:\n",
            "Train Loss: 1.7602, Train Accuracy: 0.8141, Val Loss: 1.7511, Val Accuracy: 0.8585\n",
            "Epoch 216/250:\n",
            "Train Loss: 1.7594, Train Accuracy: 0.8175, Val Loss: 1.7507, Val Accuracy: 0.8565\n",
            "\n",
            "Validation Examples:\n",
            "Question: 273+8492 | True: 8765 | Predicted: 8765 | ✓\n",
            "Question: 1738+7944 | True: 9682 | Predicted: 9682 | ✓\n",
            "Question: 4012-8082 | True: -4070 | Predicted: -4070 | ✓\n",
            "Question: 2524+730 | True: 3254 | Predicted: 3254 | ✓\n",
            "Question: 5226-5907 | True: -681 | Predicted: -671 | ✗\n",
            "\n",
            "Epoch 217/250:\n",
            "Train Loss: 1.7605, Train Accuracy: 0.8145, Val Loss: 1.7509, Val Accuracy: 0.8548\n",
            "Epoch 218/250:\n",
            "Train Loss: 1.7617, Train Accuracy: 0.8097, Val Loss: 1.7500, Val Accuracy: 0.8568\n",
            "Epoch 219/250:\n",
            "Train Loss: 1.7588, Train Accuracy: 0.8186, Val Loss: 1.7505, Val Accuracy: 0.8565\n",
            "Epoch 220/250:\n",
            "Train Loss: 1.7604, Train Accuracy: 0.8141, Val Loss: 1.7505, Val Accuracy: 0.8595\n",
            "Epoch 221/250:\n",
            "Train Loss: 1.7599, Train Accuracy: 0.8148, Val Loss: 1.7502, Val Accuracy: 0.8562\n",
            "\n",
            "Validation Examples:\n",
            "Question: 2106+6613 | True: 8719 | Predicted: 8719 | ✓\n",
            "Question: 8483-1503 | True: 6980 | Predicted: 6980 | ✓\n",
            "Question: 5385+3743 | True: 9128 | Predicted: 9128 | ✓\n",
            "Question: 8786-8985 | True: -199 | Predicted: -299 | ✗\n",
            "Question: 485+6134 | True: 6619 | Predicted: 6619 | ✓\n",
            "\n",
            "Epoch 222/250:\n",
            "Train Loss: 1.7595, Train Accuracy: 0.8162, Val Loss: 1.7505, Val Accuracy: 0.8585\n",
            "Epoch 223/250:\n",
            "Train Loss: 1.7606, Train Accuracy: 0.8154, Val Loss: 1.7538, Val Accuracy: 0.8535\n",
            "Epoch 224/250:\n",
            "Train Loss: 1.7612, Train Accuracy: 0.8106, Val Loss: 1.7493, Val Accuracy: 0.8595\n",
            "Epoch 225/250:\n",
            "Train Loss: 1.7586, Train Accuracy: 0.8181, Val Loss: 1.7506, Val Accuracy: 0.8568\n",
            "Epoch 226/250:\n",
            "Train Loss: 1.7591, Train Accuracy: 0.8174, Val Loss: 1.7509, Val Accuracy: 0.8555\n",
            "\n",
            "Validation Examples:\n",
            "Question: 2330-9136 | True: -6806 | Predicted: -6806 | ✓\n",
            "Question: 17+2967 | True: 2984 | Predicted: 2984 | ✓\n",
            "Question: 9888-1351 | True: 8537 | Predicted: 8537 | ✓\n",
            "Question: 9100-5838 | True: 3262 | Predicted: 3262 | ✓\n",
            "Question: 1966-5876 | True: -3910 | Predicted: -3910 | ✓\n",
            "\n",
            "Epoch 227/250:\n",
            "Train Loss: 1.7582, Train Accuracy: 0.8202, Val Loss: 1.7496, Val Accuracy: 0.8568\n",
            "Epoch 228/250:\n",
            "Train Loss: 1.7594, Train Accuracy: 0.8142, Val Loss: 1.7502, Val Accuracy: 0.8545\n",
            "Epoch 229/250:\n",
            "Train Loss: 1.7589, Train Accuracy: 0.8172, Val Loss: 1.7487, Val Accuracy: 0.8600\n",
            "Epoch 230/250:\n",
            "Train Loss: 1.7586, Train Accuracy: 0.8188, Val Loss: 1.7518, Val Accuracy: 0.8542\n",
            "Epoch 231/250:\n",
            "Train Loss: 1.7603, Train Accuracy: 0.8136, Val Loss: 1.7508, Val Accuracy: 0.8520\n",
            "\n",
            "Validation Examples:\n",
            "Question: 782+3272 | True: 4054 | Predicted: 4054 | ✓\n",
            "Question: 8611-7782 | True: 829 | Predicted: 8829 | ✗\n",
            "Question: 273+8492 | True: 8765 | Predicted: 8765 | ✓\n",
            "Question: 2524+730 | True: 3254 | Predicted: 3254 | ✓\n",
            "Question: 4449-9204 | True: -4755 | Predicted: -4755 | ✓\n",
            "\n",
            "Epoch 232/250:\n",
            "Train Loss: 1.7590, Train Accuracy: 0.8180, Val Loss: 1.7498, Val Accuracy: 0.8610\n",
            "Epoch 233/250:\n",
            "Train Loss: 1.7604, Train Accuracy: 0.8143, Val Loss: 1.7547, Val Accuracy: 0.8460\n",
            "Epoch 234/250:\n",
            "Train Loss: 1.7594, Train Accuracy: 0.8169, Val Loss: 1.7502, Val Accuracy: 0.8572\n",
            "Epoch 235/250:\n",
            "Train Loss: 1.7598, Train Accuracy: 0.8161, Val Loss: 1.7490, Val Accuracy: 0.8585\n",
            "Epoch 236/250:\n",
            "Train Loss: 1.7582, Train Accuracy: 0.8201, Val Loss: 1.7494, Val Accuracy: 0.8605\n",
            "\n",
            "Validation Examples:\n",
            "Question: 7315+861 | True: 8176 | Predicted: 8176 | ✓\n",
            "Question: 8041-2394 | True: 5647 | Predicted: 5647 | ✓\n",
            "Question: 5335+9732 | True: 15067 | Predicted: 15067 | ✓\n",
            "Question: 4774-2030 | True: 2744 | Predicted: 2744 | ✓\n",
            "Question: 3074-4680 | True: -1606 | Predicted: -1606 | ✓\n",
            "\n",
            "Epoch 237/250:\n",
            "Train Loss: 1.7579, Train Accuracy: 0.8197, Val Loss: 1.7501, Val Accuracy: 0.8588\n",
            "Epoch 238/250:\n",
            "Train Loss: 1.7564, Train Accuracy: 0.8258, Val Loss: 1.7493, Val Accuracy: 0.8602\n",
            "Epoch 239/250:\n",
            "Train Loss: 1.7571, Train Accuracy: 0.8225, Val Loss: 1.7518, Val Accuracy: 0.8558\n",
            "Epoch 240/250:\n",
            "Train Loss: 1.7577, Train Accuracy: 0.8224, Val Loss: 1.7503, Val Accuracy: 0.8558\n",
            "Epoch 241/250:\n",
            "Train Loss: 1.7596, Train Accuracy: 0.8171, Val Loss: 1.7520, Val Accuracy: 0.8518\n",
            "\n",
            "Validation Examples:\n",
            "Question: 1510+2015 | True: 3525 | Predicted: 3525 | ✓\n",
            "Question: 7642-4903 | True: 2739 | Predicted: 2739 | ✓\n",
            "Question: 9504+4764 | True: 14268 | Predicted: 14268 | ✓\n",
            "Question: 5112+8033 | True: 13145 | Predicted: 13145 | ✓\n",
            "Question: 3916-6927 | True: -3011 | Predicted: -3011 | ✓\n",
            "\n",
            "Epoch 242/250:\n",
            "Train Loss: 1.7574, Train Accuracy: 0.8229, Val Loss: 1.7495, Val Accuracy: 0.8578\n",
            "Epoch 243/250:\n",
            "Train Loss: 1.7569, Train Accuracy: 0.8223, Val Loss: 1.7494, Val Accuracy: 0.8568\n",
            "Epoch 244/250:\n",
            "Train Loss: 1.7577, Train Accuracy: 0.8209, Val Loss: 1.7500, Val Accuracy: 0.8588\n",
            "Epoch 245/250:\n",
            "Train Loss: 1.7592, Train Accuracy: 0.8171, Val Loss: 1.7497, Val Accuracy: 0.8560\n",
            "Epoch 246/250:\n",
            "Train Loss: 1.7578, Train Accuracy: 0.8207, Val Loss: 1.7507, Val Accuracy: 0.8548\n",
            "\n",
            "Validation Examples:\n",
            "Question: 3002+4666 | True: 7668 | Predicted: 7668 | ✓\n",
            "Question: 5911-8098 | True: -2187 | Predicted: -2187 | ✓\n",
            "Question: 1476+587 | True: 2063 | Predicted: 2063 | ✓\n",
            "Question: 4460-6176 | True: -1716 | Predicted: -1716 | ✓\n",
            "Question: 3360+1906 | True: 5266 | Predicted: 5266 | ✓\n",
            "\n",
            "Epoch 247/250:\n",
            "Train Loss: 1.7599, Train Accuracy: 0.8159, Val Loss: 1.7492, Val Accuracy: 0.8575\n",
            "Epoch 248/250:\n",
            "Train Loss: 1.7570, Train Accuracy: 0.8226, Val Loss: 1.7485, Val Accuracy: 0.8612\n",
            "Epoch 249/250:\n",
            "Train Loss: 1.7565, Train Accuracy: 0.8241, Val Loss: 1.7481, Val Accuracy: 0.8608\n",
            "Epoch 250/250:\n",
            "Train Loss: 1.7573, Train Accuracy: 0.8224, Val Loss: 1.7495, Val Accuracy: 0.8585\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from typing import List, Tuple, Dict\n",
        "import random\n",
        "from math import sqrt\n",
        "\n",
        "# Configuration\n",
        "config = {\n",
        "    \"training_size\": 40000,\n",
        "    \"digits\": 4,\n",
        "    \"embedding_size\": 128,\n",
        "    \"num_heads\": 8,\n",
        "    \"num_layers\": 3,\n",
        "    \"ffn_hidden_size\": 512,\n",
        "    \"dropout\": 0.1,\n",
        "    \"batch_size\": 128,\n",
        "    \"iterations\": 250,\n",
        "    \"learning_rate\": 1e-4,\n",
        "}\n",
        "chars = '0123456789-+ '\n",
        "\n",
        "class CharacterTable:\n",
        "    \"\"\"Handles encoding/decoding of characters to/from one-hot vectors\"\"\"\n",
        "    def __init__(self, chars: str):\n",
        "        self.chars = sorted(set(chars))\n",
        "        self.char_indices = {c: i for i, c in enumerate(self.chars)}\n",
        "        self.indices_char = {i: c for i, c in enumerate(self.chars)}\n",
        "        self.num_chars = len(self.chars)\n",
        "\n",
        "    def encode(self, C: str, num_rows: int) -> torch.Tensor:\n",
        "        \"\"\"One hot encode given string C.\"\"\"\n",
        "        x = torch.zeros(num_rows, len(self.chars))\n",
        "        for i, c in enumerate(C):\n",
        "            x[i, self.char_indices[c]] = 1\n",
        "        return x\n",
        "\n",
        "    def decode(self, x: torch.Tensor, calc_argmax: bool = True) -> str:\n",
        "        if calc_argmax:\n",
        "            x = torch.argmax(x, dim=-1)\n",
        "        return ''.join(self.indices_char[i.item()] for i in x)\n",
        "\n",
        "\n",
        "class ArithmeticDataset(Dataset):\n",
        "    def __init__(self, x: torch.Tensor, y: torch.Tensor):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        return self.x[idx], self.y[idx]\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, embedding_size: int, max_len: int = 1000):\n",
        "        super().__init__()\n",
        "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "        div_term = torch.exp(torch.arange(0, embedding_size, 2).float() * -(torch.log(torch.tensor(10000.0)) / embedding_size))\n",
        "        pe = torch.zeros(max_len, embedding_size)\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return x + self.pe[:x.size(1)]\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, input_size: int, embedding_size: int, num_heads: int, num_layers: int,\n",
        "                 output_size: int, output_length: int, ffn_hidden_size: int, dropout: float):\n",
        "        super().__init__()\n",
        "        self.embedding_size = embedding_size\n",
        "        self.output_length = output_length\n",
        "        self.input_length = 2*config['digits'] + 1\n",
        "\n",
        "        # Input embedding and positional encoding\n",
        "        self.embedding = nn.Linear(input_size, embedding_size)\n",
        "        self.pos_encoder = PositionalEncoding(embedding_size, max_len = self.input_length)\n",
        "\n",
        "        # Transformer Encoder\n",
        "        self.encoder_layers = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(embedding_size, num_heads, ffn_hidden_size, dropout, batch_first=True),\n",
        "            num_layers\n",
        "        )\n",
        "\n",
        "        # Output layer\n",
        "        self.output_layer = nn.Linear(embedding_size, output_size)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "        # Embed and add positional encoding\n",
        "        x = self.embedding(x) * sqrt(self.embedding_size) #scaling for stability\n",
        "        x = self.pos_encoder(x)\n",
        "\n",
        "        # Transformer Encoder\n",
        "        encoder_outputs = self.encoder_layers(x)\n",
        "\n",
        "        # Project the output of each token to desired output length\n",
        "        output = self.output_layer(encoder_outputs)\n",
        "\n",
        "        return torch.softmax(output, dim=-1)\n",
        "\n",
        "def generate_arithmetic_data(config: Dict) -> Tuple[List[str], List[str]]:\n",
        "    \"\"\"Generate data for both addition and subtraction\"\"\"\n",
        "    questions = []\n",
        "    expected = []\n",
        "    seen = set()\n",
        "    maxlen = config[\"digits\"] + 1 + config[\"digits\"]\n",
        "\n",
        "    print('Generating data...')\n",
        "    while len(questions) < config[\"training_size\"]:\n",
        "        # Generate random numbers with max length of digits\n",
        "        a = random.randint(0, 10 ** config[\"digits\"] - 1)\n",
        "        b = random.randint(0, 10 ** config[\"digits\"] - 1)\n",
        "\n",
        "        # Randomly choose operation\n",
        "        op = random.choice(['+', '-'])\n",
        "\n",
        "        # Calculate result\n",
        "        if op == '+':\n",
        "            result = a + b\n",
        "        else:\n",
        "            result = a - b\n",
        "\n",
        "        # Create question string\n",
        "        q = f'{a}{op}{b}'\n",
        "\n",
        "        # Skip if we've seen this before\n",
        "        key = (a, op, b)\n",
        "        if key in seen:\n",
        "            continue\n",
        "        seen.add(key)\n",
        "\n",
        "        # Pad the data\n",
        "        query = q + ' ' * (maxlen - len(q))\n",
        "        ans = str(result)\n",
        "        ans += ' ' * (config[\"digits\"] + 1 - len(ans))\n",
        "\n",
        "        questions.append(query)\n",
        "        expected.append(ans)\n",
        "\n",
        "        if len(questions) >= config[\"training_size\"]:\n",
        "            break\n",
        "\n",
        "    print(f'Total arithmetic questions: {len(questions)}')\n",
        "    return questions, expected\n",
        "\n",
        "def prepare_data(questions: List[str], expected: List[str], ctable: CharacterTable) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"Prepare data by converting to one-hot encoded tensors\"\"\"\n",
        "    maxlen = config[\"digits\"] + 1 + config[\"digits\"]\n",
        "    x = torch.zeros((len(questions), maxlen, len(chars)))\n",
        "    y = torch.zeros((len(questions), config[\"digits\"] + 1, len(chars)))\n",
        "\n",
        "    for i, sentence in enumerate(questions):\n",
        "        x[i] = ctable.encode(sentence, maxlen)\n",
        "    for i, sentence in enumerate(expected):\n",
        "        y[i] = ctable.encode(sentence, config[\"digits\"] + 1)\n",
        "\n",
        "    return x, y\n",
        "\n",
        "def train_epoch(model: nn.Module, train_loader: DataLoader, criterion: nn.Module,\n",
        "                optimizer: torch.optim.Optimizer, device: torch.device) -> float:\n",
        "    \"\"\"Train for one epoch\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for batch_x, batch_y in train_loader:\n",
        "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(batch_x)\n",
        "\n",
        "        loss = criterion(output[:, :config['digits'] + 1].reshape(-1, len(chars)), batch_y.view(-1, len(chars)))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        predicted_chars = torch.argmax(output[:, :config['digits'] + 1], dim=-1)\n",
        "        true_chars = torch.argmax(batch_y, dim=-1)\n",
        "\n",
        "        correct_predictions = (predicted_chars == true_chars).all(dim=1).sum().item()\n",
        "        total_correct += correct_predictions\n",
        "        total_samples += batch_x.size(0)\n",
        "\n",
        "    accuracy = total_correct / total_samples if total_samples > 0 else 0\n",
        "    return total_loss / len(train_loader), accuracy\n",
        "\n",
        "def validate(model: nn.Module, val_loader: DataLoader, criterion: nn.Module,\n",
        "            device: torch.device) -> float:\n",
        "    \"\"\"Validate the model\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y in val_loader:\n",
        "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "            output = model(batch_x)\n",
        "            loss = criterion(output[:, :config['digits'] + 1].reshape(-1, len(chars)), batch_y.view(-1, len(chars)))\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            predicted_chars = torch.argmax(output[:, :config['digits'] + 1], dim=-1)\n",
        "            true_chars = torch.argmax(batch_y, dim=-1)\n",
        "\n",
        "            correct_predictions = (predicted_chars == true_chars).all(dim=1).sum().item()\n",
        "            total_correct += correct_predictions\n",
        "            total_samples += batch_x.size(0)\n",
        "\n",
        "    accuracy = total_correct / total_samples if total_samples > 0 else 0\n",
        "    return total_loss / len(val_loader), accuracy\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Set device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Initialize character table\n",
        "    ctable = CharacterTable(chars)\n",
        "\n",
        "    # Generate and prepare data\n",
        "    questions, expected = generate_arithmetic_data(config)\n",
        "    x, y = prepare_data(questions, expected, ctable)\n",
        "\n",
        "    # Shuffle data\n",
        "    indices = torch.randperm(len(y))\n",
        "    x = x[indices]\n",
        "    y = y[indices]\n",
        "\n",
        "    # Split data\n",
        "    split_at = len(x) - len(x) // 10\n",
        "    x_train, x_val = x[:split_at], x[split_at:]\n",
        "    y_train, y_val = y[:split_at], y[split_at:]\n",
        "\n",
        "    # Create datasets and dataloaders\n",
        "    train_dataset = ArithmeticDataset(x_train, y_train)\n",
        "    val_dataset = ArithmeticDataset(x_val, y_val)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=config[\"batch_size\"])\n",
        "\n",
        "    # Initialize model\n",
        "    model = TransformerModel(\n",
        "        input_size=len(chars),\n",
        "        embedding_size=config[\"embedding_size\"],\n",
        "        num_heads=config[\"num_heads\"],\n",
        "        num_layers=config[\"num_layers\"],\n",
        "        output_size=len(chars),\n",
        "        output_length=config[\"digits\"] + 1,\n",
        "        ffn_hidden_size=config[\"ffn_hidden_size\"],\n",
        "        dropout=config[\"dropout\"]\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Training loop\n",
        "    print(\"Starting training...\")\n",
        "    for epoch in range(config[\"iterations\"]):\n",
        "        train_loss, train_accuracy = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        val_loss, val_accuracy = validate(model, val_loader, criterion, device)\n",
        "\n",
        "        print(f'Epoch {epoch + 1}/{config[\"iterations\"]}:')\n",
        "        print(f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
        "\n",
        "        # Validation samples\n",
        "        if epoch % 5 == 0:\n",
        "            model.eval()\n",
        "            print(\"\\nValidation Examples:\")\n",
        "            with torch.no_grad():\n",
        "                for i in range(5):\n",
        "                    idx = np.random.randint(0, len(x_val))\n",
        "                    x_sample = x_val[idx:idx+1].to(device)\n",
        "                    y_true = y_val[idx]\n",
        "\n",
        "                    pred = model(x_sample)\n",
        "                    pred = pred.cpu()\n",
        "\n",
        "                    q = ctable.decode(x_sample[0].cpu())\n",
        "                    correct = ctable.decode(y_true)\n",
        "                    guess = ctable.decode(pred[0, :config['digits'] + 1])\n",
        "\n",
        "                    print(f'Question: {q.strip()} | True: {correct.strip()} | '\n",
        "                          f'Predicted: {guess.strip()} | '\n",
        "                          f'{\"✓\" if correct.strip() == guess.strip() else \"✗\"}')\n",
        "            print()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLYA4r3LZVXj"
      },
      "source": [
        "Key Changes and Explanation:\n",
        "\n",
        "Transformer Model:\n",
        "\n",
        "Embedding Layer: Converts one-hot vectors to dense embeddings.\n",
        "\n",
        "Positional Encoding: Adds positional information to the embeddings.positional encodings are crucial to incorporate sequence order which is not handled in the transformer's self attention.\n",
        "\n",
        "Transformer Encoder: The core of the model, using multi-head self-attention to capture relationships between all positions at once, unlike the LSTM in your original model which was working sequentially. This makes the model way more powerful.\n",
        "\n",
        "Output Layer: Projects the final transformer outputs back to the size of your character set.\n",
        "\n",
        "\n",
        "Data generation and processing: remains largely the same.\n",
        "\n",
        "Hyperparameter Tuning\n",
        "\n",
        "\n",
        "\n",
        "Improved Attention: The multi-head self-attention allows for more nuanced understanding of the input tokens compared to the naive attention in the original code.\n",
        "\n",
        "Parallel Processing: The transformer can be highly parallelized on GPU, which greatly speeds up training.\n",
        "\n",
        "Better Representation: The Transformer excels at learning relationships between words/tokens (in this case characters), allowing it to better learn how to perform arithmetic.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voVYROYNlO49"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-d0eIM6FeaM"
      },
      "source": [
        "## Part 2: A language translation model with attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80jhFbWPMW_a"
      },
      "source": [
        "In this part of the problem set we are going to implement a translation with a Sequence to Sequence Network and Attention model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgL38lJGTYaF"
      },
      "source": [
        "0) Please go over the NLP From Scratch: Translation with a Sequence to Sequence Network and Attention [tutorial](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html). This attention model is very similar to what was learned in class (Luong), but a bit different. What are the main differences between  Badahnau and Luong attention mechanisms?    \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfyPTPfbbHbl"
      },
      "source": [
        "Similarities to Luong Attention (Discussed in Class)\n",
        "\n",
        "Goal: Both attention mechanisms share the primary goal of allowing the decoder to focus on the relevant parts of the input sequence.\n",
        "\n",
        "Context Vector: Both calculate a context vector by taking a weighted sum of the encoder's outputs, where the weights reflect the alignment between the decoder's current state and the encoder's states.\n",
        "\n",
        "Softmax Normalization: They use softmax to normalize the attention scores into probability distribution, where the attention weights sum to one.\n",
        "\n",
        "How This Code Differs from What You Might've Seen in Class\n",
        "\n",
        "Bahdanau: The code explicitly uses the Bahdanau attention mechanism which uses a single layer MLP to calculate the alignment scores. Whereas Luong attention may involve a dot product, general, or concat product to calculate the alignment scores.\n",
        "\n",
        "GRU Instead of LSTM: The code uses GRUs instead of LSTMs for the RNNs, but both are valid in seq2seq models. The math is also similar, therefore not causing a fundamental difference in results.\n",
        "\n",
        "Weight Calculation: Bahdanau attention combines the decoder and encoder hidden states to produce the attention weights before passing into the GRU cell in the decoder, while some forms of Luong attention (i.e. dot product form) may calculate weights after the GRU cell.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBX873GJlDl9"
      },
      "source": [
        "1.a) Using `!wget`, `!unzip` , download and extract the [hebrew-english](https://www.manythings.org/anki/) sentence pairs text file to the Colab `content/`  folder (or local folder if not using Colab).\n",
        "1.b) The `heb.txt` must be parsed and cleaned (see tutorial for requirements or change the code as you see fit).   \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvIIlNvPlGWB"
      },
      "source": [
        "2.a) Use the tutorial example to build  and train a Hebrew to English translation model with attention (using the parameters in the code cell below). Apply the same `eng_prefixes` filter to limit the train/test data.   \n",
        "2.b) Evaluate your trained model randomly on 20 sentences.  \n",
        "2.c) Show the attention plot for 5 random sentences.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcqtVxkclIWG"
      },
      "source": [
        "3) Do you think this model performs well? Why or why not? What are its limitations/disadvantages? What would you do to improve it?  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2VSrRNtlJub"
      },
      "source": [
        "4) Using any neural network architecture of your liking, build  a model with the aim to beat the model in 2.a. Compare your results in a meaningful way, and add a short explanation to why you think/thought your suggested network is better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "c-tVmomvXcKk"
      },
      "outputs": [],
      "source": [
        "# use the following parameters:\n",
        "MAX_LENGTH = 10\n",
        "hidden_size = 128\n",
        "epochs = 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9-C4pLEXzCF"
      },
      "source": [
        "SOLUTION:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_WrHkLD6p813"
      },
      "outputs": [],
      "source": [
        "### MISSING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0sWxpN2Zfc_2",
        "outputId": "dec000ed-fa8b-49d4-c169-8ef3876cc39f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-02 13:54:27--  https://www.manythings.org/anki/heb-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 173.254.30.110\n",
            "Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4466359 (4.3M) [application/zip]\n",
            "Saving to: ‘heb-eng.zip’\n",
            "\n",
            "heb-eng.zip         100%[===================>]   4.26M  2.29MB/s    in 1.9s    \n",
            "\n",
            "2025-02-02 13:54:31 (2.29 MB/s) - ‘heb-eng.zip’ saved [4466359/4466359]\n",
            "\n",
            "Archive:  heb-eng.zip\n",
            "  inflating: data/_about.txt         \n",
            "  inflating: data/heb.txt            \n"
          ]
        }
      ],
      "source": [
        "# In Colab:\n",
        "!wget https://www.manythings.org/anki/heb-eng.zip\n",
        "!unzip heb-eng.zip -d data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "9gTGvViK8wVY"
      },
      "outputs": [],
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "kBGyXvdC82J4"
      },
      "outputs": [],
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "6G39lZz884XI"
      },
      "outputs": [],
      "source": [
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# https://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
        "    return s.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "jyl01pTW84dr"
      },
      "outputs": [],
      "source": [
        "def readLangs(lang1, lang2, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Read the file and split into lines\n",
        "    lines = open('data/%s.txt' % (lang2), encoding='utf-8').\\\n",
        "        read().strip().split('\\n')\n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = []\n",
        "\n",
        "    for line in lines:\n",
        "        parts = line.split('\\t')\n",
        "        if len(parts) >= 2:  # Ensure the line has at least two parts\n",
        "            eng = normalizeString(parts[0])  # Normalize the English sentence\n",
        "            heb = parts[1].strip()  # Keep the Hebrew sentence as is\n",
        "            pairs.append((eng, heb))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Reverse pairs, make Lang instances\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "    else:\n",
        "        input_lang = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Zu5yKY7S84gR"
      },
      "outputs": [],
      "source": [
        "MAX_LENGTH = 10\n",
        "\n",
        "eng_prefixes = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s \",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \"\n",
        ")\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
        "        p[1].startswith(eng_prefixes)\n",
        "\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "AHepRNKs84ih",
        "outputId": "f62be673-721b-4988-ad38-286f21ce5f38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines...\n",
            "Read 128133 sentence pairs\n",
            "Trimmed to 9320 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "heb 7519\n",
            "eng 3067\n",
            "['הם נפלאים.', 'they re gorgeous']\n"
          ]
        }
      ],
      "source": [
        "def prepareData(lang1, lang2, reverse=False):\n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "input_lang, output_lang, pairs = prepareData('eng', 'heb', True)\n",
        "print(random.choice(pairs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "zgdigdFi_Jei"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "_xPA734S9ctT"
      },
      "outputs": [],
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, input):\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        output, hidden = self.gru(embedded)\n",
        "        return output, hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "tAutYS2t9lLy"
      },
      "outputs": [],
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
        "        batch_size = encoder_outputs.size(0)\n",
        "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoder_outputs = []\n",
        "\n",
        "        for i in range(MAX_LENGTH):\n",
        "            decoder_output, decoder_hidden  = self.forward_step(decoder_input, decoder_hidden)\n",
        "            decoder_outputs.append(decoder_output)\n",
        "\n",
        "            if target_tensor is not None:\n",
        "                # Teacher forcing: Feed the target as the next input\n",
        "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
        "            else:\n",
        "                # Without teacher forcing: use its own predictions as the next input\n",
        "                _, topi = decoder_output.topk(1)\n",
        "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
        "\n",
        "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
        "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
        "        return decoder_outputs, decoder_hidden, None # We return `None` for consistency in the training loop\n",
        "\n",
        "    def forward_step(self, input, hidden):\n",
        "        output = self.embedding(input)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.out(output)\n",
        "        return output, hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "nMNtu75y9pEe"
      },
      "outputs": [],
      "source": [
        "class BahdanauAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.Wa = nn.Linear(hidden_size, hidden_size)\n",
        "        self.Ua = nn.Linear(hidden_size, hidden_size)\n",
        "        self.Va = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, query, keys):\n",
        "        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
        "        scores = scores.squeeze(2).unsqueeze(1)\n",
        "\n",
        "        weights = F.softmax(scores, dim=-1)\n",
        "        context = torch.bmm(weights, keys)\n",
        "\n",
        "        return context, weights\n",
        "\n",
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.attention = BahdanauAttention(hidden_size)\n",
        "        self.gru = nn.GRU(2 * hidden_size, hidden_size, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
        "        batch_size = encoder_outputs.size(0)\n",
        "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoder_outputs = []\n",
        "        attentions = []\n",
        "\n",
        "        for i in range(MAX_LENGTH):\n",
        "            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n",
        "                decoder_input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "            decoder_outputs.append(decoder_output)\n",
        "            attentions.append(attn_weights)\n",
        "\n",
        "            if target_tensor is not None:\n",
        "                # Teacher forcing: Feed the target as the next input\n",
        "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
        "            else:\n",
        "                # Without teacher forcing: use its own predictions as the next input\n",
        "                _, topi = decoder_output.topk(1)\n",
        "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
        "\n",
        "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
        "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
        "        attentions = torch.cat(attentions, dim=1)\n",
        "\n",
        "        return decoder_outputs, decoder_hidden, attentions\n",
        "\n",
        "\n",
        "    def forward_step(self, input, hidden, encoder_outputs):\n",
        "        embedded =  self.dropout(self.embedding(input))\n",
        "\n",
        "        query = hidden.permute(1, 0, 2)\n",
        "        context, attn_weights = self.attention(query, encoder_outputs)\n",
        "        input_gru = torch.cat((embedded, context), dim=2)\n",
        "\n",
        "        output, hidden = self.gru(input_gru, hidden)\n",
        "        output = self.out(output)\n",
        "\n",
        "        return output, hidden, attn_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "jZgjs3Zs9rhR"
      },
      "outputs": [],
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)\n",
        "\n",
        "def get_dataloader(batch_size):\n",
        "    input_lang, output_lang, pairs = prepareData('eng', 'heb', True)\n",
        "\n",
        "    n = len(pairs)\n",
        "    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
        "    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
        "\n",
        "    for idx, (inp, tgt) in enumerate(pairs):\n",
        "        inp_ids = indexesFromSentence(input_lang, inp)\n",
        "        tgt_ids = indexesFromSentence(output_lang, tgt)\n",
        "        inp_ids.append(EOS_token)\n",
        "        tgt_ids.append(EOS_token)\n",
        "        input_ids[idx, :len(inp_ids)] = inp_ids\n",
        "        target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
        "\n",
        "    train_data = TensorDataset(torch.LongTensor(input_ids).to(device),\n",
        "                               torch.LongTensor(target_ids).to(device))\n",
        "\n",
        "    train_sampler = RandomSampler(train_data)\n",
        "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "    return input_lang, output_lang, train_dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "tCiClscg9twG"
      },
      "outputs": [],
      "source": [
        "def train_epoch(dataloader, encoder, decoder, encoder_optimizer,\n",
        "          decoder_optimizer, criterion):\n",
        "\n",
        "    total_loss = 0\n",
        "    for data in dataloader:\n",
        "        input_tensor, target_tensor = data\n",
        "\n",
        "        encoder_optimizer.zero_grad()\n",
        "        decoder_optimizer.zero_grad()\n",
        "\n",
        "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
        "\n",
        "        loss = criterion(\n",
        "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
        "            target_tensor.view(-1)\n",
        "        )\n",
        "        loss.backward()\n",
        "\n",
        "        encoder_optimizer.step()\n",
        "        decoder_optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "iXySvlt29w7e"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "jmg6ICaZ9zp_"
      },
      "outputs": [],
      "source": [
        "def train(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001,\n",
        "               print_every=100, plot_every=100):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if epoch % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs),\n",
        "                                        epoch, epoch / n_epochs * 100, print_loss_avg))\n",
        "\n",
        "        if epoch % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "tifbKI5393Bs"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "AV-4B8Eg974V"
      },
      "outputs": [],
      "source": [
        "def evaluate(encoder, decoder, sentence, input_lang, output_lang):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "\n",
        "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "        decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n",
        "\n",
        "        _, topi = decoder_outputs.topk(1)\n",
        "        decoded_ids = topi.squeeze()\n",
        "\n",
        "        decoded_words = []\n",
        "        for idx in decoded_ids:\n",
        "            if idx.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            decoded_words.append(output_lang.index2word[idx.item()])\n",
        "    return decoded_words, decoder_attn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "W6n1ot6x9-Ay"
      },
      "outputs": [],
      "source": [
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words, _ = evaluate(encoder, decoder, pair[0], input_lang, output_lang)\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_MyO84X-BWM",
        "outputId": "0123d8f3-94a8-47ea-c801-7296eb2fc5d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines...\n",
            "Read 128133 sentence pairs\n",
            "Trimmed to 9320 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "heb 7519\n",
            "eng 3067\n",
            "0m 37s (- 9m 26s) (5 6%) 1.7979\n",
            "1m 9s (- 8m 6s) (10 12%) 0.9870\n",
            "1m 41s (- 7m 18s) (15 18%) 0.6160\n",
            "2m 13s (- 6m 39s) (20 25%) 0.3841\n",
            "2m 44s (- 6m 2s) (25 31%) 0.2363\n",
            "3m 16s (- 5m 27s) (30 37%) 0.1482\n",
            "3m 48s (- 4m 54s) (35 43%) 0.0975\n",
            "4m 20s (- 4m 20s) (40 50%) 0.0719\n",
            "4m 52s (- 3m 47s) (45 56%) 0.0569\n",
            "5m 23s (- 3m 14s) (50 62%) 0.0470\n",
            "5m 55s (- 2m 41s) (55 68%) 0.0410\n",
            "6m 27s (- 2m 9s) (60 75%) 0.0363\n",
            "6m 59s (- 1m 36s) (65 81%) 0.0333\n",
            "7m 31s (- 1m 4s) (70 87%) 0.0310\n",
            "8m 2s (- 0m 32s) (75 93%) 0.0286\n",
            "8m 34s (- 0m 0s) (80 100%) 0.0278\n"
          ]
        }
      ],
      "source": [
        "hidden_size = 128\n",
        "batch_size = 32\n",
        "\n",
        "input_lang, output_lang, train_dataloader = get_dataloader(batch_size)\n",
        "\n",
        "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "decoder = AttnDecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
        "\n",
        "train(train_dataloader, encoder, decoder, 80, print_every=5, plot_every=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zvik7VZ7-DLy",
        "outputId": "ef3a2f46-c866-46f6-dc06-feebfc304a8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> הוא בכושר גופני טוב.\n",
            "= he is in good physical condition\n",
            "< he is in good physical condition <EOS>\n",
            "\n",
            "> אני בטוח שתום מתחרט על כך עכשיו.\n",
            "= i m sure tom regrets that now\n",
            "< i m sure tom regrets that now <EOS>\n",
            "\n",
            "> אתה כבר מספיק בוגר כדי לכלכל את עצמך.\n",
            "= you re now old enough to support yourself\n",
            "< you are now old enough to support yourself <EOS>\n",
            "\n",
            "> חזרתי מחופש.\n",
            "= i m back from vacation\n",
            "< i m back from vacation <EOS>\n",
            "\n",
            "> אני בטוחה שתום בדרך.\n",
            "= i m sure tom is on his way\n",
            "< tom s sure his custom motorcycle motorcycle their defenseless buried\n",
            "\n",
            "> אני מוטרדת.\n",
            "= i m upset\n",
            "< i m troubled <EOS>\n",
            "\n",
            "> אני לא מסוגל לתרגם את המשפט הזה.\n",
            "= i m not able to translate this sentence\n",
            "< i m unable to translate this sentence <EOS>\n",
            "\n",
            "> אני מתקומם.\n",
            "= i m outraged\n",
            "< i m outraged <EOS>\n",
            "\n",
            "> הוא אבא לשני ילדים.\n",
            "= he is the father of two children\n",
            "< he is the father of two children french fries <EOS>\n",
            "\n",
            "> הוא פחות או יותר בגילי.\n",
            "= he is about my age\n",
            "< he is about my age <EOS>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "encoder.eval()\n",
        "decoder.eval()\n",
        "evaluateRandomly(encoder, decoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "E7TIeyPE-Gna",
        "outputId": "6e27c709-4db2-461c-b347-5320d3e4a393"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: אני לא מומחה.\n",
            "Output: i m not an expert mother <EOS>\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 700x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: היא אדם נוח.\n",
            "Output: she is a pleasant person a rose <EOS>\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 700x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: הוא רותח מזעם.\n",
            "Output: he s outraged something <EOS>\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 700x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: אתם נמרצים.\n",
            "Output: you re energetic <EOS>\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 700x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "def showAttention(input_sentence, output_words, attentions):\n",
        "    plt.figure(figsize=(7, 5))\n",
        "    ax = plt.gca()\n",
        "\n",
        "    if torch.is_tensor(attentions):\n",
        "        attention_data = attentions.cpu().numpy()\n",
        "    else:\n",
        "        attention_data = attentions\n",
        "\n",
        "    # Flip the attention matrix horizontally for RTL text\n",
        "    attention_data = attention_data[:, ::-1]\n",
        "\n",
        "    cax = ax.matshow(attention_data, cmap='bone')\n",
        "    plt.colorbar(cax)\n",
        "\n",
        "    # Reverse the order of input labels for RTL\n",
        "    input_labels = [''] + input_sentence.split(' ')[::-1] + ['<EOS>']\n",
        "    output_labels = [''] + output_words\n",
        "\n",
        "    ax.set_xticks(range(len(input_labels)))\n",
        "    ax.set_yticks(range(len(output_labels)))\n",
        "    ax.set_xticklabels(input_labels, rotation=90)\n",
        "    ax.set_yticklabels(output_labels)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show(block=True)\n",
        "\n",
        "def evaluateAndShowAttention(input_sentence):\n",
        "    try:\n",
        "        output_words, attentions = evaluate(encoder, decoder, input_sentence, input_lang, output_lang)\n",
        "        print('Input:', input_sentence)\n",
        "        print('Output:', ' '.join(output_words))\n",
        "        showAttention(input_sentence, output_words, attentions[0, :len(output_words), :])\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {str(e)}\")\n",
        "\n",
        "evaluateAndShowAttention('אני לא מומחה.')\n",
        "\n",
        "evaluateAndShowAttention('היא אדם נוח.')\n",
        "\n",
        "evaluateAndShowAttention('הוא רותח מזעם.')\n",
        "\n",
        "evaluateAndShowAttention('אתם נמרצים.')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "PIZT5T8tE6h1"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8gtgBZnFYR6",
        "outputId": "ed85a121-b07e-4e4e-b511-ee9fb9c3cc95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines...\n",
            "Read 128133 sentence pairs\n",
            "Trimmed to 9320 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "heb 7519\n",
            "eng 3067\n",
            "Starting training...\n",
            "0m 42s (- 10m 30s) (5 6%) 1.9305\n",
            "1m 24s (- 9m 52s) (10 12%) 0.4765\n",
            "2m 6s (- 9m 9s) (15 18%) 0.1936\n",
            "2m 48s (- 8m 25s) (20 25%) 0.1238\n",
            "3m 31s (- 7m 44s) (25 31%) 0.0986\n",
            "4m 12s (- 7m 1s) (30 37%) 0.0878\n",
            "4m 54s (- 6m 18s) (35 43%) 0.0786\n",
            "5m 37s (- 5m 37s) (40 50%) 0.0742\n",
            "6m 19s (- 4m 54s) (45 56%) 0.0699\n",
            "7m 0s (- 4m 12s) (50 62%) 0.0675\n",
            "7m 45s (- 3m 31s) (55 68%) 0.0660\n",
            "8m 27s (- 2m 49s) (60 75%) 0.0622\n",
            "9m 9s (- 2m 6s) (65 81%) 0.0613\n",
            "9m 52s (- 1m 24s) (70 87%) 0.0466\n",
            "10m 33s (- 0m 42s) (75 93%) 0.0272\n",
            "11m 15s (- 0m 0s) (80 100%) 0.0247\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
        "import numpy as np\n",
        "import time\n",
        "import math\n",
        "import random\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "# Constants\n",
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "MAX_LENGTH = 10\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Data preparation functions\n",
        "def get_dataloader(batch_size=32):\n",
        "    input_lang, output_lang, pairs = prepareData('eng', 'heb', True)\n",
        "\n",
        "    n = len(pairs)\n",
        "    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
        "    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
        "\n",
        "    for idx, (inp, tgt) in enumerate(pairs):\n",
        "        inp_ids = indexesFromSentence(input_lang, inp)\n",
        "        tgt_ids = indexesFromSentence(output_lang, tgt)\n",
        "        inp_ids.append(EOS_token)\n",
        "        tgt_ids.append(EOS_token)\n",
        "        input_ids[idx, :len(inp_ids)] = inp_ids\n",
        "        target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
        "\n",
        "    train_data = TensorDataset(torch.LongTensor(input_ids).to(device),\n",
        "                              torch.LongTensor(target_ids).to(device))\n",
        "\n",
        "    train_sampler = RandomSampler(train_data)\n",
        "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "    return input_lang, output_lang, train_dataloader, pairs\n",
        "\n",
        "# Model definitions (same as before)\n",
        "class ImprovedEncoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, n_layers=2, dropout_p=0.2):\n",
        "        super(ImprovedEncoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "        self.gru = nn.GRU(\n",
        "            hidden_size,\n",
        "            hidden_size // 2,\n",
        "            num_layers=n_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "            dropout=dropout_p if n_layers > 1 else 0\n",
        "        )\n",
        "\n",
        "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
        "\n",
        "    def forward(self, input):\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        embedded = self.layer_norm(embedded)\n",
        "\n",
        "        output, hidden = self.gru(embedded)\n",
        "        hidden = self._reshape_hidden(hidden)\n",
        "\n",
        "        return output, hidden\n",
        "\n",
        "    def _reshape_hidden(self, hidden):\n",
        "        batch_size = hidden.size(1)\n",
        "        hidden = hidden.view(self.n_layers, 2, batch_size, self.hidden_size//2)\n",
        "        hidden = hidden.transpose(1, 2).contiguous()\n",
        "        hidden = hidden.view(self.n_layers, batch_size, self.hidden_size)\n",
        "        return hidden\n",
        "\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hidden_size])).to(device)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1)) / self.scale\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e10)\n",
        "\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "        context = torch.matmul(attention_weights, value)\n",
        "\n",
        "        return context, attention_weights\n",
        "\n",
        "class ImprovedDecoder(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, n_layers=2, dropout_p=0.2):\n",
        "        super(ImprovedDecoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.attention = ScaledDotProductAttention(hidden_size)\n",
        "\n",
        "        self.gru = nn.GRU(\n",
        "            2 * hidden_size,\n",
        "            hidden_size,\n",
        "            num_layers=n_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout_p if n_layers > 1 else 0\n",
        "        )\n",
        "\n",
        "        self.out = nn.Linear(hidden_size * 2, output_size)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
        "\n",
        "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
        "        batch_size = encoder_outputs.size(0)\n",
        "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoder_outputs = []\n",
        "        attentions = []\n",
        "\n",
        "        prev_context = torch.zeros(batch_size, 1, self.hidden_size, device=device)\n",
        "\n",
        "        for i in range(MAX_LENGTH):\n",
        "            decoder_output, decoder_hidden, attn_weights, context = self.forward_step(\n",
        "                decoder_input, decoder_hidden, encoder_outputs, prev_context\n",
        "            )\n",
        "            prev_context = context\n",
        "\n",
        "            decoder_outputs.append(decoder_output)\n",
        "            attentions.append(attn_weights)\n",
        "\n",
        "            if target_tensor is not None:\n",
        "                decoder_input = target_tensor[:, i].unsqueeze(1)\n",
        "            else:\n",
        "                _, topi = decoder_output.topk(1)\n",
        "                decoder_input = topi.squeeze(-1).detach()\n",
        "\n",
        "                if all(di.item() == EOS_token for di in decoder_input):\n",
        "                    break\n",
        "\n",
        "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
        "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
        "        attentions = torch.cat(attentions, dim=1)\n",
        "\n",
        "        return decoder_outputs, decoder_hidden, attentions\n",
        "\n",
        "    def forward_step(self, input, hidden, encoder_outputs, prev_context):\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        embedded = self.layer_norm(embedded)\n",
        "\n",
        "        rnn_input = torch.cat((embedded, prev_context), dim=2)\n",
        "\n",
        "        output, hidden = self.gru(rnn_input, hidden)\n",
        "\n",
        "        context, attn_weights = self.attention(output, encoder_outputs, encoder_outputs)\n",
        "\n",
        "        output = torch.cat((output, context), dim=2)\n",
        "        output = self.out(output)\n",
        "\n",
        "        return output, hidden, attn_weights, context\n",
        "\n",
        "# Training function\n",
        "def train_improved(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001,\n",
        "                  print_every=100, plot_every=100, clip=1.0):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0\n",
        "    plot_loss_total = 0\n",
        "\n",
        "    encoder_optimizer = optim.AdamW(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.AdamW(decoder.parameters(), lr=learning_rate)\n",
        "\n",
        "    encoder_scheduler = optim.lr_scheduler.ReduceLROnPlateau(encoder_optimizer, mode='min', patience=3)\n",
        "    decoder_scheduler = optim.lr_scheduler.ReduceLROnPlateau(decoder_optimizer, mode='min', patience=3)\n",
        "\n",
        "    criterion = nn.NLLLoss(ignore_index=0)\n",
        "\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        epoch_loss = 0\n",
        "\n",
        "        for data in train_dataloader:\n",
        "            input_tensor, target_tensor = data\n",
        "\n",
        "            encoder_optimizer.zero_grad()\n",
        "            decoder_optimizer.zero_grad()\n",
        "\n",
        "            encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "            decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
        "\n",
        "            loss = criterion(\n",
        "                decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
        "                target_tensor.view(-1)\n",
        "            )\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
        "            torch.nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
        "\n",
        "            encoder_optimizer.step()\n",
        "            decoder_optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        avg_loss = epoch_loss / len(train_dataloader)\n",
        "\n",
        "        encoder_scheduler.step(avg_loss)\n",
        "        decoder_scheduler.step(avg_loss)\n",
        "\n",
        "        print_loss_total += avg_loss\n",
        "        plot_loss_total += avg_loss\n",
        "\n",
        "        if epoch % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs),\n",
        "                                        epoch, epoch / n_epochs * 100, print_loss_avg))\n",
        "\n",
        "        if epoch % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    return plot_losses\n",
        "\n",
        "# Helper function for timing\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "# Main execution\n",
        "def main():\n",
        "    # Get data and create dataloader\n",
        "    input_lang, output_lang, train_dataloader, pairs = get_dataloader(batch_size=32)\n",
        "\n",
        "    # Initialize models\n",
        "    hidden_size = 256\n",
        "    improved_encoder = ImprovedEncoder(input_lang.n_words, hidden_size).to(device)\n",
        "    improved_decoder = ImprovedDecoder(hidden_size, output_lang.n_words).to(device)\n",
        "\n",
        "    # Train the model\n",
        "    print(\"Starting training...\")\n",
        "    improved_losses = train_improved(train_dataloader, improved_encoder, improved_decoder,\n",
        "                                   n_epochs=80, print_every=5, plot_every=5)\n",
        "\n",
        "    # Save the trained models\n",
        "    torch.save(improved_encoder.state_dict(), 'improved_encoder.pt')\n",
        "    torch.save(improved_decoder.state_dict(), 'improved_decoder.pt')\n",
        "\n",
        "    return improved_encoder, improved_decoder, improved_losses\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    improved_encoder, improved_decoder, improved_losses = main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "qAJE1i-pHYrf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b41c1dba-591d-4dbe-889b-61a0441da878"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines...\n",
            "Read 128133 sentence pairs\n",
            "Trimmed to 9320 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "heb 7519\n",
            "eng 3067\n",
            "Starting training...\n",
            "0m 42s (- 10m 40s) (5 6%) 1.9347\n",
            "1m 24s (- 9m 51s) (10 12%) 0.4789\n",
            "2m 7s (- 9m 10s) (15 18%) 0.1916\n",
            "2m 50s (- 8m 31s) (20 25%) 0.1222\n",
            "3m 32s (- 7m 46s) (25 31%) 0.0990\n",
            "4m 13s (- 7m 2s) (30 37%) 0.0867\n",
            "4m 56s (- 6m 21s) (35 43%) 0.0800\n",
            "5m 38s (- 5m 38s) (40 50%) 0.0731\n",
            "6m 24s (- 4m 59s) (45 56%) 0.0685\n",
            "7m 6s (- 4m 16s) (50 62%) 0.0524\n",
            "7m 48s (- 3m 32s) (55 68%) 0.0305\n",
            "8m 30s (- 2m 50s) (60 75%) 0.0270\n",
            "9m 12s (- 2m 7s) (65 81%) 0.0253\n",
            "9m 54s (- 1m 24s) (70 87%) 0.0243\n",
            "10m 36s (- 0m 42s) (75 93%) 0.0228\n",
            "11m 19s (- 0m 0s) (80 100%) 0.0225\n",
            "Input: הוא מתגאה בדייקנות שלו.\n",
            "Target: he is proud of his punctuality\n",
            "Output: he is proud of his punctuality <EOS>\n",
            "\n",
            "Input: אני אמור לאכול עם תום הערב.\n",
            "Target: i m supposed to eat with tom this evening\n",
            "Output: i m supposed to eat with tom this evening <EOS>\n",
            "\n",
            "Input: הוא המורה לאנגלית שלנו.\n",
            "Target: he is our english teacher\n",
            "Output: he is our english teacher <EOS>\n",
            "\n",
            "Input: אני בוגרת מספיק כדי לדאוג לעצמי.\n",
            "Target: i m old enough to look after myself\n",
            "Output: i m old enough to look after myself <EOS>\n",
            "\n",
            "Input: אני רגילה לאכול לבד.\n",
            "Target: i m used to eating alone\n",
            "Output: i used used to eating alone alone <EOS>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Run the training\n",
        "improved_encoder, improved_decoder, improved_losses = main()\n",
        "\n",
        "# After training, you can evaluate some translations\n",
        "def evaluate_examples():\n",
        "    improved_encoder.eval()\n",
        "    improved_decoder.eval()\n",
        "    with torch.no_grad():\n",
        "        for _ in range(5):\n",
        "            pair = random.choice(pairs)\n",
        "            print('Input:', pair[0])\n",
        "            print('Target:', pair[1])\n",
        "            output_words, _ = evaluate(improved_encoder, improved_decoder, pair[0], input_lang, output_lang)\n",
        "            output_sentence = ' '.join(output_words)\n",
        "            print('Output:', output_sentence)\n",
        "            print()\n",
        "\n",
        "evaluate_examples()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_metrics(encoder, decoder, test_pairs, input_lang, output_lang):\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "\n",
        "    total_pairs = len(test_pairs)\n",
        "    exact_matches = 0\n",
        "    bleu_total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for pair in test_pairs:\n",
        "            input_sentence = pair[0]\n",
        "            target_sentence = pair[1]\n",
        "\n",
        "            # Get model output\n",
        "            output_words, _ = evaluate(encoder, decoder, input_sentence, input_lang, output_lang)\n",
        "            output_sentence = ' '.join([word for word in output_words if word != '<EOS>'])\n",
        "\n",
        "            # Calculate exact match\n",
        "            if output_sentence == target_sentence:\n",
        "                exact_matches += 1\n",
        "\n",
        "            # Calculate BLEU score\n",
        "            reference = [target_sentence.split()]\n",
        "            hypothesis = output_sentence.split()\n",
        "            bleu_score = sentence_bleu(reference, hypothesis)\n",
        "            bleu_total += bleu_score\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = (exact_matches / total_pairs) * 100\n",
        "    avg_bleu = (bleu_total / total_pairs) * 100\n",
        "\n",
        "    print(f\"\\nMetrics on {total_pairs} test pairs:\")\n",
        "    print(f\"Exact Match Accuracy: {accuracy:.2f}%\")\n",
        "    print(f\"Average BLEU Score: {avg_bleu:.2f}\")\n",
        "\n",
        "    return accuracy, avg_bleu\n",
        "\n",
        "# Add this at the end of your main() function:\n",
        "test_pairs = pairs[-100:]  # Use last 100 pairs as test set\n",
        "print(\"\\nEvaluating baseline model...\")\n",
        "baseline_accuracy, baseline_bleu = calculate_metrics(encoder, decoder, test_pairs, input_lang, output_lang)\n",
        "print(\"\\nEvaluating improved model...\")\n",
        "improved_accuracy, improved_bleu = calculate_metrics(improved_encoder, improved_decoder, test_pairs, input_lang, output_lang)\n",
        "\n",
        "print(\"\\nComparison:\")\n",
        "print(f\"Baseline Model - Accuracy: {baseline_accuracy:.2f}%, BLEU: {baseline_bleu:.2f}\")\n",
        "print(f\"Improved Model - Accuracy: {improved_accuracy:.2f}%, BLEU: {improved_bleu:.2f}\")\n",
        "print(f\"Improvement - Accuracy: {improved_accuracy - baseline_accuracy:.2f}%, BLEU: {improved_bleu - baseline_bleu:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QylqmAt1CUN",
        "outputId": "2f33e4a3-a200-4c8b-d0cf-123efb4a2e82"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating baseline model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metrics on 100 test pairs:\n",
            "Exact Match Accuracy: 51.00%\n",
            "Average BLEU Score: 78.69\n",
            "\n",
            "Evaluating improved model...\n",
            "\n",
            "Metrics on 100 test pairs:\n",
            "Exact Match Accuracy: 86.00%\n",
            "Average BLEU Score: 94.41\n",
            "\n",
            "Comparison:\n",
            "Baseline Model - Accuracy: 51.00%, BLEU: 78.69\n",
            "Improved Model - Accuracy: 86.00%, BLEU: 94.41\n",
            "Improvement - Accuracy: 35.00%, BLEU: 15.72\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}